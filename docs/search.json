[{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"purpose","dir":"Articles","previous_headings":"","what":"Purpose","title":"Introduction to theft","text":"theft facilitates user-friendly access structured analytical workflow extraction, analysis, visualisation time-series features. structured workflow presented graphic (note theft many functions displayed graphic—keep reading ):","code":""},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"core-calculation-functions","dir":"Articles","previous_headings":"","what":"Core calculation functions","title":"Introduction to theft","text":"explore package functionality, going use dataset comes standard theft called simData. dataset contains collection randomly generated time series six different types processes. dataset can accessed via: data follows following structure:","code":"theft::simData head(simData) #>                      values timepoint               id        process #> Gaussian Noise.1 -0.6264538         1 Gaussian Noise_1 Gaussian Noise #> Gaussian Noise.2  0.1836433         2 Gaussian Noise_1 Gaussian Noise #> Gaussian Noise.3 -0.8356286         3 Gaussian Noise_1 Gaussian Noise #> Gaussian Noise.4  1.5952808         4 Gaussian Noise_1 Gaussian Noise #> Gaussian Noise.5  0.3295078         5 Gaussian Noise_1 Gaussian Noise #> Gaussian Noise.6 -0.8204684         6 Gaussian Noise_1 Gaussian Noise"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"calculating-feature-summary-statistics","dir":"Articles","previous_headings":"Core calculation functions","what":"Calculating feature summary statistics","title":"Introduction to theft","text":"core function automates calculation feature statistics calculate_features. can choose subset features calculate feature_set argument. choices currently \"catch22\", \"feasts\", \"Kats\", \"tsfeatures\", \"tsfresh\", /\"TSFEL\". Note Kats, tsfresh TSFEL Python packages. R package reticulate used call Python code uses packages applies within broader tidy data philosophy embodied theft. present, depending input time-series, theft provides access \\(>1200\\) features.","code":""},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"installing-python-feature-sets","dir":"Articles","previous_headings":"Core calculation functions > Calculating feature summary statistics","what":"Installing Python feature sets","title":"Introduction to theft","text":"Prior using theft (want use Kats, tsfresh TSFEL feature sets; R-based sets run fine) working Python 3.9 installation run function install_python_pkgs(python_path, path) first installing theft, python_path argument filepath location Python 3.9 machine path argument location wish install Python libraries virtual environment machine. example, wanted install Python libraries resulting virtual environment \"C:/Users/User/Desktop/theft\" Python 3.9 located \"/usr/bin/python\" machine, run following first installed theft: want use Python-based packages, must first tell R Python /virtual environment computer contains installed libraries. can done theft via init_theft function, two arguments: python_path – filepath version Python wish use (.e., entered install_python_pkgs ran first) venv_path – filepath Python virtual environment tsfresh, TSFEL, /Kats installed (.e., path returned console message install_python_pkgs ran function first) However, necessarily use convenience function. another method pointing R correct Python (reticulate findpython), can use workflow instead. NOTE: need call  init_theft solution per session.","code":"install_python_pkgs(\"C:/Users/User/Desktop/theft\", \"/usr/bin/python\")"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"calculating-features","dir":"Articles","previous_headings":"Core calculation functions > Calculating feature summary statistics","what":"Calculating features","title":"Introduction to theft","text":"ready use rest package’s functionality, beginning extraction time-series features. example catch22 set: Note catch22 set can set additional catch24 argument calculate mean standard deviation addition standard 22 features: tidy dataframe included features set correspond available dataframe feature_list: NOTE: using tsfresh feature set, might want consider tsfresh_cleanup argument calculate_features. argument defaults FALSE specifies whether use -built tsfresh relevant feature filter .","code":"feature_matrix <- calculate_features(data = simData,                                       id_var = \"id\",                                       time_var = \"timepoint\",                                       values_var = \"values\",                                       group_var = \"process\",                                       feature_set = \"catch22\",                                      seed = 123) feature_matrix <- calculate_features(data = simData,                                       id_var = \"id\",                                       time_var = \"timepoint\",                                       values_var = \"values\",                                       group_var = \"process\",                                       feature_set = \"catch22\",                                      catch24 = TRUE,                                      seed = 123) head(feature_list) #>   feature_set                  feature #> 1     catch22       DN_HistogramMode_5 #> 2     catch22      DN_HistogramMode_10 #> 3     catch22                CO_f1ecac #> 4     catch22           CO_FirstMin_ac #> 5     catch22 CO_HistogramAMI_even_2_5 #> 6     catch22            CO_trev_1_num"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"comparison-of-feature-sets","dir":"Articles","previous_headings":"Core calculation functions","what":"Comparison of feature sets","title":"Introduction to theft","text":"detailed comparison six feature sets, see paper detailed review1.","code":""},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"data-quality-checks","dir":"Articles","previous_headings":"","what":"Data quality checks","title":"Introduction to theft","text":"calculate_features function returns object class feature_calculations. Objects type purposefully looked-functions theft. class, simple methods plot() can called object produce range statistical graphics. first visualisation data types calculated feature vectors. useful inspecting features might need dropped due large proportions undesirable (e.g., NA, NaN etc.) values. can specify plot type = \"quality make graphic:","code":"plot(feature_matrix, type = \"quality\")"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"normalisingscaling-functions","dir":"Articles","previous_headings":"","what":"Normalising/scaling functions","title":"Introduction to theft","text":"Putting calculated feature vectors equal scale crucial statistical machine learning model variables high variance can adversely impact model’s capacity fit data appropriately, learn appropriate weight values, minimise loss function. theft includes function normalise rescale either whole feature_calculations object, single vector values (e.g. values participants just SB_BinaryStats_mean_longstretch1 feature). Four normalisation methods offered: z-score – \"z-score\" Sigmoid – \"Sigmoid\" Outlier-robust Sigmoid (credit Ben Fulcher creating original MATLAB version) - \"RobustSigmoid\" Min-max – \"MinMax\" Normalisation whole feature_calculations object can performed one line: single vector normalisation, need pass vector normalise checks object classes.","code":"normed <- normalise(feature_matrix, method = \"z-score\")"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"data-visualisation-and-low-dimensional-projections","dir":"Articles","previous_headings":"","what":"Data visualisation and low-dimensional projections","title":"Introduction to theft","text":"package also comes additional statistical graphical functionality: Feature time-series matrix heatmap Low dimensional projections feature space plotting scatterplot Pairwise feature correlation matrix heatmap","code":""},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"feature-matrices","dir":"Articles","previous_headings":"Data visualisation and low-dimensional projections","what":"Feature matrices","title":"Introduction to theft","text":"function calling type = \"matrix\" plot() feature_calculations object takes itand produces ggplot object heatmap showing feature vectors across x axis time series y axis. Prior plotting, function hierarchically clusters data across rows columns visually highlight empirical structure. Note several options hierarchical clustering linkage algorithm use: \"average\" (default) \"ward.D\" \"ward.D2\" \"single\" \"complete\" \"mcquitty\" \"median\" \"centroid\" See hclust documentation information. Note legend plot (matrix visualisations theft) discretised visual clarity continuous legends can difficult interpret meaningful value differences easily.  can control normalisation type method argument hierarchical clustering method clust_method argument (example used defaults manual specification needed).","code":"plot(feature_matrix, type = \"matrix\")"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"low-dimensional-projections","dir":"Articles","previous_headings":"Data visualisation and low-dimensional projections","what":"Low dimensional projections","title":"Introduction to theft","text":"function reduce_dims takes feature_calculations object calculates either principal components analysis (PCA) t-distributed stochastic neighbour embedding (t-SNE) fit . result stored custom object class called low_dimension: can similarly call plot() object produce two-dimensional scatterplot results:  Alternatively, t-SNE version can specified similar fashion, perplexity hyperparameter able controlled user. Typical values range 5 50, depending size data. lower levels perplexity, local variations tend dominate, high levels perplexity, results can uninterpretable clusters can merge. See interactive article detailed review. Shaded covariance ellipses can also disabled plotting low_dimension objects setting show_covariance = FALSE:  also option specify arguments reduce_dims passed either stats::prcomp Rtsne::Rtsne ellipsis additional arguments. example, may wish control maximum number iterations (max_iter) speed-accuracy trade-(theta) Rtsne::Rtsne: can consult documentation get list potential arguments calling either ?prcomp ?Rtsne.","code":"low_dim <- reduce_dims(feature_matrix,                         method = \"RobustSigmoid\",                         low_dim_method = \"PCA\",                         seed = 123) plot(low_dim) low_dim2 <- reduce_dims(feature_matrix,                          method = \"RobustSigmoid\",                          low_dim_method = \"t-SNE\",                          perplexity = 10,                         seed = 123)  plot(low_dim2, show_covariance = FALSE) low_dim3 <- reduce_dims(feature_matrix,                          method = \"RobustSigmoid\",                          low_dim_method = \"t-SNE\",                          perplexity = 10,                         seed = 123,                         max_iter = 5000,                         theta = 0.2)"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"pairwise-correlations","dir":"Articles","previous_headings":"Data visualisation and low-dimensional projections","what":"Pairwise correlations","title":"Introduction to theft","text":"can plot correlations feature vectors using plot(type = \"cor\") feature_calculations object:  Similarly, can control normalisation type method argument hierarchical clustering method clust_method argument (example used defaults manual specification needed).","code":"plot(feature_matrix, type = \"cor\")"},{"path":[]},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"feature-by-feature","dir":"Articles","previous_headings":"Time-series classification","what":"Feature-by-feature","title":"Introduction to theft","text":"Since feature-based time-series analysis shown particular promise classification problems, theft includes functionality exploring group separation. function tsfeature_classifier enables fit range classification models enable statistical comparisons using resampling methodology presented paper detailed review2. function meant serve fast answer can used guide analysis replacement development careful statistical pipeline. tsfeature_classifier following arguments: data—feature_calculations object containing raw feature matrix produced calculate_features included group column per theft::calculate_features classifier—function specifying classifier fit. function 2 arguments: formula data. Please note tsfeature_classifier z-scores data prior modelling using train set’s information disabling default scaling function uses recommended. Defaults NULL means following linear SVM fit: classifier = function(formula, data){mod <- e1071::svm(formula, data = data, kernel = \"linear\", scale = FALSE, probability = TRUE)} train_size—Numeric value denoting proportion samples use training set. Defaults 0.75 n_resamples—Integer denoting number resamples calculate. Defaults 30 by_set—Boolean specifying whether compute classifiers feature set. Defaults TRUE (see section “Multi-feature” ). FALSE, function instead find best individually-performing features use_null—Boolean whether fit null models class labels shuffled order generate null distribution can compared performance correct class labels. Defaults FALSE. known permutation testing seed—Integer fix R’s random number generator ensure reproducibility. Defaults 123 preference— Character denoting feature set keep (meaning others filtered ) \"feasts\", \"tsfeatures\", \"Kats\" since considerable overlap three sets. Defaults \"feasts\". applies by_set = TRUE (since set “features” constructed automatically comparator) NOTE: can apply duplicate filtering feature_calculations object function filter_duplicates. function 2 arguments: data (feature_calculations object) preference (). example, assumed calculated features sets, run following upfront: Since interested individual features section, calculate main null results feature using just 5 resamples efficiency (practice, use !) default linear SVM: show simple specify different classifier, can instead maybe use radial basis function SVM (though absolutely limited just e1071 models! can use anything can used R’s predict generic tsfeature_classifier internally constructs confusion matrices model predictions): raw classification results useful, often also like statistical evaluate facet . theft includes function compare_features . compare_features contains following arguments: data—List object containing classification outputs produce tsfeature_classifier metric—Character denoting classification performance metric use statistical testing. Can one \"accuracy\", \"precision\", \"recall\", \"f1\". Defaults \"accuracy\" by_set—Boolean specifying whether want compare feature sets (TRUE) individual features (FALSE). Defaults TRUE contingent whether computed set tsfeature_classifier hypothesis—Character denoting whether p-values calculated feature set feature (depending by_set argument) individually relative null use_null = TRUE tsfeature_classifier \"null\", whether pairwise comparisons set feature conducted main model fits \"pairwise\". Defaults \"null\" p_adj—Character denoting adjustment made p-values multiple comparisons. valid argument stats::p.adjust. Defaults \"none\" adjustment. \"holm\" recommended starting point adjustments can use compare_features evaluate well individual feature performs relative empirical null distribution (noting using defaults arguments code cleanliness): conduct pairwise comparisons individual features: can use ggplot2 summarise visualise results. pairwise correlation plot top 10 features catch22 toy problem. just simply filtering original full feature data making use plot generic defined objects class feature_calculations:  can also easily draw violin plot top 10 features visualise distributions group:","code":"feature_matrix_filt <- filter_duplicates(feature_matrix, preference = \"feasts\") feature_classifiers <- tsfeature_classifier(feature_matrix, by_set = FALSE, n_resamples = 5, use_null = TRUE) myclassifier <- function(formula, data){   mod <- e1071::svm(formula, data = data, kernel = \"radial\", scale = FALSE, probability = TRUE) }  feature_classifiers_radial <- tsfeature_classifier(feature_matrix, classifier = myclassifier, by_set = FALSE, n_resamples = 5, use_null = TRUE) feature_vs_null <- compare_features(feature_classifiers, by_set = FALSE, hypothesis = \"null\") head(feature_vs_null) #>                                                                                           hypothesis #> 1 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_Embed2_Dist_tau_d_expfit_meandiff (null) #> 2                                                       catch22_CO_f1ecac > catch22_CO_f1ecac (null) #> 3                                             catch22_CO_FirstMin_ac > catch22_CO_FirstMin_ac (null) #> 4                         catch22_CO_HistogramAMI_even_2_5 > catch22_CO_HistogramAMI_even_2_5 (null) #> 5                                               catch22_CO_trev_1_num > catch22_CO_trev_1_num (null) #> 6                                   catch22_DN_HistogramMode_10 > catch22_DN_HistogramMode_10 (null) #>                                          names  method #> 1 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff catch22 #> 2                            catch22_CO_f1ecac catch22 #> 3                       catch22_CO_FirstMin_ac catch22 #> 4             catch22_CO_HistogramAMI_even_2_5 catch22 #> 5                        catch22_CO_trev_1_num catch22 #> 6                  catch22_DN_HistogramMode_10 catch22 #>                         original_names   metric t_statistic     p.value #> 1 CO_Embed2_Dist_tau_d_expfit_meandiff accuracy    6.846532 0.001190873 #> 2                            CO_f1ecac accuracy    5.106615 0.003475256 #> 3                       CO_FirstMin_ac accuracy    2.195775 0.046545113 #> 4             CO_HistogramAMI_even_2_5 accuracy    3.674235 0.010655821 #> 5                        CO_trev_1_num accuracy    4.321159 0.006218020 #> 6                  DN_HistogramMode_10 accuracy    2.660532 0.028179542 #>   p_value_adj #> 1 0.001190873 #> 2 0.003475256 #> 3 0.046545113 #> 4 0.010655821 #> 5 0.006218020 #> 6 0.028179542 pairwise_features <- compare_features(feature_classifiers, by_set = FALSE, hypothesis = \"pairwise\") head(pairwise_features) #>                                                                        hypothesis #> 1                catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_f1ecac #> 2           catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_FirstMin_ac #> 3 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_HistogramAMI_even_2_5 #> 4            catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_trev_1_num #> 5      catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_DN_HistogramMode_10 #> 6       catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_DN_HistogramMode_5 #>                                        names_a                          names_b #> 1 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff                catch22_CO_f1ecac #> 2 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff           catch22_CO_FirstMin_ac #> 3 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff catch22_CO_HistogramAMI_even_2_5 #> 4 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff            catch22_CO_trev_1_num #> 5 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff      catch22_DN_HistogramMode_10 #> 6 catch22_CO_Embed2_Dist_tau_d_expfit_meandiff       catch22_DN_HistogramMode_5 #>     metric t_statistic     p.value p_value_adj #> 1 accuracy    4.146140 0.007152509 0.007152509 #> 2 accuracy    2.927700 0.021456091 0.021456091 #> 3 accuracy    1.107823 0.165032825 0.165032825 #> 4 accuracy    3.166667 0.016983072 0.016983072 #> 5 accuracy    3.761703 0.009872873 0.009872873 #> 6 accuracy    7.892800 0.000696776 0.000696776 top_10 <- feature_vs_null %>%    dplyr::slice_min(p_value_adj, n = 10) %>%    dplyr::select(c(method, original_names, p_value_adj))  feature_matrix_filt <- feature_matrix[[1]] %>%    dplyr::filter(method %in% top_10$method & names %in% top_10$original_names)  feature_matrix_filt <- structure(list(feature_matrix_filt), class = \"feature_calculations\") plot(feature_matrix_filt, type = \"cor\") feature_matrix_filt[[1]] %>%   ggplot2::ggplot(ggplot2::aes(x = group, y = values, colour = group)) +   ggplot2::geom_violin() +   ggplot2::geom_point(size = 1, alpha = 0.9, position = ggplot2::position_jitter(w = 0.05)) +   ggplot2::labs(x = \"Class\",                 y = \"Value\") +   ggplot2::scale_colour_brewer(palette = \"Dark2\") +   ggplot2::theme_bw() +   ggplot2::theme(legend.position = \"none\",                  panel.grid.minor = ggplot2::element_blank(),                  strip.background = ggplot2::element_blank(),                  axis.text.x = ggplot2::element_text(angle = 90)) +   ggplot2::facet_wrap(~ names, ncol = 2, scales = \"free_y\")"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"multi-feature","dir":"Articles","previous_headings":"Time-series classification","what":"Multi-feature","title":"Introduction to theft","text":"Since theft contains entire sets features, can also use tsfeature_classifier compare set level by_set argument: Since calculated catch22 vignette, comparate set \"features\" automatically always included default. means identical results, functionality becomes powerful feature sets also calculated. Similar individual feature case, can also use ggplot2 summarise findings. comparison mean accuracy \\(\\pm 1SD\\) feature sets:  can see, given computed catch22 plot super informative, computed others, provide fast summary average performance set level—useful reference point potentially huge set calculated features.","code":"set_classifiers <- tsfeature_classifier(feature_matrix, by_set = TRUE, n_resamples = 5, use_null = TRUE) head(set_classifiers) #> $TrainTestSizes #> train_size  test_size  #>        135         45  #>  #> $ClassificationResults #>    model_type resample   accuracy mean_precision mean_recall mean_f1_score #> 1        Main        1 0.55555556      0.6396825  0.64861111     0.6311182 #> 2        Main        2 0.60000000      0.6825397  0.69188034     0.6667894 #> 3        Main        3 0.60000000      0.6769841  0.66111111     0.6637639 #> 4        Main        4 0.64444444      0.7246032  0.71825397     0.6979972 #> 5        Main        5 0.60000000      0.6968254  0.69841270     0.6782930 #> 6        Null        1 0.13333333      0.1515873  0.14615385     0.2364672 #> 7        Null        2 0.06666667      0.1000000  0.05357143     0.2064777 #> 8        Null        3 0.13333333      0.1710317  0.15170614     0.1708604 #> 9        Null        4 0.13333333      0.2000000  0.05138340     0.2410714 #> 10       Null        5 0.22222222      0.3166667  0.25015873     0.4711955 #> 11       Main        1 0.55555556      0.6396825  0.64861111     0.6311182 #> 12       Main        2 0.60000000      0.6825397  0.69188034     0.6667894 #> 13       Main        3 0.60000000      0.6769841  0.66111111     0.6637639 #> 14       Main        4 0.64444444      0.7246032  0.71825397     0.6979972 #> 15       Main        5 0.60000000      0.6968254  0.69841270     0.6782930 #> 16       Null        1 0.13333333      0.1515873  0.14615385     0.2364672 #> 17       Null        2 0.06666667      0.1000000  0.05357143     0.2064777 #> 18       Null        3 0.13333333      0.1710317  0.15170614     0.1708604 #> 19       Null        4 0.13333333      0.2000000  0.05138340     0.2410714 #> 20       Null        5 0.22222222      0.3166667  0.25015873     0.4711955 #>          method #> 1  All features #> 2  All features #> 3  All features #> 4  All features #> 5  All features #> 6  All features #> 7  All features #> 8  All features #> 9  All features #> 10 All features #> 11      catch22 #> 12      catch22 #> 13      catch22 #> 14      catch22 #> 15      catch22 #> 16      catch22 #> 17      catch22 #> 18      catch22 #> 19      catch22 #> 20      catch22 set_classifiers$ClassificationResults %>%   dplyr::filter(model_type == \"Main\") %>%    dplyr::group_by(method) %>%   dplyr::summarise(mu = mean(accuracy),                    lower = mu - (1 * sd(accuracy)),                    upper = mu + (1 * sd(accuracy))) %>%   dplyr::ungroup() %>%   ggplot2::ggplot(ggplot2::aes(x = reorder(method, -mu), y = mu, colour = method)) +   ggplot2::geom_errorbar(ggplot2::aes(ymin = lower, ymax = upper)) +   ggplot2::geom_point(size = 5) +   ggplot2::labs(x = \"Feature set\",                 y = \"Classification accuracy\") +   ggplot2::scale_colour_brewer(palette = \"Dark2\") +   ggplot2::theme_bw() +   ggplot2::theme(legend.position = \"none\",                  panel.grid.minor = ggplot2::element_blank())"},{"path":"https://hendersontrent.github.io/theft/articles/theft.html","id":"reading-and-processing-hctsa-formatted-files","dir":"Articles","previous_headings":"","what":"Reading and processing hctsa-formatted files","title":"Introduction to theft","text":"theft based foundations laid hctsa, also functionality reading hctsa-formatted Matlab files automatically processing tidy dataframes ready feature extraction theft. process_hctsa_file function takes string filepath Matlab file work , returning dataframe naming conventions consistent theft functionality. per hctsa specifications Input File Format 1, file 3 variables following exact names: timeSeriesData, labels, keywords. filepath can local drive path URL.","code":""},{"path":"https://hendersontrent.github.io/theft/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Trent Henderson. Maintainer, author. Annie Bryant. Contributor.            Balanced classification accuracy","code":""},{"path":"https://hendersontrent.github.io/theft/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Henderson T (2023). theft: Tools Handling Extraction Features Time Series. R package version 0.5.0, https://hendersontrent.github.io/theft/.","code":"@Manual{,   title = {theft: Tools for Handling Extraction of Features from Time Series},   author = {Trent Henderson},   year = {2023},   note = {R package version 0.5.0},   url = {https://hendersontrent.github.io/theft/}, }"},{"path":"https://hendersontrent.github.io/theft/index.html","id":"theft-","dir":"","previous_headings":"","what":"Tools for Handling Extraction of Features from Time Series","title":"Tools for Handling Extraction of Features from Time Series","text":"Tools Handling Extraction Features Time series (theft)","code":""},{"path":"https://hendersontrent.github.io/theft/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tools for Handling Extraction of Features from Time Series","text":"can install stable version theft CRAN: can install development version theft GitHub using following: Please also check new paper Feature-Based Time-Series Analysis R using theft Package discusses motivation theoretical underpinnings theft walks functionality using Bonn EEG dataset — well-studied neuroscience dataset.","code":"install.packages(\"theft\") devtools::install_github(\"hendersontrent/theft\")"},{"path":"https://hendersontrent.github.io/theft/index.html","id":"general-purpose","dir":"","previous_headings":"","what":"General purpose","title":"Tools for Handling Extraction of Features from Time Series","text":"theft software package R facilitates user-friendly access structured analytical workflow extraction, analysis, visualisation time-series features. package provides single point access large number time-series features range existing R Python packages lets user specify groups () features calculate. packages theft currently ‘steals’ features include: catch22 (R; see Rcatch22 native implementation CRAN) feasts (R) tsfeatures (R) Kats (Python) tsfresh (Python) TSFEL (Python) Note Kats, tsfresh TSFEL Python packages. R package reticulate used call Python code uses packages applies within broader tidy data philosophy embodied theft. present, depending input time series, theft provides access  > 1200 features. Prior using theft (want use Kats, tsfresh TSFEL feature sets; R-based sets run fine) working Python 3.9 installation run function install_python_pkgs first installing theft. Please run ?install_python_pkgs consult vignette information. comprehensive comparison six feature sets, please refer recent paper Empirical Evaluation Time-Series Feature Sets.","code":""},{"path":"https://hendersontrent.github.io/theft/index.html","id":"statistical-and-graphical-tools","dir":"","previous_headings":"","what":"Statistical and graphical tools","title":"Tools for Handling Extraction of Features from Time Series","text":"theft also contains extensive suite tools automatic processing extracted feature vectors (including data quality assessments normalisation methods), low dimensional projections (linear nonlinear), data matrix visualisations, single feature multiple feature time-series classification procedures, various statistical graphical tools.","code":""},{"path":"https://hendersontrent.github.io/theft/index.html","id":"web-application","dir":"","previous_headings":"","what":"Web application","title":"Tools for Handling Extraction of Features from Time Series","text":"interactive web application built top theft enables users access functionality included package within web browser without code. application automates entire workflow included theft, converts static graphics included package interactive visualisations, enables downloads feature calculations. Note since theft active development project, functionality copied across webtool yet.","code":""},{"path":"https://hendersontrent.github.io/theft/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Tools for Handling Extraction of Features from Time Series","text":"use theft work, please cite paper: T. Henderson Ben D. Fulcher. Feature-Based Time-Series Analysis R using theft Package. arXiv, (2022). software:","code":"To cite package 'theft' in publications use:    Henderson T (2023). _theft: Tools for Handling Extraction of Features   from Time Series_. R package version 0.5.0,   <https://hendersontrent.github.io/theft/>.  A BibTeX entry for LaTeX users is    @Manual{,     title = {theft: Tools for Handling Extraction of Features from Time Series},     author = {Trent Henderson},     year = {2023},     note = {R package version 0.5.0},     url = {https://hendersontrent.github.io/theft/},   }"},{"path":"https://hendersontrent.github.io/theft/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2022 Trent Henderson Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://hendersontrent.github.io/theft/reference/calculate_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute features on an input time series dataset — calculate_features","title":"Compute features on an input time series dataset — calculate_features","text":"Compute features input time series dataset","code":""},{"path":"https://hendersontrent.github.io/theft/reference/calculate_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute features on an input time series dataset — calculate_features","text":"","code":"calculate_features(   data,   id_var = \"id\",   time_var = \"timepoint\",   values_var = \"values\",   group_var = NULL,   feature_set = c(\"catch22\", \"feasts\", \"tsfeatures\", \"Kats\", \"tsfresh\", \"TSFEL\"),   catch24 = FALSE,   tsfresh_cleanup = FALSE,   seed = 123 )"},{"path":"https://hendersontrent.github.io/theft/reference/calculate_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute features on an input time series dataset — calculate_features","text":"data data.frame least 4 columns: id variable, group variable, time variable, value variable id_var character specifying ID variable identify time series. Defaults \"id\" time_var character specifying time index variable. Defaults \"timepoint\" values_var character specifying values variable. Defaults \"values\" group_var character specifying grouping variable unique series sits (one exists). Defaults NULL feature_set character vector character denoting set time-series features calculate. Defaults \"catch22\" catch24 Boolean specifying whether compute catch24 addition catch22 catch22 one feature sets selected. Defaults FALSE tsfresh_cleanup Boolean specifying whether use -built tsfresh relevant feature filter . Defaults FALSE seed integer denoting fixed number R's random number generator ensure reproducibility. Defaults 123","code":""},{"path":"https://hendersontrent.github.io/theft/reference/calculate_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute features on an input time series dataset — calculate_features","text":"object class feature_calculations contains summary statistics feature","code":""},{"path":"https://hendersontrent.github.io/theft/reference/calculate_features.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute features on an input time series dataset — calculate_features","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/calculate_features.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute features on an input time series dataset — calculate_features","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #> Warning: As of 0.1.14 the feature 'CO_f1ecac' returns a double instead of int #> This warning is displayed once per session. #>  #> Calculations completed for catch22."},{"path":"https://hendersontrent.github.io/theft/reference/check_vector_quality.html","id":null,"dir":"Reference","previous_headings":"","what":"Check for presence of NAs and non-numerics in a vector — check_vector_quality","title":"Check for presence of NAs and non-numerics in a vector — check_vector_quality","text":"Check presence NAs non-numerics vector","code":""},{"path":"https://hendersontrent.github.io/theft/reference/check_vector_quality.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check for presence of NAs and non-numerics in a vector — check_vector_quality","text":"","code":"check_vector_quality(x)"},{"path":"https://hendersontrent.github.io/theft/reference/check_vector_quality.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check for presence of NAs and non-numerics in a vector — check_vector_quality","text":"x input vector","code":""},{"path":"https://hendersontrent.github.io/theft/reference/check_vector_quality.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check for presence of NAs and non-numerics in a vector — check_vector_quality","text":"Boolean whether data good extract features ","code":""},{"path":"https://hendersontrent.github.io/theft/reference/check_vector_quality.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Check for presence of NAs and non-numerics in a vector — check_vector_quality","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compare_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Conduct statistical testing on time-series feature classification performance to identify top features or compare entire sets — compare_features","title":"Conduct statistical testing on time-series feature classification performance to identify top features or compare entire sets — compare_features","text":"Conduct statistical testing time-series feature classification performance identify top features compare entire sets","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compare_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Conduct statistical testing on time-series feature classification performance to identify top features or compare entire sets — compare_features","text":"","code":"compare_features(   data,   metric = c(\"accuracy\", \"precision\", \"recall\", \"f1\"),   by_set = TRUE,   hypothesis = c(\"null\", \"pairwise\"),   p_adj = c(\"none\", \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", \"fdr\") )"},{"path":"https://hendersontrent.github.io/theft/reference/compare_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Conduct statistical testing on time-series feature classification performance to identify top features or compare entire sets — compare_features","text":"data list object containing classification outputs produce tsfeature_classifier metric character denoting classification performance metric use statistical testing. Can one \"accuracy\", \"precision\", \"recall\", \"f1\". Defaults \"accuracy\" by_set Boolean specifying whether want compare feature sets (TRUE) individual features (FALSE). Defaults TRUE contingent whether computed set tsfeature_classifier hypothesis character denoting whether p-values calculated feature set feature (depending by_set argument) individually relative null use_null = TRUE tsfeature_classifier \"null\", whether pairwise comparisons set feature conducted main model fits \"pairwise\". Defaults \"null\" p_adj character denoting adjustment made p-values multiple comparisons. valid argument stats::p.adjust. Defaults \"none\" adjustment. \"holm\" recommended starting point adjustments","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compare_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Conduct statistical testing on time-series feature classification performance to identify top features or compare entire sets — compare_features","text":"data.frame containing results","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compare_features.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Conduct statistical testing on time-series feature classification performance to identify top features or compare entire sets — compare_features","text":"Henderson, T., Bryant, . G., Fulcher, B. D. Never Dull Moment: Distributional Properties Baseline Time-Series Classification. 27th Pacific-Asia Conference Knowledge Discovery Data Mining, (2023).","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compare_features.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Conduct statistical testing on time-series feature classification performance to identify top features or compare entire sets — compare_features","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compare_features.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Conduct statistical testing on time-series feature classification performance to identify top features or compare entire sets — compare_features","text":"","code":"# \\donttest{ featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.    classifiers <- tsfeature_classifier(featMat,   by_set = FALSE) #> Fitting model 1/660 #> Fitting model 2/660 #> Fitting model 3/660 #> Fitting model 4/660 #> Fitting model 5/660 #> Fitting model 6/660 #> Fitting model 7/660 #> Fitting model 8/660 #> Fitting model 9/660 #> Fitting model 10/660 #> Fitting model 11/660 #> Fitting model 12/660 #> Fitting model 13/660 #> Fitting model 14/660 #> Fitting model 15/660 #> Fitting model 16/660 #> Fitting model 17/660 #> Fitting model 18/660 #> Fitting model 19/660 #> Fitting model 20/660 #> Fitting model 21/660 #> Fitting model 22/660 #> Fitting model 23/660 #> Fitting model 24/660 #> Fitting model 25/660 #> Fitting model 26/660 #> Fitting model 27/660 #> Fitting model 28/660 #> Fitting model 29/660 #> Fitting model 30/660 #> Fitting model 31/660 #> Fitting model 32/660 #> Fitting model 33/660 #> Fitting model 34/660 #> Fitting model 35/660 #> Fitting model 36/660 #> Fitting model 37/660 #> Fitting model 38/660 #> Fitting model 39/660 #> Fitting model 40/660 #> Fitting model 41/660 #> Fitting model 42/660 #> Fitting model 43/660 #> Fitting model 44/660 #> Fitting model 45/660 #> Fitting model 46/660 #> Fitting model 47/660 #> Fitting model 48/660 #> Fitting model 49/660 #> Fitting model 50/660 #> Fitting model 51/660 #> Fitting model 52/660 #> Fitting model 53/660 #> Fitting model 54/660 #> Fitting model 55/660 #> Fitting model 56/660 #> Fitting model 57/660 #> Fitting model 58/660 #> Fitting model 59/660 #> Fitting model 60/660 #> Fitting model 61/660 #> Fitting model 62/660 #> Fitting model 63/660 #> Fitting model 64/660 #> Fitting model 65/660 #> Fitting model 66/660 #> Fitting model 67/660 #> Fitting model 68/660 #> Fitting model 69/660 #> Fitting model 70/660 #> Fitting model 71/660 #> Fitting model 72/660 #> Fitting model 73/660 #> Fitting model 74/660 #> Fitting model 75/660 #> Fitting model 76/660 #> Fitting model 77/660 #> Fitting model 78/660 #> Fitting model 79/660 #> Fitting model 80/660 #> Fitting model 81/660 #> Fitting model 82/660 #> Fitting model 83/660 #> Fitting model 84/660 #> Fitting model 85/660 #> Fitting model 86/660 #> Fitting model 87/660 #> Fitting model 88/660 #> Fitting model 89/660 #> Fitting model 90/660 #> Fitting model 91/660 #> Fitting model 92/660 #> Fitting model 93/660 #> Fitting model 94/660 #> Fitting model 95/660 #> Fitting model 96/660 #> Fitting model 97/660 #> Fitting model 98/660 #> Fitting model 99/660 #> Fitting model 100/660 #> Fitting model 101/660 #> Fitting model 102/660 #> Fitting model 103/660 #> Fitting model 104/660 #> Fitting model 105/660 #> Fitting model 106/660 #> Fitting model 107/660 #> Fitting model 108/660 #> Fitting model 109/660 #> Fitting model 110/660 #> Fitting model 111/660 #> Fitting model 112/660 #> Fitting model 113/660 #> Fitting model 114/660 #> Fitting model 115/660 #> Fitting model 116/660 #> Fitting model 117/660 #> Fitting model 118/660 #> Fitting model 119/660 #> Fitting model 120/660 #> Fitting model 121/660 #> Fitting model 122/660 #> Fitting model 123/660 #> Fitting model 124/660 #> Fitting model 125/660 #> Fitting model 126/660 #> Fitting model 127/660 #> Fitting model 128/660 #> Fitting model 129/660 #> Fitting model 130/660 #> Fitting model 131/660 #> Fitting model 132/660 #> Fitting model 133/660 #> Fitting model 134/660 #> Fitting model 135/660 #> Fitting model 136/660 #> Fitting model 137/660 #> Fitting model 138/660 #> Fitting model 139/660 #> Fitting model 140/660 #> Fitting model 141/660 #> Fitting model 142/660 #> Fitting model 143/660 #> Fitting model 144/660 #> Fitting model 145/660 #> Fitting model 146/660 #> Fitting model 147/660 #> Fitting model 148/660 #> Fitting model 149/660 #> Fitting model 150/660 #> Fitting model 151/660 #> Fitting model 152/660 #> Fitting model 153/660 #> Fitting model 154/660 #> Fitting model 155/660 #> Fitting model 156/660 #> Fitting model 157/660 #> Fitting model 158/660 #> Fitting model 159/660 #> Fitting model 160/660 #> Fitting model 161/660 #> Fitting model 162/660 #> Fitting model 163/660 #> Fitting model 164/660 #> Fitting model 165/660 #> Fitting model 166/660 #> Fitting model 167/660 #> Fitting model 168/660 #> Fitting model 169/660 #> Fitting model 170/660 #> Fitting model 171/660 #> Fitting model 172/660 #> Fitting model 173/660 #> Fitting model 174/660 #> Fitting model 175/660 #> Fitting model 176/660 #> Fitting model 177/660 #> Fitting model 178/660 #> Fitting model 179/660 #> Fitting model 180/660 #> Fitting model 181/660 #> Fitting model 182/660 #> Fitting model 183/660 #> Fitting model 184/660 #> Fitting model 185/660 #> Fitting model 186/660 #> Fitting model 187/660 #> Fitting model 188/660 #> Fitting model 189/660 #> Fitting model 190/660 #> Fitting model 191/660 #> Fitting model 192/660 #> Fitting model 193/660 #> Fitting model 194/660 #> Fitting model 195/660 #> Fitting model 196/660 #> Fitting model 197/660 #> Fitting model 198/660 #> Fitting model 199/660 #> Fitting model 200/660 #> Fitting model 201/660 #> Fitting model 202/660 #> Fitting model 203/660 #> Fitting model 204/660 #> Fitting model 205/660 #> Fitting model 206/660 #> Fitting model 207/660 #> Fitting model 208/660 #> Fitting model 209/660 #> Fitting model 210/660 #> Fitting model 211/660 #> Fitting model 212/660 #> Fitting model 213/660 #> Fitting model 214/660 #> Fitting model 215/660 #> Fitting model 216/660 #> Fitting model 217/660 #> Fitting model 218/660 #> Fitting model 219/660 #> Fitting model 220/660 #> Fitting model 221/660 #> Fitting model 222/660 #> Fitting model 223/660 #> Fitting model 224/660 #> Fitting model 225/660 #> Fitting model 226/660 #> Fitting model 227/660 #> Fitting model 228/660 #> Fitting model 229/660 #> Fitting model 230/660 #> Fitting model 231/660 #> Fitting model 232/660 #> Fitting model 233/660 #> Fitting model 234/660 #> Fitting model 235/660 #> Fitting model 236/660 #> Fitting model 237/660 #> Fitting model 238/660 #> Fitting model 239/660 #> Fitting model 240/660 #> Fitting model 241/660 #> Fitting model 242/660 #> Fitting model 243/660 #> Fitting model 244/660 #> Fitting model 245/660 #> Fitting model 246/660 #> Fitting model 247/660 #> Fitting model 248/660 #> Fitting model 249/660 #> Fitting model 250/660 #> Fitting model 251/660 #> Fitting model 252/660 #> Fitting model 253/660 #> Fitting model 254/660 #> Fitting model 255/660 #> Fitting model 256/660 #> Fitting model 257/660 #> Fitting model 258/660 #> Fitting model 259/660 #> Fitting model 260/660 #> Fitting model 261/660 #> Fitting model 262/660 #> Fitting model 263/660 #> Fitting model 264/660 #> Fitting model 265/660 #> Fitting model 266/660 #> Fitting model 267/660 #> Fitting model 268/660 #> Fitting model 269/660 #> Fitting model 270/660 #> Fitting model 271/660 #> Fitting model 272/660 #> Fitting model 273/660 #> Fitting model 274/660 #> Fitting model 275/660 #> Fitting model 276/660 #> Fitting model 277/660 #> Fitting model 278/660 #> Fitting model 279/660 #> Fitting model 280/660 #> Fitting model 281/660 #> Fitting model 282/660 #> Fitting model 283/660 #> Fitting model 284/660 #> Fitting model 285/660 #> Fitting model 286/660 #> Fitting model 287/660 #> Fitting model 288/660 #> Fitting model 289/660 #> Fitting model 290/660 #> Fitting model 291/660 #> Fitting model 292/660 #> Fitting model 293/660 #> Fitting model 294/660 #> Fitting model 295/660 #> Fitting model 296/660 #> Fitting model 297/660 #> Fitting model 298/660 #> Fitting model 299/660 #> Fitting model 300/660 #> Fitting model 301/660 #> Fitting model 302/660 #> Fitting model 303/660 #> Fitting model 304/660 #> Fitting model 305/660 #> Fitting model 306/660 #> Fitting model 307/660 #> Fitting model 308/660 #> Fitting model 309/660 #> Fitting model 310/660 #> Fitting model 311/660 #> Fitting model 312/660 #> Fitting model 313/660 #> Fitting model 314/660 #> Fitting model 315/660 #> Fitting model 316/660 #> Fitting model 317/660 #> Fitting model 318/660 #> Fitting model 319/660 #> Fitting model 320/660 #> Fitting model 321/660 #> Fitting model 322/660 #> Fitting model 323/660 #> Fitting model 324/660 #> Fitting model 325/660 #> Fitting model 326/660 #> Fitting model 327/660 #> Fitting model 328/660 #> Fitting model 329/660 #> Fitting model 330/660 #> Fitting model 331/660 #> Fitting model 332/660 #> Fitting model 333/660 #> Fitting model 334/660 #> Fitting model 335/660 #> Fitting model 336/660 #> Fitting model 337/660 #> Fitting model 338/660 #> Fitting model 339/660 #> Fitting model 340/660 #> Fitting model 341/660 #> Fitting model 342/660 #> Fitting model 343/660 #> Fitting model 344/660 #> Fitting model 345/660 #> Fitting model 346/660 #> Fitting model 347/660 #> Fitting model 348/660 #> Fitting model 349/660 #> Fitting model 350/660 #> Fitting model 351/660 #> Fitting model 352/660 #> Fitting model 353/660 #> Fitting model 354/660 #> Fitting model 355/660 #> Fitting model 356/660 #> Fitting model 357/660 #> Fitting model 358/660 #> Fitting model 359/660 #> Fitting model 360/660 #> Fitting model 361/660 #> Fitting model 362/660 #> Fitting model 363/660 #> Fitting model 364/660 #> Fitting model 365/660 #> Fitting model 366/660 #> Fitting model 367/660 #> Fitting model 368/660 #> Fitting model 369/660 #> Fitting model 370/660 #> Fitting model 371/660 #> Fitting model 372/660 #> Fitting model 373/660 #> Fitting model 374/660 #> Fitting model 375/660 #> Fitting model 376/660 #> Fitting model 377/660 #> Fitting model 378/660 #> Fitting model 379/660 #> Fitting model 380/660 #> Fitting model 381/660 #> Fitting model 382/660 #> Fitting model 383/660 #> Fitting model 384/660 #> Fitting model 385/660 #> Fitting model 386/660 #> Fitting model 387/660 #> Fitting model 388/660 #> Fitting model 389/660 #> Fitting model 390/660 #> Fitting model 391/660 #> Fitting model 392/660 #> Fitting model 393/660 #> Fitting model 394/660 #> Fitting model 395/660 #> Fitting model 396/660 #> Fitting model 397/660 #> Fitting model 398/660 #> Fitting model 399/660 #> Fitting model 400/660 #> Fitting model 401/660 #> Fitting model 402/660 #> Fitting model 403/660 #> Fitting model 404/660 #> Fitting model 405/660 #> Fitting model 406/660 #> Fitting model 407/660 #> Fitting model 408/660 #> Fitting model 409/660 #> Fitting model 410/660 #> Fitting model 411/660 #> Fitting model 412/660 #> Fitting model 413/660 #> Fitting model 414/660 #> Fitting model 415/660 #> Fitting model 416/660 #> Fitting model 417/660 #> Fitting model 418/660 #> Fitting model 419/660 #> Fitting model 420/660 #> Fitting model 421/660 #> Fitting model 422/660 #> Fitting model 423/660 #> Fitting model 424/660 #> Fitting model 425/660 #> Fitting model 426/660 #> Fitting model 427/660 #> Fitting model 428/660 #> Fitting model 429/660 #> Fitting model 430/660 #> Fitting model 431/660 #> Fitting model 432/660 #> Fitting model 433/660 #> Fitting model 434/660 #> Fitting model 435/660 #> Fitting model 436/660 #> Fitting model 437/660 #> Fitting model 438/660 #> Fitting model 439/660 #> Fitting model 440/660 #> Fitting model 441/660 #> Fitting model 442/660 #> Fitting model 443/660 #> Fitting model 444/660 #> Fitting model 445/660 #> Fitting model 446/660 #> Fitting model 447/660 #> Fitting model 448/660 #> Fitting model 449/660 #> Fitting model 450/660 #> Fitting model 451/660 #> Fitting model 452/660 #> Fitting model 453/660 #> Fitting model 454/660 #> Fitting model 455/660 #> Fitting model 456/660 #> Fitting model 457/660 #> Fitting model 458/660 #> Fitting model 459/660 #> Fitting model 460/660 #> Fitting model 461/660 #> Fitting model 462/660 #> Fitting model 463/660 #> Fitting model 464/660 #> Fitting model 465/660 #> Fitting model 466/660 #> Fitting model 467/660 #> Fitting model 468/660 #> Fitting model 469/660 #> Fitting model 470/660 #> Fitting model 471/660 #> Fitting model 472/660 #> Fitting model 473/660 #> Fitting model 474/660 #> Fitting model 475/660 #> Fitting model 476/660 #> Fitting model 477/660 #> Fitting model 478/660 #> Fitting model 479/660 #> Fitting model 480/660 #> Fitting model 481/660 #> Fitting model 482/660 #> Fitting model 483/660 #> Fitting model 484/660 #> Fitting model 485/660 #> Fitting model 486/660 #> Fitting model 487/660 #> Fitting model 488/660 #> Fitting model 489/660 #> Fitting model 490/660 #> Fitting model 491/660 #> Fitting model 492/660 #> Fitting model 493/660 #> Fitting model 494/660 #> Fitting model 495/660 #> Fitting model 496/660 #> Fitting model 497/660 #> Fitting model 498/660 #> Fitting model 499/660 #> Fitting model 500/660 #> Fitting model 501/660 #> Fitting model 502/660 #> Fitting model 503/660 #> Fitting model 504/660 #> Fitting model 505/660 #> Fitting model 506/660 #> Fitting model 507/660 #> Fitting model 508/660 #> Fitting model 509/660 #> Fitting model 510/660 #> Fitting model 511/660 #> Fitting model 512/660 #> Fitting model 513/660 #> Fitting model 514/660 #> Fitting model 515/660 #> Fitting model 516/660 #> Fitting model 517/660 #> Fitting model 518/660 #> Fitting model 519/660 #> Fitting model 520/660 #> Fitting model 521/660 #> Fitting model 522/660 #> Fitting model 523/660 #> Fitting model 524/660 #> Fitting model 525/660 #> Fitting model 526/660 #> Fitting model 527/660 #> Fitting model 528/660 #> Fitting model 529/660 #> Fitting model 530/660 #> Fitting model 531/660 #> Fitting model 532/660 #> Fitting model 533/660 #> Fitting model 534/660 #> Fitting model 535/660 #> Fitting model 536/660 #> Fitting model 537/660 #> Fitting model 538/660 #> Fitting model 539/660 #> Fitting model 540/660 #> Fitting model 541/660 #> Fitting model 542/660 #> Fitting model 543/660 #> Fitting model 544/660 #> Fitting model 545/660 #> Fitting model 546/660 #> Fitting model 547/660 #> Fitting model 548/660 #> Fitting model 549/660 #> Fitting model 550/660 #> Fitting model 551/660 #> Fitting model 552/660 #> Fitting model 553/660 #> Fitting model 554/660 #> Fitting model 555/660 #> Fitting model 556/660 #> Fitting model 557/660 #> Fitting model 558/660 #> Fitting model 559/660 #> Fitting model 560/660 #> Fitting model 561/660 #> Fitting model 562/660 #> Fitting model 563/660 #> Fitting model 564/660 #> Fitting model 565/660 #> Fitting model 566/660 #> Fitting model 567/660 #> Fitting model 568/660 #> Fitting model 569/660 #> Fitting model 570/660 #> Fitting model 571/660 #> Fitting model 572/660 #> Fitting model 573/660 #> Fitting model 574/660 #> Fitting model 575/660 #> Fitting model 576/660 #> Fitting model 577/660 #> Fitting model 578/660 #> Fitting model 579/660 #> Fitting model 580/660 #> Fitting model 581/660 #> Fitting model 582/660 #> Fitting model 583/660 #> Fitting model 584/660 #> Fitting model 585/660 #> Fitting model 586/660 #> Fitting model 587/660 #> Fitting model 588/660 #> Fitting model 589/660 #> Fitting model 590/660 #> Fitting model 591/660 #> Fitting model 592/660 #> Fitting model 593/660 #> Fitting model 594/660 #> Fitting model 595/660 #> Fitting model 596/660 #> Fitting model 597/660 #> Fitting model 598/660 #> Fitting model 599/660 #> Fitting model 600/660 #> Fitting model 601/660 #> Fitting model 602/660 #> Fitting model 603/660 #> Fitting model 604/660 #> Fitting model 605/660 #> Fitting model 606/660 #> Fitting model 607/660 #> Fitting model 608/660 #> Fitting model 609/660 #> Fitting model 610/660 #> Fitting model 611/660 #> Fitting model 612/660 #> Fitting model 613/660 #> Fitting model 614/660 #> Fitting model 615/660 #> Fitting model 616/660 #> Fitting model 617/660 #> Fitting model 618/660 #> Fitting model 619/660 #> Fitting model 620/660 #> Fitting model 621/660 #> Fitting model 622/660 #> Fitting model 623/660 #> Fitting model 624/660 #> Fitting model 625/660 #> Fitting model 626/660 #> Fitting model 627/660 #> Fitting model 628/660 #> Fitting model 629/660 #> Fitting model 630/660 #> Fitting model 631/660 #> Fitting model 632/660 #> Fitting model 633/660 #> Fitting model 634/660 #> Fitting model 635/660 #> Fitting model 636/660 #> Fitting model 637/660 #> Fitting model 638/660 #> Fitting model 639/660 #> Fitting model 640/660 #> Fitting model 641/660 #> Fitting model 642/660 #> Fitting model 643/660 #> Fitting model 644/660 #> Fitting model 645/660 #> Fitting model 646/660 #> Fitting model 647/660 #> Fitting model 648/660 #> Fitting model 649/660 #> Fitting model 650/660 #> Fitting model 651/660 #> Fitting model 652/660 #> Fitting model 653/660 #> Fitting model 654/660 #> Fitting model 655/660 #> Fitting model 656/660 #> Fitting model 657/660 #> Fitting model 658/660 #> Fitting model 659/660 #> Fitting model 660/660    compare_features(classifiers,   by_set = FALSE,   hypothesis = \"pairwise\")  #> Calculating comparison 1/231 #> Calculating comparison 2/231 #> Calculating comparison 3/231 #> Calculating comparison 4/231 #> Calculating comparison 5/231 #> Calculating comparison 6/231 #> Calculating comparison 7/231 #> Calculating comparison 8/231 #> Calculating comparison 9/231 #> Calculating comparison 10/231 #> Calculating comparison 11/231 #> Calculating comparison 12/231 #> Calculating comparison 13/231 #> Calculating comparison 14/231 #> Calculating comparison 15/231 #> Calculating comparison 16/231 #> Calculating comparison 17/231 #> Calculating comparison 18/231 #> Calculating comparison 19/231 #> Calculating comparison 20/231 #> Calculating comparison 21/231 #> Calculating comparison 22/231 #> Calculating comparison 23/231 #> Calculating comparison 24/231 #> Calculating comparison 25/231 #> Calculating comparison 26/231 #> Calculating comparison 27/231 #> Calculating comparison 28/231 #> Calculating comparison 29/231 #> Calculating comparison 30/231 #> Calculating comparison 31/231 #> Calculating comparison 32/231 #> Calculating comparison 33/231 #> Calculating comparison 34/231 #> Calculating comparison 35/231 #> Calculating comparison 36/231 #> Calculating comparison 37/231 #> Calculating comparison 38/231 #> Calculating comparison 39/231 #> Calculating comparison 40/231 #> Calculating comparison 41/231 #> Calculating comparison 42/231 #> Calculating comparison 43/231 #> Calculating comparison 44/231 #> Calculating comparison 45/231 #> Calculating comparison 46/231 #> Calculating comparison 47/231 #> Calculating comparison 48/231 #> Calculating comparison 49/231 #> Calculating comparison 50/231 #> Calculating comparison 51/231 #> Calculating comparison 52/231 #> Calculating comparison 53/231 #> Calculating comparison 54/231 #> Calculating comparison 55/231 #> Calculating comparison 56/231 #> Calculating comparison 57/231 #> Calculating comparison 58/231 #> Calculating comparison 59/231 #> Calculating comparison 60/231 #> Calculating comparison 61/231 #> Calculating comparison 62/231 #> Calculating comparison 63/231 #> Calculating comparison 64/231 #> Calculating comparison 65/231 #> Calculating comparison 66/231 #> Calculating comparison 67/231 #> Calculating comparison 68/231 #> Calculating comparison 69/231 #> Calculating comparison 70/231 #> Calculating comparison 71/231 #> Calculating comparison 72/231 #> Calculating comparison 73/231 #> Calculating comparison 74/231 #> Calculating comparison 75/231 #> Calculating comparison 76/231 #> Calculating comparison 77/231 #> Calculating comparison 78/231 #> Calculating comparison 79/231 #> Calculating comparison 80/231 #> Calculating comparison 81/231 #> Calculating comparison 82/231 #> Calculating comparison 83/231 #> Calculating comparison 84/231 #> Calculating comparison 85/231 #> Calculating comparison 86/231 #> Calculating comparison 87/231 #> Calculating comparison 88/231 #> Calculating comparison 89/231 #> Calculating comparison 90/231 #> Calculating comparison 91/231 #> Calculating comparison 92/231 #> Calculating comparison 93/231 #> Calculating comparison 94/231 #> Calculating comparison 95/231 #> Calculating comparison 96/231 #> Calculating comparison 97/231 #> Calculating comparison 98/231 #> Calculating comparison 99/231 #> Calculating comparison 100/231 #> Calculating comparison 101/231 #> Calculating comparison 102/231 #> Calculating comparison 103/231 #> Calculating comparison 104/231 #> Calculating comparison 105/231 #> Calculating comparison 106/231 #> Calculating comparison 107/231 #> Calculating comparison 108/231 #> Calculating comparison 109/231 #> Calculating comparison 110/231 #> Calculating comparison 111/231 #> Calculating comparison 112/231 #> Calculating comparison 113/231 #> Calculating comparison 114/231 #> Calculating comparison 115/231 #> Calculating comparison 116/231 #> Calculating comparison 117/231 #> Calculating comparison 118/231 #> Calculating comparison 119/231 #> Calculating comparison 120/231 #> Calculating comparison 121/231 #> Calculating comparison 122/231 #> Calculating comparison 123/231 #> Calculating comparison 124/231 #> Calculating comparison 125/231 #> Calculating comparison 126/231 #> Calculating comparison 127/231 #> Calculating comparison 128/231 #> Calculating comparison 129/231 #> Calculating comparison 130/231 #> Calculating comparison 131/231 #> Calculating comparison 132/231 #> Calculating comparison 133/231 #> Calculating comparison 134/231 #> Calculating comparison 135/231 #> Calculating comparison 136/231 #> Calculating comparison 137/231 #> Calculating comparison 138/231 #> Calculating comparison 139/231 #> Calculating comparison 140/231 #> Calculating comparison 141/231 #> Calculating comparison 142/231 #> Calculating comparison 143/231 #> Calculating comparison 144/231 #> Calculating comparison 145/231 #> Calculating comparison 146/231 #> Calculating comparison 147/231 #> Calculating comparison 148/231 #> Calculating comparison 149/231 #> Calculating comparison 150/231 #> Calculating comparison 151/231 #> Calculating comparison 152/231 #> Calculating comparison 153/231 #> Calculating comparison 154/231 #> Calculating comparison 155/231 #> Calculating comparison 156/231 #> Calculating comparison 157/231 #> Calculating comparison 158/231 #> Calculating comparison 159/231 #> Calculating comparison 160/231 #> Calculating comparison 161/231 #> Calculating comparison 162/231 #> Calculating comparison 163/231 #> Calculating comparison 164/231 #> Calculating comparison 165/231 #> Calculating comparison 166/231 #> Calculating comparison 167/231 #> Calculating comparison 168/231 #> Calculating comparison 169/231 #> Calculating comparison 170/231 #> Calculating comparison 171/231 #> Calculating comparison 172/231 #> Calculating comparison 173/231 #> Calculating comparison 174/231 #> Calculating comparison 175/231 #> Calculating comparison 176/231 #> Calculating comparison 177/231 #> Calculating comparison 178/231 #> Calculating comparison 179/231 #> Calculating comparison 180/231 #> Calculating comparison 181/231 #> Calculating comparison 182/231 #> Calculating comparison 183/231 #> Calculating comparison 184/231 #> Calculating comparison 185/231 #> Calculating comparison 186/231 #> Calculating comparison 187/231 #> Calculating comparison 188/231 #> Calculating comparison 189/231 #> Calculating comparison 190/231 #> Calculating comparison 191/231 #> Calculating comparison 192/231 #> Calculating comparison 193/231 #> Calculating comparison 194/231 #> Calculating comparison 195/231 #> Calculating comparison 196/231 #> Calculating comparison 197/231 #> Calculating comparison 198/231 #> Calculating comparison 199/231 #> Calculating comparison 200/231 #> Calculating comparison 201/231 #> Calculating comparison 202/231 #> Calculating comparison 203/231 #> Calculating comparison 204/231 #> Calculating comparison 205/231 #> Calculating comparison 206/231 #> Calculating comparison 207/231 #> Calculating comparison 208/231 #> Calculating comparison 209/231 #> Calculating comparison 210/231 #> Calculating comparison 211/231 #> Calculating comparison 212/231 #> Calculating comparison 213/231 #> Calculating comparison 214/231 #> Calculating comparison 215/231 #> Calculating comparison 216/231 #> Calculating comparison 217/231 #> Calculating comparison 218/231 #> Calculating comparison 219/231 #> Calculating comparison 220/231 #> Calculating comparison 221/231 #> Calculating comparison 222/231 #> Calculating comparison 223/231 #> Calculating comparison 224/231 #> Calculating comparison 225/231 #> Calculating comparison 226/231 #> Calculating comparison 227/231 #> Calculating comparison 228/231 #> Calculating comparison 229/231 #> Calculating comparison 230/231 #> Calculating comparison 231/231 #>                                                                                                hypothesis #> 1                                   catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_FirstMin_ac #> 2                         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_HistogramAMI_even_2_5 #> 3                                        catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_f1ecac #> 4                                    catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_CO_trev_1_num #> 5                              catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_DN_HistogramMode_10 #> 6                               catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_DN_HistogramMode_5 #> 7                    catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_DN_OutlierInclude_n_001_mdrmd #> 8                    catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_DN_OutlierInclude_p_001_mdrmd #> 9                   catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_FC_LocalSimple_mean1_tauresrat #> 10                     catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_FC_LocalSimple_mean3_stderr #> 11         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 12                            catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_MD_hrv_classic_pnn40 #> 13                       catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_PD_PeriodicityWang_th0_01 #> 14                catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_SB_BinaryStats_diff_longstretch0 #> 15                catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_SB_BinaryStats_mean_longstretch1 #> 16                       catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_SB_MotifThree_quantile_hh #> 17              catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 18          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 19     catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 20                catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_SP_Summaries_welch_rect_area_5_1 #> 21                catch22_CO_Embed2_Dist_tau_d_expfit_meandiff > catch22_SP_Summaries_welch_rect_centroid #> 22                                              catch22_CO_FirstMin_ac > catch22_CO_HistogramAMI_even_2_5 #> 23                                                             catch22_CO_FirstMin_ac > catch22_CO_f1ecac #> 24                                                         catch22_CO_FirstMin_ac > catch22_CO_trev_1_num #> 25                                                   catch22_CO_FirstMin_ac > catch22_DN_HistogramMode_10 #> 26                                                    catch22_CO_FirstMin_ac > catch22_DN_HistogramMode_5 #> 27                                         catch22_CO_FirstMin_ac > catch22_DN_OutlierInclude_n_001_mdrmd #> 28                                         catch22_CO_FirstMin_ac > catch22_DN_OutlierInclude_p_001_mdrmd #> 29                                        catch22_CO_FirstMin_ac > catch22_FC_LocalSimple_mean1_tauresrat #> 30                                           catch22_CO_FirstMin_ac > catch22_FC_LocalSimple_mean3_stderr #> 31                               catch22_CO_FirstMin_ac > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 32                                                  catch22_CO_FirstMin_ac > catch22_MD_hrv_classic_pnn40 #> 33                                             catch22_CO_FirstMin_ac > catch22_PD_PeriodicityWang_th0_01 #> 34                                      catch22_CO_FirstMin_ac > catch22_SB_BinaryStats_diff_longstretch0 #> 35                                      catch22_CO_FirstMin_ac > catch22_SB_BinaryStats_mean_longstretch1 #> 36                                             catch22_CO_FirstMin_ac > catch22_SB_MotifThree_quantile_hh #> 37                                    catch22_CO_FirstMin_ac > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 38                                catch22_CO_FirstMin_ac > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 39                           catch22_CO_FirstMin_ac > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 40                                      catch22_CO_FirstMin_ac > catch22_SP_Summaries_welch_rect_area_5_1 #> 41                                      catch22_CO_FirstMin_ac > catch22_SP_Summaries_welch_rect_centroid #> 42                                                   catch22_CO_HistogramAMI_even_2_5 > catch22_CO_f1ecac #> 43                                               catch22_CO_HistogramAMI_even_2_5 > catch22_CO_trev_1_num #> 44                                         catch22_CO_HistogramAMI_even_2_5 > catch22_DN_HistogramMode_10 #> 45                                          catch22_CO_HistogramAMI_even_2_5 > catch22_DN_HistogramMode_5 #> 46                               catch22_CO_HistogramAMI_even_2_5 > catch22_DN_OutlierInclude_n_001_mdrmd #> 47                               catch22_CO_HistogramAMI_even_2_5 > catch22_DN_OutlierInclude_p_001_mdrmd #> 48                              catch22_CO_HistogramAMI_even_2_5 > catch22_FC_LocalSimple_mean1_tauresrat #> 49                                 catch22_CO_HistogramAMI_even_2_5 > catch22_FC_LocalSimple_mean3_stderr #> 50                     catch22_CO_HistogramAMI_even_2_5 > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 51                                        catch22_CO_HistogramAMI_even_2_5 > catch22_MD_hrv_classic_pnn40 #> 52                                   catch22_CO_HistogramAMI_even_2_5 > catch22_PD_PeriodicityWang_th0_01 #> 53                            catch22_CO_HistogramAMI_even_2_5 > catch22_SB_BinaryStats_diff_longstretch0 #> 54                            catch22_CO_HistogramAMI_even_2_5 > catch22_SB_BinaryStats_mean_longstretch1 #> 55                                   catch22_CO_HistogramAMI_even_2_5 > catch22_SB_MotifThree_quantile_hh #> 56                          catch22_CO_HistogramAMI_even_2_5 > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 57                      catch22_CO_HistogramAMI_even_2_5 > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 58                 catch22_CO_HistogramAMI_even_2_5 > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 59                            catch22_CO_HistogramAMI_even_2_5 > catch22_SP_Summaries_welch_rect_area_5_1 #> 60                            catch22_CO_HistogramAMI_even_2_5 > catch22_SP_Summaries_welch_rect_centroid #> 61                                                              catch22_CO_f1ecac > catch22_CO_trev_1_num #> 62                                                        catch22_CO_f1ecac > catch22_DN_HistogramMode_10 #> 63                                                         catch22_CO_f1ecac > catch22_DN_HistogramMode_5 #> 64                                              catch22_CO_f1ecac > catch22_DN_OutlierInclude_n_001_mdrmd #> 65                                              catch22_CO_f1ecac > catch22_DN_OutlierInclude_p_001_mdrmd #> 66                                             catch22_CO_f1ecac > catch22_FC_LocalSimple_mean1_tauresrat #> 67                                                catch22_CO_f1ecac > catch22_FC_LocalSimple_mean3_stderr #> 68                                    catch22_CO_f1ecac > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 69                                                       catch22_CO_f1ecac > catch22_MD_hrv_classic_pnn40 #> 70                                                  catch22_CO_f1ecac > catch22_PD_PeriodicityWang_th0_01 #> 71                                           catch22_CO_f1ecac > catch22_SB_BinaryStats_diff_longstretch0 #> 72                                           catch22_CO_f1ecac > catch22_SB_BinaryStats_mean_longstretch1 #> 73                                                  catch22_CO_f1ecac > catch22_SB_MotifThree_quantile_hh #> 74                                         catch22_CO_f1ecac > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 75                                     catch22_CO_f1ecac > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 76                                catch22_CO_f1ecac > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 77                                           catch22_CO_f1ecac > catch22_SP_Summaries_welch_rect_area_5_1 #> 78                                           catch22_CO_f1ecac > catch22_SP_Summaries_welch_rect_centroid #> 79                                                    catch22_CO_trev_1_num > catch22_DN_HistogramMode_10 #> 80                                                     catch22_CO_trev_1_num > catch22_DN_HistogramMode_5 #> 81                                          catch22_CO_trev_1_num > catch22_DN_OutlierInclude_n_001_mdrmd #> 82                                          catch22_CO_trev_1_num > catch22_DN_OutlierInclude_p_001_mdrmd #> 83                                         catch22_CO_trev_1_num > catch22_FC_LocalSimple_mean1_tauresrat #> 84                                            catch22_CO_trev_1_num > catch22_FC_LocalSimple_mean3_stderr #> 85                                catch22_CO_trev_1_num > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 86                                                   catch22_CO_trev_1_num > catch22_MD_hrv_classic_pnn40 #> 87                                              catch22_CO_trev_1_num > catch22_PD_PeriodicityWang_th0_01 #> 88                                       catch22_CO_trev_1_num > catch22_SB_BinaryStats_diff_longstretch0 #> 89                                       catch22_CO_trev_1_num > catch22_SB_BinaryStats_mean_longstretch1 #> 90                                              catch22_CO_trev_1_num > catch22_SB_MotifThree_quantile_hh #> 91                                     catch22_CO_trev_1_num > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 92                                 catch22_CO_trev_1_num > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 93                            catch22_CO_trev_1_num > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 94                                       catch22_CO_trev_1_num > catch22_SP_Summaries_welch_rect_area_5_1 #> 95                                       catch22_CO_trev_1_num > catch22_SP_Summaries_welch_rect_centroid #> 96                                               catch22_DN_HistogramMode_10 > catch22_DN_HistogramMode_5 #> 97                                    catch22_DN_HistogramMode_10 > catch22_DN_OutlierInclude_n_001_mdrmd #> 98                                    catch22_DN_HistogramMode_10 > catch22_DN_OutlierInclude_p_001_mdrmd #> 99                                   catch22_DN_HistogramMode_10 > catch22_FC_LocalSimple_mean1_tauresrat #> 100                                     catch22_DN_HistogramMode_10 > catch22_FC_LocalSimple_mean3_stderr #> 101                         catch22_DN_HistogramMode_10 > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 102                                            catch22_DN_HistogramMode_10 > catch22_MD_hrv_classic_pnn40 #> 103                                       catch22_DN_HistogramMode_10 > catch22_PD_PeriodicityWang_th0_01 #> 104                                catch22_DN_HistogramMode_10 > catch22_SB_BinaryStats_diff_longstretch0 #> 105                                catch22_DN_HistogramMode_10 > catch22_SB_BinaryStats_mean_longstretch1 #> 106                                       catch22_DN_HistogramMode_10 > catch22_SB_MotifThree_quantile_hh #> 107                              catch22_DN_HistogramMode_10 > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 108                          catch22_DN_HistogramMode_10 > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 109                     catch22_DN_HistogramMode_10 > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 110                                catch22_DN_HistogramMode_10 > catch22_SP_Summaries_welch_rect_area_5_1 #> 111                                catch22_DN_HistogramMode_10 > catch22_SP_Summaries_welch_rect_centroid #> 112                                    catch22_DN_HistogramMode_5 > catch22_DN_OutlierInclude_n_001_mdrmd #> 113                                    catch22_DN_HistogramMode_5 > catch22_DN_OutlierInclude_p_001_mdrmd #> 114                                   catch22_DN_HistogramMode_5 > catch22_FC_LocalSimple_mean1_tauresrat #> 115                                      catch22_DN_HistogramMode_5 > catch22_FC_LocalSimple_mean3_stderr #> 116                          catch22_DN_HistogramMode_5 > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 117                                             catch22_DN_HistogramMode_5 > catch22_MD_hrv_classic_pnn40 #> 118                                        catch22_DN_HistogramMode_5 > catch22_PD_PeriodicityWang_th0_01 #> 119                                 catch22_DN_HistogramMode_5 > catch22_SB_BinaryStats_diff_longstretch0 #> 120                                 catch22_DN_HistogramMode_5 > catch22_SB_BinaryStats_mean_longstretch1 #> 121                                        catch22_DN_HistogramMode_5 > catch22_SB_MotifThree_quantile_hh #> 122                               catch22_DN_HistogramMode_5 > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 123                           catch22_DN_HistogramMode_5 > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 124                      catch22_DN_HistogramMode_5 > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 125                                 catch22_DN_HistogramMode_5 > catch22_SP_Summaries_welch_rect_area_5_1 #> 126                                 catch22_DN_HistogramMode_5 > catch22_SP_Summaries_welch_rect_centroid #> 127                         catch22_DN_OutlierInclude_n_001_mdrmd > catch22_DN_OutlierInclude_p_001_mdrmd #> 128                        catch22_DN_OutlierInclude_n_001_mdrmd > catch22_FC_LocalSimple_mean1_tauresrat #> 129                           catch22_DN_OutlierInclude_n_001_mdrmd > catch22_FC_LocalSimple_mean3_stderr #> 130               catch22_DN_OutlierInclude_n_001_mdrmd > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 131                                  catch22_DN_OutlierInclude_n_001_mdrmd > catch22_MD_hrv_classic_pnn40 #> 132                             catch22_DN_OutlierInclude_n_001_mdrmd > catch22_PD_PeriodicityWang_th0_01 #> 133                      catch22_DN_OutlierInclude_n_001_mdrmd > catch22_SB_BinaryStats_diff_longstretch0 #> 134                      catch22_DN_OutlierInclude_n_001_mdrmd > catch22_SB_BinaryStats_mean_longstretch1 #> 135                             catch22_DN_OutlierInclude_n_001_mdrmd > catch22_SB_MotifThree_quantile_hh #> 136                    catch22_DN_OutlierInclude_n_001_mdrmd > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 137                catch22_DN_OutlierInclude_n_001_mdrmd > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 138           catch22_DN_OutlierInclude_n_001_mdrmd > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 139                      catch22_DN_OutlierInclude_n_001_mdrmd > catch22_SP_Summaries_welch_rect_area_5_1 #> 140                      catch22_DN_OutlierInclude_n_001_mdrmd > catch22_SP_Summaries_welch_rect_centroid #> 141                        catch22_DN_OutlierInclude_p_001_mdrmd > catch22_FC_LocalSimple_mean1_tauresrat #> 142                           catch22_DN_OutlierInclude_p_001_mdrmd > catch22_FC_LocalSimple_mean3_stderr #> 143               catch22_DN_OutlierInclude_p_001_mdrmd > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 144                                  catch22_DN_OutlierInclude_p_001_mdrmd > catch22_MD_hrv_classic_pnn40 #> 145                             catch22_DN_OutlierInclude_p_001_mdrmd > catch22_PD_PeriodicityWang_th0_01 #> 146                      catch22_DN_OutlierInclude_p_001_mdrmd > catch22_SB_BinaryStats_diff_longstretch0 #> 147                      catch22_DN_OutlierInclude_p_001_mdrmd > catch22_SB_BinaryStats_mean_longstretch1 #> 148                             catch22_DN_OutlierInclude_p_001_mdrmd > catch22_SB_MotifThree_quantile_hh #> 149                    catch22_DN_OutlierInclude_p_001_mdrmd > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 150                catch22_DN_OutlierInclude_p_001_mdrmd > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 151           catch22_DN_OutlierInclude_p_001_mdrmd > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 152                      catch22_DN_OutlierInclude_p_001_mdrmd > catch22_SP_Summaries_welch_rect_area_5_1 #> 153                      catch22_DN_OutlierInclude_p_001_mdrmd > catch22_SP_Summaries_welch_rect_centroid #> 154                          catch22_FC_LocalSimple_mean1_tauresrat > catch22_FC_LocalSimple_mean3_stderr #> 155              catch22_FC_LocalSimple_mean1_tauresrat > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 156                                 catch22_FC_LocalSimple_mean1_tauresrat > catch22_MD_hrv_classic_pnn40 #> 157                            catch22_FC_LocalSimple_mean1_tauresrat > catch22_PD_PeriodicityWang_th0_01 #> 158                     catch22_FC_LocalSimple_mean1_tauresrat > catch22_SB_BinaryStats_diff_longstretch0 #> 159                     catch22_FC_LocalSimple_mean1_tauresrat > catch22_SB_BinaryStats_mean_longstretch1 #> 160                            catch22_FC_LocalSimple_mean1_tauresrat > catch22_SB_MotifThree_quantile_hh #> 161                   catch22_FC_LocalSimple_mean1_tauresrat > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 162               catch22_FC_LocalSimple_mean1_tauresrat > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 163          catch22_FC_LocalSimple_mean1_tauresrat > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 164                     catch22_FC_LocalSimple_mean1_tauresrat > catch22_SP_Summaries_welch_rect_area_5_1 #> 165                     catch22_FC_LocalSimple_mean1_tauresrat > catch22_SP_Summaries_welch_rect_centroid #> 166                 catch22_FC_LocalSimple_mean3_stderr > catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 167                                    catch22_FC_LocalSimple_mean3_stderr > catch22_MD_hrv_classic_pnn40 #> 168                               catch22_FC_LocalSimple_mean3_stderr > catch22_PD_PeriodicityWang_th0_01 #> 169                        catch22_FC_LocalSimple_mean3_stderr > catch22_SB_BinaryStats_diff_longstretch0 #> 170                        catch22_FC_LocalSimple_mean3_stderr > catch22_SB_BinaryStats_mean_longstretch1 #> 171                               catch22_FC_LocalSimple_mean3_stderr > catch22_SB_MotifThree_quantile_hh #> 172                      catch22_FC_LocalSimple_mean3_stderr > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 173                  catch22_FC_LocalSimple_mean3_stderr > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 174             catch22_FC_LocalSimple_mean3_stderr > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 175                        catch22_FC_LocalSimple_mean3_stderr > catch22_SP_Summaries_welch_rect_area_5_1 #> 176                        catch22_FC_LocalSimple_mean3_stderr > catch22_SP_Summaries_welch_rect_centroid #> 177                        catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_MD_hrv_classic_pnn40 #> 178                   catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_PD_PeriodicityWang_th0_01 #> 179            catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_SB_BinaryStats_diff_longstretch0 #> 180            catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_SB_BinaryStats_mean_longstretch1 #> 181                   catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_SB_MotifThree_quantile_hh #> 182          catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 183      catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 184 catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 185            catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_SP_Summaries_welch_rect_area_5_1 #> 186            catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi > catch22_SP_Summaries_welch_rect_centroid #> 187                                      catch22_MD_hrv_classic_pnn40 > catch22_PD_PeriodicityWang_th0_01 #> 188                               catch22_MD_hrv_classic_pnn40 > catch22_SB_BinaryStats_diff_longstretch0 #> 189                               catch22_MD_hrv_classic_pnn40 > catch22_SB_BinaryStats_mean_longstretch1 #> 190                                      catch22_MD_hrv_classic_pnn40 > catch22_SB_MotifThree_quantile_hh #> 191                             catch22_MD_hrv_classic_pnn40 > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 192                         catch22_MD_hrv_classic_pnn40 > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 193                    catch22_MD_hrv_classic_pnn40 > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 194                               catch22_MD_hrv_classic_pnn40 > catch22_SP_Summaries_welch_rect_area_5_1 #> 195                               catch22_MD_hrv_classic_pnn40 > catch22_SP_Summaries_welch_rect_centroid #> 196                          catch22_PD_PeriodicityWang_th0_01 > catch22_SB_BinaryStats_diff_longstretch0 #> 197                          catch22_PD_PeriodicityWang_th0_01 > catch22_SB_BinaryStats_mean_longstretch1 #> 198                                 catch22_PD_PeriodicityWang_th0_01 > catch22_SB_MotifThree_quantile_hh #> 199                        catch22_PD_PeriodicityWang_th0_01 > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 200                    catch22_PD_PeriodicityWang_th0_01 > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 201               catch22_PD_PeriodicityWang_th0_01 > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 202                          catch22_PD_PeriodicityWang_th0_01 > catch22_SP_Summaries_welch_rect_area_5_1 #> 203                          catch22_PD_PeriodicityWang_th0_01 > catch22_SP_Summaries_welch_rect_centroid #> 204                   catch22_SB_BinaryStats_diff_longstretch0 > catch22_SB_BinaryStats_mean_longstretch1 #> 205                          catch22_SB_BinaryStats_diff_longstretch0 > catch22_SB_MotifThree_quantile_hh #> 206                 catch22_SB_BinaryStats_diff_longstretch0 > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 207             catch22_SB_BinaryStats_diff_longstretch0 > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 208        catch22_SB_BinaryStats_diff_longstretch0 > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 209                   catch22_SB_BinaryStats_diff_longstretch0 > catch22_SP_Summaries_welch_rect_area_5_1 #> 210                   catch22_SB_BinaryStats_diff_longstretch0 > catch22_SP_Summaries_welch_rect_centroid #> 211                          catch22_SB_BinaryStats_mean_longstretch1 > catch22_SB_MotifThree_quantile_hh #> 212                 catch22_SB_BinaryStats_mean_longstretch1 > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 213             catch22_SB_BinaryStats_mean_longstretch1 > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 214        catch22_SB_BinaryStats_mean_longstretch1 > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 215                   catch22_SB_BinaryStats_mean_longstretch1 > catch22_SP_Summaries_welch_rect_area_5_1 #> 216                   catch22_SB_BinaryStats_mean_longstretch1 > catch22_SP_Summaries_welch_rect_centroid #> 217                        catch22_SB_MotifThree_quantile_hh > catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 218                    catch22_SB_MotifThree_quantile_hh > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 219               catch22_SB_MotifThree_quantile_hh > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 220                          catch22_SB_MotifThree_quantile_hh > catch22_SP_Summaries_welch_rect_area_5_1 #> 221                          catch22_SB_MotifThree_quantile_hh > catch22_SP_Summaries_welch_rect_centroid #> 222           catch22_SB_TransitionMatrix_3ac_sumdiagcov > catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 223      catch22_SB_TransitionMatrix_3ac_sumdiagcov > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 224                 catch22_SB_TransitionMatrix_3ac_sumdiagcov > catch22_SP_Summaries_welch_rect_area_5_1 #> 225                 catch22_SB_TransitionMatrix_3ac_sumdiagcov > catch22_SP_Summaries_welch_rect_centroid #> 226  catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 > catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 227             catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 > catch22_SP_Summaries_welch_rect_area_5_1 #> 228             catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 > catch22_SP_Summaries_welch_rect_centroid #> 229        catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 > catch22_SP_Summaries_welch_rect_area_5_1 #> 230        catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 > catch22_SP_Summaries_welch_rect_centroid #> 231                   catch22_SP_Summaries_welch_rect_area_5_1 > catch22_SP_Summaries_welch_rect_centroid #>                                                 names_a #> 1          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 2          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 3          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 4          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 5          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 6          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 7          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 8          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 9          catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 10         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 11         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 12         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 13         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 14         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 15         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 16         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 17         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 18         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 19         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 20         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 21         catch22_CO_Embed2_Dist_tau_d_expfit_meandiff #> 22                               catch22_CO_FirstMin_ac #> 23                               catch22_CO_FirstMin_ac #> 24                               catch22_CO_FirstMin_ac #> 25                               catch22_CO_FirstMin_ac #> 26                               catch22_CO_FirstMin_ac #> 27                               catch22_CO_FirstMin_ac #> 28                               catch22_CO_FirstMin_ac #> 29                               catch22_CO_FirstMin_ac #> 30                               catch22_CO_FirstMin_ac #> 31                               catch22_CO_FirstMin_ac #> 32                               catch22_CO_FirstMin_ac #> 33                               catch22_CO_FirstMin_ac #> 34                               catch22_CO_FirstMin_ac #> 35                               catch22_CO_FirstMin_ac #> 36                               catch22_CO_FirstMin_ac #> 37                               catch22_CO_FirstMin_ac #> 38                               catch22_CO_FirstMin_ac #> 39                               catch22_CO_FirstMin_ac #> 40                               catch22_CO_FirstMin_ac #> 41                               catch22_CO_FirstMin_ac #> 42                     catch22_CO_HistogramAMI_even_2_5 #> 43                     catch22_CO_HistogramAMI_even_2_5 #> 44                     catch22_CO_HistogramAMI_even_2_5 #> 45                     catch22_CO_HistogramAMI_even_2_5 #> 46                     catch22_CO_HistogramAMI_even_2_5 #> 47                     catch22_CO_HistogramAMI_even_2_5 #> 48                     catch22_CO_HistogramAMI_even_2_5 #> 49                     catch22_CO_HistogramAMI_even_2_5 #> 50                     catch22_CO_HistogramAMI_even_2_5 #> 51                     catch22_CO_HistogramAMI_even_2_5 #> 52                     catch22_CO_HistogramAMI_even_2_5 #> 53                     catch22_CO_HistogramAMI_even_2_5 #> 54                     catch22_CO_HistogramAMI_even_2_5 #> 55                     catch22_CO_HistogramAMI_even_2_5 #> 56                     catch22_CO_HistogramAMI_even_2_5 #> 57                     catch22_CO_HistogramAMI_even_2_5 #> 58                     catch22_CO_HistogramAMI_even_2_5 #> 59                     catch22_CO_HistogramAMI_even_2_5 #> 60                     catch22_CO_HistogramAMI_even_2_5 #> 61                                    catch22_CO_f1ecac #> 62                                    catch22_CO_f1ecac #> 63                                    catch22_CO_f1ecac #> 64                                    catch22_CO_f1ecac #> 65                                    catch22_CO_f1ecac #> 66                                    catch22_CO_f1ecac #> 67                                    catch22_CO_f1ecac #> 68                                    catch22_CO_f1ecac #> 69                                    catch22_CO_f1ecac #> 70                                    catch22_CO_f1ecac #> 71                                    catch22_CO_f1ecac #> 72                                    catch22_CO_f1ecac #> 73                                    catch22_CO_f1ecac #> 74                                    catch22_CO_f1ecac #> 75                                    catch22_CO_f1ecac #> 76                                    catch22_CO_f1ecac #> 77                                    catch22_CO_f1ecac #> 78                                    catch22_CO_f1ecac #> 79                                catch22_CO_trev_1_num #> 80                                catch22_CO_trev_1_num #> 81                                catch22_CO_trev_1_num #> 82                                catch22_CO_trev_1_num #> 83                                catch22_CO_trev_1_num #> 84                                catch22_CO_trev_1_num #> 85                                catch22_CO_trev_1_num #> 86                                catch22_CO_trev_1_num #> 87                                catch22_CO_trev_1_num #> 88                                catch22_CO_trev_1_num #> 89                                catch22_CO_trev_1_num #> 90                                catch22_CO_trev_1_num #> 91                                catch22_CO_trev_1_num #> 92                                catch22_CO_trev_1_num #> 93                                catch22_CO_trev_1_num #> 94                                catch22_CO_trev_1_num #> 95                                catch22_CO_trev_1_num #> 96                          catch22_DN_HistogramMode_10 #> 97                          catch22_DN_HistogramMode_10 #> 98                          catch22_DN_HistogramMode_10 #> 99                          catch22_DN_HistogramMode_10 #> 100                         catch22_DN_HistogramMode_10 #> 101                         catch22_DN_HistogramMode_10 #> 102                         catch22_DN_HistogramMode_10 #> 103                         catch22_DN_HistogramMode_10 #> 104                         catch22_DN_HistogramMode_10 #> 105                         catch22_DN_HistogramMode_10 #> 106                         catch22_DN_HistogramMode_10 #> 107                         catch22_DN_HistogramMode_10 #> 108                         catch22_DN_HistogramMode_10 #> 109                         catch22_DN_HistogramMode_10 #> 110                         catch22_DN_HistogramMode_10 #> 111                         catch22_DN_HistogramMode_10 #> 112                          catch22_DN_HistogramMode_5 #> 113                          catch22_DN_HistogramMode_5 #> 114                          catch22_DN_HistogramMode_5 #> 115                          catch22_DN_HistogramMode_5 #> 116                          catch22_DN_HistogramMode_5 #> 117                          catch22_DN_HistogramMode_5 #> 118                          catch22_DN_HistogramMode_5 #> 119                          catch22_DN_HistogramMode_5 #> 120                          catch22_DN_HistogramMode_5 #> 121                          catch22_DN_HistogramMode_5 #> 122                          catch22_DN_HistogramMode_5 #> 123                          catch22_DN_HistogramMode_5 #> 124                          catch22_DN_HistogramMode_5 #> 125                          catch22_DN_HistogramMode_5 #> 126                          catch22_DN_HistogramMode_5 #> 127               catch22_DN_OutlierInclude_n_001_mdrmd #> 128               catch22_DN_OutlierInclude_n_001_mdrmd #> 129               catch22_DN_OutlierInclude_n_001_mdrmd #> 130               catch22_DN_OutlierInclude_n_001_mdrmd #> 131               catch22_DN_OutlierInclude_n_001_mdrmd #> 132               catch22_DN_OutlierInclude_n_001_mdrmd #> 133               catch22_DN_OutlierInclude_n_001_mdrmd #> 134               catch22_DN_OutlierInclude_n_001_mdrmd #> 135               catch22_DN_OutlierInclude_n_001_mdrmd #> 136               catch22_DN_OutlierInclude_n_001_mdrmd #> 137               catch22_DN_OutlierInclude_n_001_mdrmd #> 138               catch22_DN_OutlierInclude_n_001_mdrmd #> 139               catch22_DN_OutlierInclude_n_001_mdrmd #> 140               catch22_DN_OutlierInclude_n_001_mdrmd #> 141               catch22_DN_OutlierInclude_p_001_mdrmd #> 142               catch22_DN_OutlierInclude_p_001_mdrmd #> 143               catch22_DN_OutlierInclude_p_001_mdrmd #> 144               catch22_DN_OutlierInclude_p_001_mdrmd #> 145               catch22_DN_OutlierInclude_p_001_mdrmd #> 146               catch22_DN_OutlierInclude_p_001_mdrmd #> 147               catch22_DN_OutlierInclude_p_001_mdrmd #> 148               catch22_DN_OutlierInclude_p_001_mdrmd #> 149               catch22_DN_OutlierInclude_p_001_mdrmd #> 150               catch22_DN_OutlierInclude_p_001_mdrmd #> 151               catch22_DN_OutlierInclude_p_001_mdrmd #> 152               catch22_DN_OutlierInclude_p_001_mdrmd #> 153               catch22_DN_OutlierInclude_p_001_mdrmd #> 154              catch22_FC_LocalSimple_mean1_tauresrat #> 155              catch22_FC_LocalSimple_mean1_tauresrat #> 156              catch22_FC_LocalSimple_mean1_tauresrat #> 157              catch22_FC_LocalSimple_mean1_tauresrat #> 158              catch22_FC_LocalSimple_mean1_tauresrat #> 159              catch22_FC_LocalSimple_mean1_tauresrat #> 160              catch22_FC_LocalSimple_mean1_tauresrat #> 161              catch22_FC_LocalSimple_mean1_tauresrat #> 162              catch22_FC_LocalSimple_mean1_tauresrat #> 163              catch22_FC_LocalSimple_mean1_tauresrat #> 164              catch22_FC_LocalSimple_mean1_tauresrat #> 165              catch22_FC_LocalSimple_mean1_tauresrat #> 166                 catch22_FC_LocalSimple_mean3_stderr #> 167                 catch22_FC_LocalSimple_mean3_stderr #> 168                 catch22_FC_LocalSimple_mean3_stderr #> 169                 catch22_FC_LocalSimple_mean3_stderr #> 170                 catch22_FC_LocalSimple_mean3_stderr #> 171                 catch22_FC_LocalSimple_mean3_stderr #> 172                 catch22_FC_LocalSimple_mean3_stderr #> 173                 catch22_FC_LocalSimple_mean3_stderr #> 174                 catch22_FC_LocalSimple_mean3_stderr #> 175                 catch22_FC_LocalSimple_mean3_stderr #> 176                 catch22_FC_LocalSimple_mean3_stderr #> 177     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 178     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 179     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 180     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 181     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 182     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 183     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 184     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 185     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 186     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi #> 187                        catch22_MD_hrv_classic_pnn40 #> 188                        catch22_MD_hrv_classic_pnn40 #> 189                        catch22_MD_hrv_classic_pnn40 #> 190                        catch22_MD_hrv_classic_pnn40 #> 191                        catch22_MD_hrv_classic_pnn40 #> 192                        catch22_MD_hrv_classic_pnn40 #> 193                        catch22_MD_hrv_classic_pnn40 #> 194                        catch22_MD_hrv_classic_pnn40 #> 195                        catch22_MD_hrv_classic_pnn40 #> 196                   catch22_PD_PeriodicityWang_th0_01 #> 197                   catch22_PD_PeriodicityWang_th0_01 #> 198                   catch22_PD_PeriodicityWang_th0_01 #> 199                   catch22_PD_PeriodicityWang_th0_01 #> 200                   catch22_PD_PeriodicityWang_th0_01 #> 201                   catch22_PD_PeriodicityWang_th0_01 #> 202                   catch22_PD_PeriodicityWang_th0_01 #> 203                   catch22_PD_PeriodicityWang_th0_01 #> 204            catch22_SB_BinaryStats_diff_longstretch0 #> 205            catch22_SB_BinaryStats_diff_longstretch0 #> 206            catch22_SB_BinaryStats_diff_longstretch0 #> 207            catch22_SB_BinaryStats_diff_longstretch0 #> 208            catch22_SB_BinaryStats_diff_longstretch0 #> 209            catch22_SB_BinaryStats_diff_longstretch0 #> 210            catch22_SB_BinaryStats_diff_longstretch0 #> 211            catch22_SB_BinaryStats_mean_longstretch1 #> 212            catch22_SB_BinaryStats_mean_longstretch1 #> 213            catch22_SB_BinaryStats_mean_longstretch1 #> 214            catch22_SB_BinaryStats_mean_longstretch1 #> 215            catch22_SB_BinaryStats_mean_longstretch1 #> 216            catch22_SB_BinaryStats_mean_longstretch1 #> 217                   catch22_SB_MotifThree_quantile_hh #> 218                   catch22_SB_MotifThree_quantile_hh #> 219                   catch22_SB_MotifThree_quantile_hh #> 220                   catch22_SB_MotifThree_quantile_hh #> 221                   catch22_SB_MotifThree_quantile_hh #> 222          catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 223          catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 224          catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 225          catch22_SB_TransitionMatrix_3ac_sumdiagcov #> 226      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 227      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 228      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 #> 229 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 230 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 #> 231            catch22_SP_Summaries_welch_rect_area_5_1 #>                                                 names_b   metric  t_statistic #> 1                                catch22_CO_FirstMin_ac accuracy   0.75296741 #> 2                      catch22_CO_HistogramAMI_even_2_5 accuracy   1.08333827 #> 3                                     catch22_CO_f1ecac accuracy   1.97554451 #> 4                                 catch22_CO_trev_1_num accuracy   0.96153135 #> 5                           catch22_DN_HistogramMode_10 accuracy   3.19633884 #> 6                            catch22_DN_HistogramMode_5 accuracy   3.39891847 #> 7                 catch22_DN_OutlierInclude_n_001_mdrmd accuracy   1.62980426 #> 8                 catch22_DN_OutlierInclude_p_001_mdrmd accuracy   2.47867980 #> 9                catch22_FC_LocalSimple_mean1_tauresrat accuracy  -0.26227005 #> 10                  catch22_FC_LocalSimple_mean3_stderr accuracy  -2.98095404 #> 11      catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy   1.01533492 #> 12                         catch22_MD_hrv_classic_pnn40 accuracy   3.49402992 #> 13                    catch22_PD_PeriodicityWang_th0_01 accuracy   0.34800321 #> 14             catch22_SB_BinaryStats_diff_longstretch0 accuracy   0.66850802 #> 15             catch22_SB_BinaryStats_mean_longstretch1 accuracy  -0.76857337 #> 16                    catch22_SB_MotifThree_quantile_hh accuracy  -0.76938347 #> 17           catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -0.12158089 #> 18       catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   5.33200934 #> 19  catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   2.40495371 #> 20             catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -3.31828955 #> 21             catch22_SP_Summaries_welch_rect_centroid accuracy  -3.18258253 #> 22                     catch22_CO_HistogramAMI_even_2_5 accuracy   0.21090438 #> 23                                    catch22_CO_f1ecac accuracy   1.18566142 #> 24                                catch22_CO_trev_1_num accuracy   0.24435985 #> 25                          catch22_DN_HistogramMode_10 accuracy   3.01986092 #> 26                           catch22_DN_HistogramMode_5 accuracy   3.32719606 #> 27                catch22_DN_OutlierInclude_n_001_mdrmd accuracy   0.83103119 #> 28                catch22_DN_OutlierInclude_p_001_mdrmd accuracy   1.28263841 #> 29               catch22_FC_LocalSimple_mean1_tauresrat accuracy  -0.92064437 #> 30                  catch22_FC_LocalSimple_mean3_stderr accuracy  -3.60486756 #> 31      catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy   0.14136348 #> 32                         catch22_MD_hrv_classic_pnn40 accuracy   3.38796837 #> 33                    catch22_PD_PeriodicityWang_th0_01 accuracy  -0.46469967 #> 34             catch22_SB_BinaryStats_diff_longstretch0 accuracy  -0.03898031 #> 35             catch22_SB_BinaryStats_mean_longstretch1 accuracy  -1.90860756 #> 36                    catch22_SB_MotifThree_quantile_hh accuracy  -1.56274416 #> 37           catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -1.00946589 #> 38       catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   4.59304307 #> 39  catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   1.95089972 #> 40             catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -4.50845559 #> 41             catch22_SP_Summaries_welch_rect_centroid accuracy  -4.19208994 #> 42                                    catch22_CO_f1ecac accuracy   0.94338728 #> 43                                catch22_CO_trev_1_num accuracy   0.02709776 #> 44                          catch22_DN_HistogramMode_10 accuracy   2.73396529 #> 45                           catch22_DN_HistogramMode_5 accuracy   2.39025792 #> 46                catch22_DN_OutlierInclude_n_001_mdrmd accuracy   0.53819801 #> 47                catch22_DN_OutlierInclude_p_001_mdrmd accuracy   1.09751228 #> 48               catch22_FC_LocalSimple_mean1_tauresrat accuracy  -0.92957795 #> 49                  catch22_FC_LocalSimple_mean3_stderr accuracy  -5.11407639 #> 50      catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy  -0.11243520 #> 51                         catch22_MD_hrv_classic_pnn40 accuracy   2.51079445 #> 52                    catch22_PD_PeriodicityWang_th0_01 accuracy  -0.53268064 #> 53             catch22_SB_BinaryStats_diff_longstretch0 accuracy  -0.22883664 #> 54             catch22_SB_BinaryStats_mean_longstretch1 accuracy  -1.65913524 #> 55                    catch22_SB_MotifThree_quantile_hh accuracy  -1.60635679 #> 56           catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -1.12621588 #> 57       catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   3.97013254 #> 58  catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   1.40150009 #> 59             catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -4.37949633 #> 60             catch22_SP_Summaries_welch_rect_centroid accuracy  -3.80917810 #> 61                                catch22_CO_trev_1_num accuracy  -0.72612705 #> 62                          catch22_DN_HistogramMode_10 accuracy   2.30702694 #> 63                           catch22_DN_HistogramMode_5 accuracy   1.71907932 #> 64                catch22_DN_OutlierInclude_n_001_mdrmd accuracy  -0.24400084 #> 65                catch22_DN_OutlierInclude_p_001_mdrmd accuracy   0.39105388 #> 66               catch22_FC_LocalSimple_mean1_tauresrat accuracy  -1.93729161 #> 67                  catch22_FC_LocalSimple_mean3_stderr accuracy  -5.97982564 #> 68      catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy  -1.29656944 #> 69                         catch22_MD_hrv_classic_pnn40 accuracy   2.07662045 #> 70                    catch22_PD_PeriodicityWang_th0_01 accuracy  -1.69837987 #> 71             catch22_SB_BinaryStats_diff_longstretch0 accuracy  -1.18633040 #> 72             catch22_SB_BinaryStats_mean_longstretch1 accuracy  -3.48579679 #> 73                    catch22_SB_MotifThree_quantile_hh accuracy  -3.31387495 #> 74           catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -2.51130741 #> 75       catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   3.71807001 #> 76  catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   0.42447372 #> 77             catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -6.04036854 #> 78             catch22_SP_Summaries_welch_rect_centroid accuracy  -6.21084705 #> 79                          catch22_DN_HistogramMode_10 accuracy   2.35516337 #> 80                           catch22_DN_HistogramMode_5 accuracy   2.34524148 #> 81                catch22_DN_OutlierInclude_n_001_mdrmd accuracy   0.49923237 #> 82                catch22_DN_OutlierInclude_p_001_mdrmd accuracy   0.93292832 #> 83               catch22_FC_LocalSimple_mean1_tauresrat accuracy  -0.89983194 #> 84                  catch22_FC_LocalSimple_mean3_stderr accuracy  -3.78096633 #> 85      catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy  -0.12283505 #> 86                         catch22_MD_hrv_classic_pnn40 accuracy   2.39636832 #> 87                    catch22_PD_PeriodicityWang_th0_01 accuracy  -0.55991067 #> 88             catch22_SB_BinaryStats_diff_longstretch0 accuracy  -0.25214395 #> 89             catch22_SB_BinaryStats_mean_longstretch1 accuracy  -1.85794529 #> 90                    catch22_SB_MotifThree_quantile_hh accuracy  -1.61659274 #> 91           catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -1.09422115 #> 92       catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   3.55882716 #> 93  catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   1.41418012 #> 94             catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -4.17083538 #> 95             catch22_SP_Summaries_welch_rect_centroid accuracy  -3.84957698 #> 96                           catch22_DN_HistogramMode_5 accuracy   0.22589022 #> 97                catch22_DN_OutlierInclude_n_001_mdrmd accuracy  -1.95136225 #> 98                catch22_DN_OutlierInclude_p_001_mdrmd accuracy  -1.16284046 #> 99               catch22_FC_LocalSimple_mean1_tauresrat accuracy  -2.86919477 #> 100                 catch22_FC_LocalSimple_mean3_stderr accuracy  -6.74899097 #> 101     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy  -2.64831408 #> 102                        catch22_MD_hrv_classic_pnn40 accuracy   0.10703650 #> 103                   catch22_PD_PeriodicityWang_th0_01 accuracy  -3.14600710 #> 104            catch22_SB_BinaryStats_diff_longstretch0 accuracy  -3.24922247 #> 105            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -4.76360731 #> 106                   catch22_SB_MotifThree_quantile_hh accuracy  -4.08614548 #> 107          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -3.92734968 #> 108      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   1.87224339 #> 109 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy  -1.64787279 #> 110            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -6.35580133 #> 111            catch22_SP_Summaries_welch_rect_centroid accuracy  -7.74516759 #> 112               catch22_DN_OutlierInclude_n_001_mdrmd accuracy  -2.48684377 #> 113               catch22_DN_OutlierInclude_p_001_mdrmd accuracy  -1.24109352 #> 114              catch22_FC_LocalSimple_mean1_tauresrat accuracy  -2.78171841 #> 115                 catch22_FC_LocalSimple_mean3_stderr accuracy  -6.05701411 #> 116     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy  -2.61311106 #> 117                        catch22_MD_hrv_classic_pnn40 accuracy  -0.10306771 #> 118                   catch22_PD_PeriodicityWang_th0_01 accuracy  -3.05856808 #> 119            catch22_SB_BinaryStats_diff_longstretch0 accuracy  -3.03249049 #> 120            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -3.95397415 #> 121                   catch22_SB_MotifThree_quantile_hh accuracy  -4.02829035 #> 122          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -3.76719642 #> 123      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   1.52975274 #> 124 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy  -1.60978037 #> 125            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -6.05592081 #> 126            catch22_SP_Summaries_welch_rect_centroid accuracy  -6.04447272 #> 127               catch22_DN_OutlierInclude_p_001_mdrmd accuracy   0.61399189 #> 128              catch22_FC_LocalSimple_mean1_tauresrat accuracy  -1.45864910 #> 129                 catch22_FC_LocalSimple_mean3_stderr accuracy  -4.28649640 #> 130     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy  -0.64321226 #> 131                        catch22_MD_hrv_classic_pnn40 accuracy   2.09440732 #> 132                   catch22_PD_PeriodicityWang_th0_01 accuracy  -1.12555662 #> 133            catch22_SB_BinaryStats_diff_longstretch0 accuracy  -0.68391661 #> 134            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -2.32704800 #> 135                   catch22_SB_MotifThree_quantile_hh accuracy  -2.33953747 #> 136          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -1.74495766 #> 137      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   4.10362690 #> 138 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   0.69100402 #> 139            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -4.15265036 #> 140            catch22_SP_Summaries_welch_rect_centroid accuracy  -4.73423084 #> 141              catch22_FC_LocalSimple_mean1_tauresrat accuracy  -1.88011098 #> 142                 catch22_FC_LocalSimple_mean3_stderr accuracy  -4.90415523 #> 143     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy  -1.38582973 #> 144                        catch22_MD_hrv_classic_pnn40 accuracy   1.48466045 #> 145                   catch22_PD_PeriodicityWang_th0_01 accuracy  -1.94856722 #> 146            catch22_SB_BinaryStats_diff_longstretch0 accuracy  -1.35408581 #> 147            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -2.64521246 #> 148                   catch22_SB_MotifThree_quantile_hh accuracy  -2.94201062 #> 149          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -2.62787977 #> 150      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   4.05183722 #> 151 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy  -0.03898031 #> 152            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -4.96707040 #> 153            catch22_SP_Summaries_welch_rect_centroid accuracy  -7.25412273 #> 154                 catch22_FC_LocalSimple_mean3_stderr accuracy  -1.76168600 #> 155     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy   1.09043116 #> 156                        catch22_MD_hrv_classic_pnn40 accuracy   3.18408967 #> 157                   catch22_PD_PeriodicityWang_th0_01 accuracy   0.63051185 #> 158            catch22_SB_BinaryStats_diff_longstretch0 accuracy   0.85258097 #> 159            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -0.38961860 #> 160                   catch22_SB_MotifThree_quantile_hh accuracy  -0.17888390 #> 161          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy   0.24746480 #> 162      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   3.67649901 #> 163 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   1.77310213 #> 164            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -2.17913262 #> 165            catch22_SP_Summaries_welch_rect_centroid accuracy  -2.10914089 #> 166     catch22_IN_AutoMutualInfoStats_40_gaussian_fmmi accuracy   4.95068327 #> 167                        catch22_MD_hrv_classic_pnn40 accuracy   6.07008173 #> 168                   catch22_PD_PeriodicityWang_th0_01 accuracy   2.65839790 #> 169            catch22_SB_BinaryStats_diff_longstretch0 accuracy   4.10174761 #> 170            catch22_SB_BinaryStats_mean_longstretch1 accuracy   1.50106765 #> 171                   catch22_SB_MotifThree_quantile_hh accuracy   2.66602203 #> 172          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy   2.70416061 #> 173      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   8.44672214 #> 174 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   5.77419355 #> 175            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -1.17047810 #> 176            catch22_SP_Summaries_welch_rect_centroid accuracy  -0.51118432 #> 177                        catch22_MD_hrv_classic_pnn40 accuracy   3.71703876 #> 178                   catch22_PD_PeriodicityWang_th0_01 accuracy  -0.57545176 #> 179            catch22_SB_BinaryStats_diff_longstretch0 accuracy  -0.17104543 #> 180            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -1.95609865 #> 181                   catch22_SB_MotifThree_quantile_hh accuracy  -2.26777137 #> 182          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -1.38454872 #> 183      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   4.95295891 #> 184 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   1.51190241 #> 185            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -5.45321637 #> 186            catch22_SP_Summaries_welch_rect_centroid accuracy  -4.75480591 #> 187                   catch22_PD_PeriodicityWang_th0_01 accuracy  -3.24016070 #> 188            catch22_SB_BinaryStats_diff_longstretch0 accuracy  -2.33658231 #> 189            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -5.88651733 #> 190                   catch22_SB_MotifThree_quantile_hh accuracy  -5.04322108 #> 191          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -4.36448855 #> 192      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   1.81756791 #> 193 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy  -1.58282915 #> 194            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -6.74526009 #> 195            catch22_SP_Summaries_welch_rect_centroid accuracy  -8.08131208 #> 196            catch22_SB_BinaryStats_diff_longstretch0 accuracy   0.42829019 #> 197            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -1.38613962 #> 198                   catch22_SB_MotifThree_quantile_hh accuracy  -1.02836623 #> 199          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -0.53358456 #> 200      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   4.76809314 #> 201 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   1.70091294 #> 202            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -3.27633760 #> 203            catch22_SP_Summaries_welch_rect_centroid accuracy  -5.00381897 #> 204            catch22_SB_BinaryStats_mean_longstretch1 accuracy  -1.54256331 #> 205                   catch22_SB_MotifThree_quantile_hh accuracy  -1.37347648 #> 206          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy  -0.88939782 #> 207      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   4.33683171 #> 208 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   1.44581204 #> 209            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -4.04691307 #> 210            catch22_SP_Summaries_welch_rect_centroid accuracy  -4.02347363 #> 211                   catch22_SB_MotifThree_quantile_hh accuracy   0.32055168 #> 212          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy   0.84654862 #> 213      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   6.43834065 #> 214 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   3.20730287 #> 215            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -2.24432403 #> 216            catch22_SP_Summaries_welch_rect_centroid accuracy  -2.12960572 #> 217          catch22_SB_TransitionMatrix_3ac_sumdiagcov accuracy   0.77828964 #> 218      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   6.20792599 #> 219 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   3.10224421 #> 220            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -3.61739484 #> 221            catch22_SP_Summaries_welch_rect_centroid accuracy  -3.24860392 #> 222      catch22_SC_FluctAnal_2_dfa_50_1_2_logi_prop_r1 accuracy   5.57799108 #> 223 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy   2.63809695 #> 224            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -3.79066753 #> 225            catch22_SP_Summaries_welch_rect_centroid accuracy  -3.64669387 #> 226 catch22_SC_FluctAnal_2_rsrangefit_50_1_logi_prop_r1 accuracy  -3.16545594 #> 227            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -8.56311244 #> 228            catch22_SP_Summaries_welch_rect_centroid accuracy -10.64629836 #> 229            catch22_SP_Summaries_welch_rect_area_5_1 accuracy  -5.68916885 #> 230            catch22_SP_Summaries_welch_rect_centroid accuracy  -5.34708853 #> 231            catch22_SP_Summaries_welch_rect_centroid accuracy   0.47948223 #>          p.value  p_value_adj #> 1   2.287694e-01 2.287694e-01 #> 2   1.437918e-01 1.437918e-01 #> 3   2.889567e-02 2.889567e-02 #> 4   1.721172e-01 1.721172e-01 #> 5   1.674808e-03 1.674808e-03 #> 6   9.930473e-04 9.930473e-04 #> 7   5.697927e-02 5.697927e-02 #> 8   9.624426e-03 9.624426e-03 #> 9   3.974837e-01 3.974837e-01 #> 10  2.883258e-03 2.883258e-03 #> 11  1.591714e-01 1.591714e-01 #> 12  7.742752e-04 7.742752e-04 #> 13  3.651756e-01 3.651756e-01 #> 14  2.545480e-01 2.545480e-01 #> 15  2.241804e-01 2.241804e-01 #> 16  2.239437e-01 2.239437e-01 #> 17  4.520348e-01 4.520348e-01 #> 18  5.041270e-06 5.041270e-06 #> 19  1.139088e-02 1.139088e-02 #> 20  1.224226e-03 1.224226e-03 #> 21  1.734637e-03 1.734637e-03 #> 22  4.172184e-01 4.172184e-01 #> 23  1.226903e-01 1.226903e-01 #> 24  4.043364e-01 4.043364e-01 #> 25  2.616490e-03 2.616490e-03 #> 26  1.196343e-03 1.196343e-03 #> 27  2.063731e-01 2.063731e-01 #> 28  1.048882e-01 1.048882e-01 #> 29  1.824158e-01 1.824158e-01 #> 30  5.779221e-04 5.779221e-04 #> 31  4.442801e-01 4.442801e-01 #> 32  1.021772e-03 1.021772e-03 #> 33  3.228084e-01 3.228084e-01 #> 34  4.845866e-01 4.845866e-01 #> 35  3.312747e-02 3.312747e-02 #> 36  6.448041e-02 6.448041e-02 #> 37  1.605501e-01 1.605501e-01 #> 38  3.924723e-05 3.924723e-05 #> 39  3.039542e-02 3.039542e-02 #> 40  4.959481e-05 4.959481e-05 #> 41  1.184620e-04 1.184620e-04 #> 42  1.766382e-01 1.766382e-01 #> 43  4.892837e-01 4.892837e-01 #> 44  5.277340e-03 5.277340e-03 #> 45  1.177663e-02 1.177663e-02 #> 46  2.972737e-01 2.972737e-01 #> 47  1.407236e-01 1.407236e-01 #> 48  1.801317e-01 1.801317e-01 #> 49  9.236490e-06 9.236490e-06 #> 50  4.556265e-01 4.556265e-01 #> 51  8.936745e-03 8.936745e-03 #> 52  2.991565e-01 2.991565e-01 #> 53  4.103010e-01 4.103010e-01 #> 54  5.393334e-02 5.393334e-02 #> 55  5.951554e-02 5.951554e-02 #> 56  1.346540e-01 1.346540e-01 #> 57  2.169187e-04 2.169187e-04 #> 58  8.583836e-02 8.583836e-02 #> 59  7.079438e-05 7.079438e-05 #> 60  3.349748e-04 3.349748e-04 #> 61  2.367905e-01 2.367905e-01 #> 62  1.419509e-02 1.419509e-02 #> 63  4.812937e-02 4.812937e-02 #> 64  4.044741e-01 4.044741e-01 #> 65  3.493079e-01 3.493079e-01 #> 66  3.125227e-02 3.125227e-02 #> 67  8.417296e-07 8.417296e-07 #> 68  1.025010e-01 1.025010e-01 #> 69  2.340184e-02 2.340184e-02 #> 70  5.007122e-02 5.007122e-02 #> 71  1.225602e-01 1.225602e-01 #> 72  7.911989e-04 7.911989e-04 #> 73  1.238277e-03 1.238277e-03 #> 74  8.926138e-03 8.926138e-03 #> 75  4.276000e-04 4.276000e-04 #> 76  3.371765e-01 3.371765e-01 #> 77  7.129435e-07 7.129435e-07 #> 78  4.473248e-07 4.473248e-07 #> 79  1.274650e-02 1.274650e-02 #> 80  1.303360e-02 1.303360e-02 #> 81  3.106910e-01 3.106910e-01 #> 82  1.792799e-01 1.792799e-01 #> 83  1.878107e-01 1.878107e-01 #> 84  3.613339e-04 3.613339e-04 #> 85  4.515425e-01 4.515425e-01 #> 86  1.161481e-02 1.161481e-02 #> 87  2.899200e-01 2.899200e-01 #> 88  4.013541e-01 4.013541e-01 #> 89  3.667820e-02 3.667820e-02 #> 90  5.839710e-02 5.839710e-02 #> 91  1.414318e-01 1.414318e-01 #> 92  6.527835e-04 6.527835e-04 #> 93  8.397792e-02 8.397792e-02 #> 94  1.255579e-04 1.255579e-04 #> 95  3.004738e-04 3.004738e-04 #> 96  4.114356e-01 4.114356e-01 #> 97  3.036666e-02 3.036666e-02 #> 98  1.271870e-01 1.271870e-01 #> 99  3.800227e-03 3.800227e-03 #> 100 1.044229e-07 1.044229e-07 #> 101 6.474739e-03 6.474739e-03 #> 102 4.577485e-01 4.577485e-01 #> 103 1.903822e-03 1.903822e-03 #> 104 1.462697e-03 1.462697e-03 #> 105 2.445920e-05 2.445920e-05 #> 106 1.582338e-04 1.582338e-04 #> 107 2.435687e-04 2.435687e-04 #> 108 3.564433e-02 3.564433e-02 #> 109 5.508649e-02 5.508649e-02 #> 110 3.015109e-07 3.015109e-07 #> 111 7.665682e-09 7.665682e-09 #> 112 9.445139e-03 9.445139e-03 #> 113 1.122586e-01 1.122586e-01 #> 114 4.703091e-03 4.703091e-03 #> 115 6.811571e-07 6.811571e-07 #> 116 7.036628e-03 7.036628e-03 #> 117 4.593093e-01 4.593093e-01 #> 118 2.374475e-03 2.374475e-03 #> 119 2.535057e-03 2.535057e-03 #> 120 2.266297e-04 2.266297e-04 #> 121 1.852307e-04 1.852307e-04 #> 122 3.749248e-04 3.749248e-04 #> 123 6.845701e-02 6.845701e-02 #> 124 5.913951e-02 5.913951e-02 #> 125 6.832002e-07 6.832002e-07 #> 126 7.049696e-07 7.049696e-07 #> 127 2.720038e-01 2.720038e-01 #> 128 7.770425e-02 7.770425e-02 #> 129 9.143703e-05 9.143703e-05 #> 130 2.625701e-01 2.625701e-01 #> 131 2.253699e-02 2.253699e-02 #> 132 1.347912e-01 1.347912e-01 #> 133 2.497284e-01 2.497284e-01 #> 134 1.357535e-02 1.357535e-02 #> 135 1.320130e-02 1.320130e-02 #> 136 4.579124e-02 4.579124e-02 #> 137 1.508665e-04 1.508665e-04 #> 138 2.475288e-01 2.475288e-01 #> 139 1.319604e-04 1.319604e-04 #> 140 2.653670e-05 2.653670e-05 #> 141 3.508622e-02 3.508622e-02 #> 142 1.655450e-05 1.655450e-05 #> 143 8.818222e-02 8.818222e-02 #> 144 7.421167e-02 7.421167e-02 #> 145 3.054082e-02 3.054082e-02 #> 146 9.308376e-02 9.308376e-02 #> 147 6.522516e-03 6.522516e-03 #> 148 3.175970e-03 3.175970e-03 #> 149 6.795598e-03 6.795598e-03 #> 150 1.737363e-04 1.737363e-04 #> 151 4.845866e-01 4.845866e-01 #> 152 1.389886e-05 1.389886e-05 #> 153 2.737828e-08 2.737828e-08 #> 154 4.433150e-02 4.433150e-02 #> 155 1.422505e-01 1.422505e-01 #> 156 1.727984e-03 1.727984e-03 #> 157 2.666487e-01 2.666487e-01 #> 158 2.004392e-01 2.004392e-01 #> 159 3.498326e-01 3.498326e-01 #> 160 4.296364e-01 4.296364e-01 #> 161 4.031461e-01 4.031461e-01 #> 162 4.777538e-04 4.777538e-04 #> 163 4.335812e-02 4.335812e-02 #> 164 1.879514e-02 1.879514e-02 #> 165 2.184220e-02 2.184220e-02 #> 166 1.454655e-05 1.454655e-05 #> 167 6.572088e-07 6.572088e-07 #> 168 6.321650e-03 6.321650e-03 #> 169 1.516420e-04 1.516420e-04 #> 170 7.207429e-02 7.207429e-02 #> 171 6.208147e-03 6.208147e-03 #> 172 5.668336e-03 5.668336e-03 #> 173 1.310545e-09 1.310545e-09 #> 174 1.482208e-06 1.482208e-06 #> 175 1.256688e-01 1.256688e-01 #> 176 3.065461e-01 3.065461e-01 #> 177 4.287797e-04 4.287797e-04 #> 178 2.847122e-01 2.847122e-01 #> 179 4.326880e-01 4.326880e-01 #> 180 3.007351e-02 3.007351e-02 #> 181 1.548516e-02 1.548516e-02 #> 182 8.837602e-02 8.837602e-02 #> 183 1.445483e-05 1.445483e-05 #> 184 7.069026e-02 7.069026e-02 #> 185 3.601491e-06 3.601491e-06 #> 186 2.506403e-05 2.506403e-05 #> 187 1.497114e-03 1.497114e-03 #> 188 1.328895e-02 1.328895e-02 #> 189 1.087762e-06 1.087762e-06 #> 190 1.124730e-05 1.124730e-05 #> 191 7.378230e-05 7.378230e-05 #> 192 3.973819e-02 3.973819e-02 #> 193 6.215333e-02 6.215333e-02 #> 194 1.054713e-07 1.054713e-07 #> 195 3.262697e-09 3.262697e-09 #> 196 3.358021e-01 3.358021e-01 #> 197 8.813538e-02 8.813538e-02 #> 198 1.561395e-01 1.561395e-01 #> 199 2.988477e-01 2.988477e-01 #> 200 2.415654e-05 2.415654e-05 #> 201 4.983011e-02 4.983011e-02 #> 202 1.364187e-03 1.364187e-03 #> 203 1.254923e-05 1.254923e-05 #> 204 6.688985e-02 6.688985e-02 #> 205 9.006507e-02 9.006507e-02 #> 206 1.905540e-01 1.905540e-01 #> 207 7.961900e-05 7.961900e-05 #> 208 7.947566e-02 7.947566e-02 #> 209 1.760805e-04 1.760805e-04 #> 210 1.876725e-04 1.876725e-04 #> 211 3.754243e-01 3.754243e-01 #> 212 2.020893e-01 2.020893e-01 #> 213 2.410494e-07 2.410494e-07 #> 214 1.628544e-03 1.628544e-03 #> 215 1.630533e-02 1.630533e-02 #> 216 2.090873e-02 2.090873e-02 #> 217 2.213512e-01 2.213512e-01 #> 218 4.509030e-07 4.509030e-07 #> 219 2.126998e-03 2.126998e-03 #> 220 5.590409e-04 5.590409e-04 #> 221 1.465022e-03 1.465022e-03 #> 222 2.548917e-06 2.548917e-06 #> 223 6.633364e-03 6.633364e-03 #> 224 3.520489e-04 3.520489e-04 #> 225 5.171927e-04 5.171927e-04 #> 226 1.811986e-03 1.811986e-03 #> 227 9.837052e-10 9.837052e-10 #> 228 7.821460e-12 7.821460e-12 #> 229 1.874286e-06 1.874286e-06 #> 230 4.834585e-06 4.834585e-06 #> 231 3.175960e-01 3.175960e-01 # }"},{"path":"https://hendersontrent.github.io/theft/reference/compute_top_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Return an object containing results from top-performing features on a classification task — compute_top_features","title":"Return an object containing results from top-performing features on a classification task — compute_top_features","text":"Return object containing results top-performing features classification task","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compute_top_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return an object containing results from top-performing features on a classification task — compute_top_features","text":"","code":"compute_top_features(   data,   num_features = 40,   normalise_violin_plots = FALSE,   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\"),   cor_method = c(\"pearson\", \"spearman\"),   test_method = \"gaussprRadial\",   clust_method = c(\"average\", \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"mcquitty\",     \"median\", \"centroid\"),   use_balanced_accuracy = FALSE,   use_k_fold = FALSE,   num_folds = 10,   use_empirical_null = FALSE,   null_testing_method = c(\"ModelFreeShuffles\", \"NullModelFits\"),   p_value_method = c(\"empirical\", \"gaussian\"),   num_permutations = 50,   pool_empirical_null = FALSE,   seed = 123 )"},{"path":"https://hendersontrent.github.io/theft/reference/compute_top_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return an object containing results from top-performing features on a classification task — compute_top_features","text":"data feature_calculations object containing raw feature matrix produced calculate_features num_features integer denoting number top features retain explore. Defaults 40 normalise_violin_plots Boolean whether normalise features plotting. Defaults FALSE method rescaling/normalising method apply violin plots. Defaults \"z-score\" cor_method string denoting correlation method use. Defaults \"pearson\" test_method string specifying algorithm use quantifying class separation. Defaults \"gaussprRadial\". either \"t-test\", \"wilcox\", \"binomial logistic\" two-class problems obtain exact statistics, valid caret classification model everything else clust_method string denoting hierarchical clustering method use pairwise correlation plot. Defaults \"average\" use_balanced_accuracy Boolean specifying whether use balanced accuracy summary metric caret model training. Defaults FALSE use_k_fold Boolean specifying whether use k-fold procedures generating distribution classification accuracy estimates caret model specified test_method. Defaults  FALSE num_folds integer specifying number k-folds perform use_k_fold set TRUE. Defaults 10 use_empirical_null Boolean specifying whether use empirical null procedures compute p-values caret model specified test_method. Defaults FALSE null_testing_method string specifying type statistical method use calculate p-values. Defaults \"ModelFreeShuffles\" p_value_method string specifying method calculating p-values. Defaults \"empirical\" num_permutations integer specifying number class label shuffles perform use_empirical_null TRUE. Defaults 50 pool_empirical_null Boolean specifying whether use pooled empirical null distribution features features' individual empirical null distribution caret model specified test_method use_empirical_null TRUE. Defaults FALSE seed integer denoting fixed number R's random number generator ensure reproducibility","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compute_top_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return an object containing results from top-performing features on a classification task — compute_top_features","text":"object class list containing data.frame results, ggplot feature x feature matrix plot, ggplot violin plot","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compute_top_features.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Return an object containing results from top-performing features on a classification task — compute_top_features","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/compute_top_features.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return an object containing results from top-performing features on a classification task — compute_top_features","text":"","code":"# \\donttest{ featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.    compute_top_features(featMat,   num_features = 10,   normalise_violin_plots = FALSE,   method = \"RobustSigmoid\",   cor_method = \"pearson\",   test_method = \"gaussprRadial\",   clust_method = \"average\",   use_balanced_accuracy = FALSE,   use_k_fold = FALSE,   num_folds = 10,   use_empirical_null = TRUE,   null_testing_method = \"ModelFreeShuffles\",   p_value_method = \"gaussian\",   num_permutations = 100,   pool_empirical_null = FALSE,   seed = 123)  #> This will take a while. Great reason to go grab a coffee and relax ^_^ #>  #> Selecting top features using p-value. #> $ResultsTable #>                                         feature  accuracy p_value_accuracy #> 1                             catch22_co_f1ecac 0.7666667     1.010919e-96 #> 2      catch22_sp_summaries_welch_rect_area_5_1 0.7055556     1.988011e-78 #> 3          catch22_fc_local_simple_mean3_stderr 0.7055556     1.988011e-78 #> 4      catch22_sp_summaries_welch_rect_centroid 0.6833333     2.923146e-72 #> 5                       catch22_co_first_min_ac 0.6222222     1.240886e-56 #> 6     catch22_sb_binary_stats_mean_longstretch1 0.6166667     2.615618e-55 #> 7                         catch22_co_trev_1_num 0.6055556     1.039544e-52 #> 8            catch22_sb_motif_three_quantile_hh 0.6055556     1.039544e-52 #> 9  catch22_co_embed2_dist_tau_d_expfit_meandiff 0.5611111     5.869639e-43 #> 10            catch22_co_histogram_ami_even_2_5 0.5555556     8.222765e-42 #>    classifier_name               statistic_name #> 1    gaussprRadial Mean classification accuracy #> 2    gaussprRadial Mean classification accuracy #> 3    gaussprRadial Mean classification accuracy #> 4    gaussprRadial Mean classification accuracy #> 5    gaussprRadial Mean classification accuracy #> 6    gaussprRadial Mean classification accuracy #> 7    gaussprRadial Mean classification accuracy #> 8    gaussprRadial Mean classification accuracy #> 9    gaussprRadial Mean classification accuracy #> 10   gaussprRadial Mean classification accuracy #>  #> $FeatureFeatureCorrelationPlot  #>  #> $ViolinPlots  #>  # }"},{"path":"https://hendersontrent.github.io/theft/reference/demo_multi_outputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Computed values for multi-feature classification results for use in vignette — demo_multi_outputs","title":"Computed values for multi-feature classification results for use in vignette — demo_multi_outputs","text":"Format :","code":""},{"path":"https://hendersontrent.github.io/theft/reference/demo_multi_outputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computed values for multi-feature classification results for use in vignette — demo_multi_outputs","text":"","code":"demo_multi_outputs"},{"path":"https://hendersontrent.github.io/theft/reference/demo_multi_outputs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Computed values for multi-feature classification results for use in vignette — demo_multi_outputs","text":"list object FeatureSetResultsPlot ggplot comparing feature set classification accuracy TestStatistics data.frame test statistics RawClassificationResults data.frame raw classification accuracy","code":""},{"path":"https://hendersontrent.github.io/theft/reference/demo_outputs.html","id":null,"dir":"Reference","previous_headings":"","what":"Computed values for top features results for use in vignette — demo_outputs","title":"Computed values for top features results for use in vignette — demo_outputs","text":"Format :","code":""},{"path":"https://hendersontrent.github.io/theft/reference/demo_outputs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Computed values for top features results for use in vignette — demo_outputs","text":"","code":"demo_outputs"},{"path":"https://hendersontrent.github.io/theft/reference/demo_outputs.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Computed values for top features results for use in vignette — demo_outputs","text":"list object ResultsTable data.frame results top features FeatureFeatureCorrelationPlot ggplot heatmap feature-feature correlation matrix ViolinPlots ggplot distributions rendered violins top features","code":""},{"path":"https://hendersontrent.github.io/theft/reference/feature_list.html","id":null,"dir":"Reference","previous_headings":"","what":"All features available in theft in tidy format — feature_list","title":"All features available in theft in tidy format — feature_list","text":"variables include:","code":""},{"path":"https://hendersontrent.github.io/theft/reference/feature_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"All features available in theft in tidy format — feature_list","text":"","code":"feature_list"},{"path":"https://hendersontrent.github.io/theft/reference/feature_list.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"All features available in theft in tidy format — feature_list","text":"tidy data frame 2 variables: feature_set Name set feature feature Name feature","code":""},{"path":"https://hendersontrent.github.io/theft/reference/filter_duplicates.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function to remove duplicate features that exist in multiple feature sets — filter_duplicates","title":"Helper function to remove duplicate features that exist in multiple feature sets — filter_duplicates","text":"Helper function remove duplicate features exist multiple feature sets","code":""},{"path":"https://hendersontrent.github.io/theft/reference/filter_duplicates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function to remove duplicate features that exist in multiple feature sets — filter_duplicates","text":"","code":"filter_duplicates(data, preference = c(\"feasts\", \"tsfeatures\", \"Kats\"))"},{"path":"https://hendersontrent.github.io/theft/reference/filter_duplicates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper function to remove duplicate features that exist in multiple feature sets — filter_duplicates","text":"data feature_calculations object containing raw feature matrix produced calculate_features preference character denoting feature set keep (meaning others filtered ) \"feasts\", \"tsfeatures\", \"Kats\" since considerable overlap three sets. Defaults \"feasts\". applies by_set = TRUE (since set \"features\" constructed automatically comparator)","code":""},{"path":"https://hendersontrent.github.io/theft/reference/filter_duplicates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper function to remove duplicate features that exist in multiple feature sets — filter_duplicates","text":"feature_calculations object containing filtered feature data","code":""},{"path":"https://hendersontrent.github.io/theft/reference/filter_duplicates.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Helper function to remove duplicate features that exist in multiple feature sets — filter_duplicates","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/filter_good_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter resample data sets according to good feature list — filter_good_features","title":"Filter resample data sets according to good feature list — filter_good_features","text":"Filter resample data sets according good feature list","code":""},{"path":"https://hendersontrent.github.io/theft/reference/filter_good_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter resample data sets according to good feature list — filter_good_features","text":"","code":"filter_good_features(data, x, good_features)"},{"path":"https://hendersontrent.github.io/theft/reference/filter_good_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter resample data sets according to good feature list — filter_good_features","text":"data list \"Train\" \"Test\" data x integer denoting resample index operate good_features character vector good features keep","code":""},{"path":"https://hendersontrent.github.io/theft/reference/filter_good_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter resample data sets according to good feature list — filter_good_features","text":"list filtered train test data","code":""},{"path":"https://hendersontrent.github.io/theft/reference/filter_good_features.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Filter resample data sets according to good feature list — filter_good_features","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/find_good_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function to find features in both train and test set that are ","title":"Helper function to find features in both train and test set that are ","text":"Helper function find features train test set \"good\"","code":""},{"path":"https://hendersontrent.github.io/theft/reference/find_good_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function to find features in both train and test set that are ","text":"","code":"find_good_features(data, x)"},{"path":"https://hendersontrent.github.io/theft/reference/find_good_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper function to find features in both train and test set that are ","text":"data list \"Train\" \"Test\" data x integer denoting resample index operate ","code":""},{"path":"https://hendersontrent.github.io/theft/reference/find_good_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper function to find features in both train and test set that are ","text":"character vector \"good\" feature names","code":""},{"path":"https://hendersontrent.github.io/theft/reference/find_good_features.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Helper function to find features in both train and test set that are ","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_models.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit classification model and compute key metrics — fit_models","title":"Fit classification model and compute key metrics — fit_models","text":"Fit classification model compute key metrics","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit classification model and compute key metrics — fit_models","text":"","code":"fit_models(data, iter_data, row_id, is_null_run = FALSE, classifier)"},{"path":"https://hendersontrent.github.io/theft/reference/fit_models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit classification model and compute key metrics — fit_models","text":"data list containing train test sets iter_data data.frame containing values iterate seed either feature name set name row_id integer denoting row ID iter_data filter is_null_run Boolean whether calculation null model. Defaults FALSE classifier function specifying classifier fit. function 2 arguments: formula data. Please note tsfeature_classifier z-scores data prior modelling using train set's information disabling default scaling function uses recommended.","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit classification model and compute key metrics — fit_models","text":"data.frame classification results","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_models.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit classification model and compute key metrics — fit_models","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multivariable_classifier.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a classifier to feature matrix using all features or all features by set — fit_multivariable_classifier","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multivariable_classifier","text":"Fit classifier feature matrix using features features set","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multivariable_classifier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multivariable_classifier","text":"","code":"fit_multivariable_classifier(   data,   id_var = \"id\",   group_var = \"group\",   by_set = FALSE,   test_method = \"gaussprRadial\",   use_balanced_accuracy = FALSE,   use_k_fold = TRUE,   num_folds = 10,   use_empirical_null = FALSE,   null_testing_method = c(\"model free shuffles\", \"null model fits\"),   p_value_method = c(\"empirical\", \"gaussian\"),   num_permutations = 100,   seed = 123 )"},{"path":"https://hendersontrent.github.io/theft/reference/fit_multivariable_classifier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multivariable_classifier","text":"data dataframe containing raw feature data calculated theft::calculate_features id_var string specifying ID variable group data (one exists). Defaults \"id\" group_var string specifying grouping variable data aggregates . Defaults \"group\" by_set Boolean specifying whether compute classifiers feature set. Defaults FALSE test_method algorithm use quantifying class separation. Defaults \"gaussprRadial\" use_balanced_accuracy Boolean specifying whether use balanced accuracy summary metric caret model training. Defaults FALSE use_k_fold Boolean specifying whether use k-fold procedures generating distribution classification accuracy estimates. Defaults TRUE num_folds integer specifying number folds (train-test splits) perform use_k_fold set TRUE. Defaults 10 use_empirical_null Boolean specifying whether use empirical null procedures compute p-values. Defaults FALSE null_testing_method string specifying type statistical method use calculate p-values. Defaults model free shuffles p_value_method string specifying method calculating p-values. Defaults \"empirical\" num_permutations integer specifying number class label shuffles perform use_empirical_null TRUE. Defaults 100 seed fixed number R's random number generator ensure reproducibility","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multivariable_classifier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multivariable_classifier","text":"object class list containing dataframe summaries classification models ggplot object by_set TRUE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multivariable_classifier.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multivariable_classifier","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multivariable_classifier.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multivariable_classifier","text":"","code":"if (FALSE) { featMat <- calculate_features(data = simData,   id_var = \"id\",   time_var = \"timepoint\",   values_var = \"values\",   group_var = \"process\",   feature_set = \"catch22\")  fit_multivariable_classifier(featMat,   id_var = \"id\",   group_var = \"group\",   by_set = FALSE,   test_method = \"gaussprRadial\",   use_balanced_accuracy = FALSE,   use_k_fold = TRUE,   num_folds = 10,   use_empirical_null = TRUE,   null_testing_method = \"model free shuffles\",   p_value_method = \"empirical\",   num_permutations = 100,   seed = 123) }"},{"path":"https://hendersontrent.github.io/theft/reference/fit_multi_feature_classifier.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a classifier to feature matrix using all features or all features by set — fit_multi_feature_classifier","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multi_feature_classifier","text":"Fit classifier feature matrix using features features set","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multi_feature_classifier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multi_feature_classifier","text":"","code":"fit_multi_feature_classifier(   data,   by_set = FALSE,   test_method = \"gaussprRadial\",   use_balanced_accuracy = FALSE,   use_k_fold = TRUE,   num_folds = 10,   use_empirical_null = FALSE,   null_testing_method = c(\"ModelFreeShuffles\", \"NullModelFits\"),   p_value_method = c(\"empirical\", \"gaussian\"),   num_permutations = 100,   seed = 123 )"},{"path":"https://hendersontrent.github.io/theft/reference/fit_multi_feature_classifier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multi_feature_classifier","text":"data feature_calculations object containing raw feature matrix produced calculate_features by_set Boolean specifying whether compute classifiers feature set. Defaults FALSE test_method character specifying algorithm use quantifying class separation. Defaults \"gaussprRadial\". Must valid caret classification model use_balanced_accuracy Boolean specifying whether use balanced accuracy summary metric caret model training. Defaults FALSE use_k_fold Boolean specifying whether use k-fold procedures generating distribution classification accuracy estimates. Defaults TRUE num_folds integer specifying number folds (train-test splits) perform use_k_fold set TRUE. Defaults 10 use_empirical_null Boolean specifying whether use empirical null procedures compute p-values. Defaults FALSE null_testing_method character specifying type statistical method use calculate p-values. Defaults model free shuffles p_value_method character specifying method calculating p-values. Defaults \"empirical\" num_permutations integer specifying number class label shuffles perform use_empirical_null TRUE. Defaults 100 seed integer fix R's random number generator ensure reproducibility. Defaults 123","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multi_feature_classifier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multi_feature_classifier","text":"object class list containing data.frame summary raw classification results, data.frame summary test statistics, ggplot object by_set TRUE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multi_feature_classifier.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multi_feature_classifier","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_multi_feature_classifier.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a classifier to feature matrix using all features or all features by set — fit_multi_feature_classifier","text":"","code":"# \\donttest{ featMat <- calculate_features(data = simData,   id_var = \"id\",   time_var = \"timepoint\",   values_var = \"values\",   group_var = \"process\",   feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.  fit_multi_feature_classifier(featMat,   by_set = FALSE,   test_method = \"gaussprRadial\",   use_balanced_accuracy = FALSE,   use_k_fold = TRUE,   num_folds = 10,   use_empirical_null = TRUE,   null_testing_method = \"ModelFreeShuffles\",   p_value_method = \"gaussian\",   num_permutations = 50,   seed = 123) #> Assessing feature values and unique IDs for NAs using matrix of all features. #> Loading required package: ggplot2 #> Loading required package: lattice #> $TestStatistics #>    accuracy p_value_accuracy classifier_name               statistic_name #> 1 0.6722222     7.518342e-61   gaussprRadial Mean classification accuracy #>  #> $RawClassificationResults #>     accuracy accuracy_sd category            method num_features_used #> 1  0.6722222  0.08050765     Main              <NA>                NA #> 2  0.1222222          NA     Null ModelFreeShuffles                NA #> 3  0.2277778          NA     Null ModelFreeShuffles                NA #> 4  0.1444444          NA     Null ModelFreeShuffles                NA #> 5  0.1444444          NA     Null ModelFreeShuffles                NA #> 6  0.1444444          NA     Null ModelFreeShuffles                NA #> 7  0.1777778          NA     Null ModelFreeShuffles                NA #> 8  0.1888889          NA     Null ModelFreeShuffles                NA #> 9  0.1777778          NA     Null ModelFreeShuffles                NA #> 10 0.1611111          NA     Null ModelFreeShuffles                NA #> 11 0.1277778          NA     Null ModelFreeShuffles                NA #> 12 0.1722222          NA     Null ModelFreeShuffles                NA #> 13 0.1611111          NA     Null ModelFreeShuffles                NA #> 14 0.1555556          NA     Null ModelFreeShuffles                NA #> 15 0.1555556          NA     Null ModelFreeShuffles                NA #> 16 0.2055556          NA     Null ModelFreeShuffles                NA #> 17 0.1944444          NA     Null ModelFreeShuffles                NA #> 18 0.1500000          NA     Null ModelFreeShuffles                NA #> 19 0.1777778          NA     Null ModelFreeShuffles                NA #> 20 0.1444444          NA     Null ModelFreeShuffles                NA #> 21 0.1500000          NA     Null ModelFreeShuffles                NA #> 22 0.1611111          NA     Null ModelFreeShuffles                NA #> 23 0.1277778          NA     Null ModelFreeShuffles                NA #> 24 0.1722222          NA     Null ModelFreeShuffles                NA #> 25 0.2333333          NA     Null ModelFreeShuffles                NA #> 26 0.2222222          NA     Null ModelFreeShuffles                NA #> 27 0.1777778          NA     Null ModelFreeShuffles                NA #> 28 0.2222222          NA     Null ModelFreeShuffles                NA #> 29 0.1611111          NA     Null ModelFreeShuffles                NA #> 30 0.2222222          NA     Null ModelFreeShuffles                NA #> 31 0.2111111          NA     Null ModelFreeShuffles                NA #> 32 0.1388889          NA     Null ModelFreeShuffles                NA #> 33 0.1388889          NA     Null ModelFreeShuffles                NA #> 34 0.1666667          NA     Null ModelFreeShuffles                NA #> 35 0.1833333          NA     Null ModelFreeShuffles                NA #> 36 0.1444444          NA     Null ModelFreeShuffles                NA #> 37 0.1500000          NA     Null ModelFreeShuffles                NA #> 38 0.1722222          NA     Null ModelFreeShuffles                NA #> 39 0.1888889          NA     Null ModelFreeShuffles                NA #> 40 0.1888889          NA     Null ModelFreeShuffles                NA #> 41 0.1722222          NA     Null ModelFreeShuffles                NA #> 42 0.1555556          NA     Null ModelFreeShuffles                NA #> 43 0.1555556          NA     Null ModelFreeShuffles                NA #> 44 0.1944444          NA     Null ModelFreeShuffles                NA #> 45 0.1555556          NA     Null ModelFreeShuffles                NA #> 46 0.1277778          NA     Null ModelFreeShuffles                NA #> 47 0.1444444          NA     Null ModelFreeShuffles                NA #> 48 0.1777778          NA     Null ModelFreeShuffles                NA #> 49 0.1833333          NA     Null ModelFreeShuffles                NA #> 50 0.2000000          NA     Null ModelFreeShuffles                NA #> 51 0.2555556          NA     Null ModelFreeShuffles                NA #>    classifier_name               statistic_name #> 1    gaussprRadial Mean classification accuracy #> 2    gaussprRadial Mean classification accuracy #> 3    gaussprRadial Mean classification accuracy #> 4    gaussprRadial Mean classification accuracy #> 5    gaussprRadial Mean classification accuracy #> 6    gaussprRadial Mean classification accuracy #> 7    gaussprRadial Mean classification accuracy #> 8    gaussprRadial Mean classification accuracy #> 9    gaussprRadial Mean classification accuracy #> 10   gaussprRadial Mean classification accuracy #> 11   gaussprRadial Mean classification accuracy #> 12   gaussprRadial Mean classification accuracy #> 13   gaussprRadial Mean classification accuracy #> 14   gaussprRadial Mean classification accuracy #> 15   gaussprRadial Mean classification accuracy #> 16   gaussprRadial Mean classification accuracy #> 17   gaussprRadial Mean classification accuracy #> 18   gaussprRadial Mean classification accuracy #> 19   gaussprRadial Mean classification accuracy #> 20   gaussprRadial Mean classification accuracy #> 21   gaussprRadial Mean classification accuracy #> 22   gaussprRadial Mean classification accuracy #> 23   gaussprRadial Mean classification accuracy #> 24   gaussprRadial Mean classification accuracy #> 25   gaussprRadial Mean classification accuracy #> 26   gaussprRadial Mean classification accuracy #> 27   gaussprRadial Mean classification accuracy #> 28   gaussprRadial Mean classification accuracy #> 29   gaussprRadial Mean classification accuracy #> 30   gaussprRadial Mean classification accuracy #> 31   gaussprRadial Mean classification accuracy #> 32   gaussprRadial Mean classification accuracy #> 33   gaussprRadial Mean classification accuracy #> 34   gaussprRadial Mean classification accuracy #> 35   gaussprRadial Mean classification accuracy #> 36   gaussprRadial Mean classification accuracy #> 37   gaussprRadial Mean classification accuracy #> 38   gaussprRadial Mean classification accuracy #> 39   gaussprRadial Mean classification accuracy #> 40   gaussprRadial Mean classification accuracy #> 41   gaussprRadial Mean classification accuracy #> 42   gaussprRadial Mean classification accuracy #> 43   gaussprRadial Mean classification accuracy #> 44   gaussprRadial Mean classification accuracy #> 45   gaussprRadial Mean classification accuracy #> 46   gaussprRadial Mean classification accuracy #> 47   gaussprRadial Mean classification accuracy #> 48   gaussprRadial Mean classification accuracy #> 49   gaussprRadial Mean classification accuracy #> 50   gaussprRadial Mean classification accuracy #> 51   gaussprRadial Mean classification accuracy #>  # }"},{"path":"https://hendersontrent.github.io/theft/reference/fit_single_feature_classifier.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a classifier to feature matrix to extract top performers — fit_single_feature_classifier","title":"Fit a classifier to feature matrix to extract top performers — fit_single_feature_classifier","text":"Fit classifier feature matrix extract top performers","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_single_feature_classifier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a classifier to feature matrix to extract top performers — fit_single_feature_classifier","text":"","code":"fit_single_feature_classifier(   data,   test_method = \"gaussprRadial\",   use_balanced_accuracy = FALSE,   use_k_fold = FALSE,   num_folds = 10,   use_empirical_null = FALSE,   null_testing_method = c(\"ModelFreeShuffles\", \"NullModelFits\"),   p_value_method = c(\"empirical\", \"gaussian\"),   num_permutations = 50,   pool_empirical_null = FALSE,   seed = 123 )"},{"path":"https://hendersontrent.github.io/theft/reference/fit_single_feature_classifier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a classifier to feature matrix to extract top performers — fit_single_feature_classifier","text":"data data.frame containing raw feature matrix test_method character specifying algorithm use quantifying class separation. Defaults \"gaussprRadial\". either \"t-test\", \"wilcox\", \"binomial logistic\" two-class problems obtain exact statistics, valid caret classification model everything else use_balanced_accuracy Boolean specifying whether use balanced accuracy summary metric caret model training. Defaults FALSE use_k_fold Boolean specifying whether use k-fold procedures generating distribution classification accuracy estimates caret model specified test_method. Defaults  FALSE num_folds integer specifying number k-folds perform use_k_fold set TRUE. Defaults 10 use_empirical_null Boolean specifying whether use empirical null procedures compute p-values caret model specified test_method. Defaults FALSE null_testing_method character specifying type statistical method use calculate p-values. Defaults model free shuffles p_value_method character specifying method calculating p-values. Defaults \"empirical\" num_permutations integer specifying number class label shuffles perform use_empirical_null TRUE. Defaults 50 pool_empirical_null Boolean specifying whether use pooled empirical null distribution features features' individual empirical null distribution caret model specified test_method use_empirical_null TRUE. Defaults FALSE seed integer denoting fixed number R's random number generator ensure reproducibility. Defaults 123","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_single_feature_classifier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a classifier to feature matrix to extract top performers — fit_single_feature_classifier","text":"object class data.frame","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_single_feature_classifier.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit a classifier to feature matrix to extract top performers — fit_single_feature_classifier","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_univariable_classifier.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a classifier to feature matrix to extract top performers — fit_univariable_classifier","title":"Fit a classifier to feature matrix to extract top performers — fit_univariable_classifier","text":"Fit classifier feature matrix extract top performers","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_univariable_classifier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a classifier to feature matrix to extract top performers — fit_univariable_classifier","text":"","code":"fit_univariable_classifier(   data,   id_var = \"id\",   group_var = \"group\",   test_method = \"gaussprRadial\",   use_balanced_accuracy = FALSE,   use_k_fold = FALSE,   num_folds = 10,   use_empirical_null = FALSE,   null_testing_method = c(\"model free shuffles\", \"null model fits\"),   p_value_method = c(\"empirical\", \"gaussian\"),   num_permutations = 50,   pool_empirical_null = FALSE,   seed = 123,   return_raw_estimates = FALSE )"},{"path":"https://hendersontrent.github.io/theft/reference/fit_univariable_classifier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a classifier to feature matrix to extract top performers — fit_univariable_classifier","text":"data dataframe containing raw feature matrix id_var string specifying ID variable group data (one exists). Defaults \"id\" group_var string specifying grouping variable data aggregates . Defaults \"group\" test_method algorithm use quantifying class separation. Defaults \"gaussprRadial\". either \"t-test\", \"wilcox\", \"binomial logistic\" two-class problems obtain exact statistics, valid caret classification model everything else use_balanced_accuracy Boolean specifying whether use balanced accuracy summary metric caret model training. Defaults FALSE use_k_fold Boolean specifying whether use k-fold procedures generating distribution classification accuracy estimates caret model specified test_method. Defaults  FALSE num_folds integer specifying number k-folds perform use_k_fold set TRUE. Defaults 10 use_empirical_null Boolean specifying whether use empirical null procedures compute p-values caret model specified test_method. Defaults FALSE null_testing_method string specifying type statistical method use calculate p-values. Defaults model free shuffles p_value_method string specifying method calculating p-values. Defaults \"empirical\" num_permutations integer specifying number class label shuffles perform use_empirical_null TRUE. Defaults 50 pool_empirical_null Boolean specifying whether use pooled empirical null distribution features features' individual empirical null distribution caret model specified test_method use_empirical_null TRUE. Defaults FALSE seed fixed number R's random number generator ensure reproducibility return_raw_estimates Boolean (testing purposes -- break compute_top_features!!) specifying whether return raw main null model results","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_univariable_classifier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a classifier to feature matrix to extract top performers — fit_univariable_classifier","text":"object class dataframe containing results","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_univariable_classifier.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit a classifier to feature matrix to extract top performers — fit_univariable_classifier","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/fit_univariable_classifier.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a classifier to feature matrix to extract top performers — fit_univariable_classifier","text":"","code":"if (FALSE) { featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\")    fit_univariable_classifier(featMat,   id_var = \"id\",   group_var = \"group\",   test_method = \"linear svm\",   use_balanced_accuracy = FALSE,   use_k_fold = TRUE,   num_folds = 10,   use_empirical_null = TRUE,   null_testing_method = \"model free shuffles\",   p_value_method = \"empirical\",   num_permutations = 50,   pool_empirical_null = FALSE,   seed = 123,   return_raw_estimates = FALSE)  }"},{"path":"https://hendersontrent.github.io/theft/reference/get_rescale_vals.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate central tendency and spread values for all numeric columns in a dataset — get_rescale_vals","title":"Calculate central tendency and spread values for all numeric columns in a dataset — get_rescale_vals","text":"Calculate central tendency spread values numeric columns dataset","code":""},{"path":"https://hendersontrent.github.io/theft/reference/get_rescale_vals.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate central tendency and spread values for all numeric columns in a dataset — get_rescale_vals","text":"","code":"get_rescale_vals(data)"},{"path":"https://hendersontrent.github.io/theft/reference/get_rescale_vals.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate central tendency and spread values for all numeric columns in a dataset — get_rescale_vals","text":"data data.frame containing data normalise","code":""},{"path":"https://hendersontrent.github.io/theft/reference/get_rescale_vals.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate central tendency and spread values for all numeric columns in a dataset — get_rescale_vals","text":"list central tendency spread values","code":""},{"path":"https://hendersontrent.github.io/theft/reference/get_rescale_vals.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate central tendency and spread values for all numeric columns in a dataset — get_rescale_vals","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/init_theft.html","id":null,"dir":"Reference","previous_headings":"","what":"Communicate to R the Python virtual environment containing the relevant libraries for calculating features — init_theft","title":"Communicate to R the Python virtual environment containing the relevant libraries for calculating features — init_theft","text":"Communicate R Python virtual environment containing relevant libraries calculating features","code":""},{"path":"https://hendersontrent.github.io/theft/reference/init_theft.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Communicate to R the Python virtual environment containing the relevant libraries for calculating features — init_theft","text":"","code":"init_theft(python_path, venv_path)"},{"path":"https://hendersontrent.github.io/theft/reference/init_theft.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Communicate to R the Python virtual environment containing the relevant libraries for calculating features — init_theft","text":"python_path character specifying filepath version Python wish use venv_path character specifying filepath Python virtual environment \"tsfresh\", \"tsfel\", /\"kats\" installed","code":""},{"path":"https://hendersontrent.github.io/theft/reference/init_theft.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Communicate to R the Python virtual environment containing the relevant libraries for calculating features — init_theft","text":"return value; called side effects","code":""},{"path":"https://hendersontrent.github.io/theft/reference/init_theft.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Communicate to R the Python virtual environment containing the relevant libraries for calculating features — init_theft","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/install_python_pkgs.html","id":null,"dir":"Reference","previous_headings":"","what":"Download and install all the relevant Python packages into a target location — install_python_pkgs","title":"Download and install all the relevant Python packages into a target location — install_python_pkgs","text":"Download install relevant Python packages target location","code":""},{"path":"https://hendersontrent.github.io/theft/reference/install_python_pkgs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download and install all the relevant Python packages into a target location — install_python_pkgs","text":"","code":"install_python_pkgs(python_path, path)"},{"path":"https://hendersontrent.github.io/theft/reference/install_python_pkgs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download and install all the relevant Python packages into a target location — install_python_pkgs","text":"python_path character specifying filepath location Python 3.9 machine path character denoting filepath install Python libraries virtual environment ","code":""},{"path":"https://hendersontrent.github.io/theft/reference/install_python_pkgs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Download and install all the relevant Python packages into a target location — install_python_pkgs","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/minmax_scaler.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescales a numeric vector into the unit interval [0,1] — minmax_scaler","title":"Rescales a numeric vector into the unit interval [0,1] — minmax_scaler","text":"\\(z_{} = \\frac{x_{} - \\text{min}(\\mathbf{x})}{\\text{max}(\\mathbf{x}) - \\text{min}(\\mathbf{x})}\\)","code":""},{"path":"https://hendersontrent.github.io/theft/reference/minmax_scaler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescales a numeric vector into the unit interval [0,1] — minmax_scaler","text":"","code":"minmax_scaler(x)"},{"path":"https://hendersontrent.github.io/theft/reference/minmax_scaler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescales a numeric vector into the unit interval [0,1] — minmax_scaler","text":"x numeric vector","code":""},{"path":"https://hendersontrent.github.io/theft/reference/minmax_scaler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescales a numeric vector into the unit interval [0,1] — minmax_scaler","text":"numeric vector","code":""},{"path":"https://hendersontrent.github.io/theft/reference/minmax_scaler.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Rescales a numeric vector into the unit interval [0,1] — minmax_scaler","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise","text":"Scale feature vector user-specified range visualisation modelling","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise","text":"","code":"normalise(data, method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\"))"},{"path":"https://hendersontrent.github.io/theft/reference/normalise.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise","text":"data either feature_calculations object containing raw feature matrix produced calculate_features vector class numeric containing values normalised method character denoting rescaling/normalising method apply violin plots. Defaults \"z-score\"","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise","text":"either object class data.frame numeric","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise_feature_frame","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise_feature_frame","text":"Scale feature vector user-specified range visualisation modelling","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise_feature_frame","text":"","code":"normalise_feature_frame(   data,   names_var = \"names\",   values_var = \"values\",   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\") )"},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise_feature_frame","text":"data dataframe least 2 columns: names variable (feature names) value variable names_var string denoting name variable/column holds feature names. Defaults \"names\" values_var string denoting name variable/column holds numerical feature values. Defaults \"values\" method rescaling/normalising method apply. Defaults \"RobustSigmoid\"","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise_feature_frame","text":"dataframe value column rescaled specified range","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_frame.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise_feature_frame","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_frame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalise_feature_frame","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.    normed <- normalise_feature_frame(featMat,    names_var = \"names\",    values_var = \"values\",    method = \"RobustSigmoid\")"},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_vector.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale each value into a user-specified range for visualisation and analysis — normalise_feature_vector","title":"Scale each value into a user-specified range for visualisation and analysis — normalise_feature_vector","text":"Scale value user-specified range visualisation analysis","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_vector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale each value into a user-specified range for visualisation and analysis — normalise_feature_vector","text":"","code":"normalise_feature_vector(   x,   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\") )"},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_vector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale each value into a user-specified range for visualisation and analysis — normalise_feature_vector","text":"x vector scalar values method rescaling/normalising method apply. Defaults \"RobustSigmoid\"","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_vector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scale each value into a user-specified range for visualisation and analysis — normalise_feature_vector","text":"vector scalar values normalised selected range","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_vector.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Scale each value into a user-specified range for visualisation and analysis — normalise_feature_vector","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalise_feature_vector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scale each value into a user-specified range for visualisation and analysis — normalise_feature_vector","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.    x <- featMat[featMat$names == \"DN_HistogramMode_5\", ] xnormed <- normalise_feature_vector(x$values, method = \"RobustSigmoid\")"},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale each feature vector into a user-specified range for visualisation and modelling — normalize_feature_frame","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalize_feature_frame","text":"Scale feature vector user-specified range visualisation modelling","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalize_feature_frame","text":"","code":"normalize_feature_frame(   data,   names_var = \"names\",   values_var = \"values\",   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\") )"},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalize_feature_frame","text":"data dataframe least 2 columns: names variable (feature names) value variable names_var string denoting name variable/column holds feature names. Defaults \"names\" values_var string denoting name variable/column holds numerical feature values. Defaults \"values\" method rescaling/normalising method apply. Defaults \"RobustSigmoid\"","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalize_feature_frame","text":"dataframe value column rescaled specified range","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_frame.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalize_feature_frame","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_frame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scale each feature vector into a user-specified range for visualisation and modelling — normalize_feature_frame","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.    normed <- normalize_feature_frame(featMat,    names_var = \"names\",    values_var = \"values\",    method = \"RobustSigmoid\")"},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_vector.html","id":null,"dir":"Reference","previous_headings":"","what":"Scale each value into a user-specified range for visualisation and analysis — normalize_feature_vector","title":"Scale each value into a user-specified range for visualisation and analysis — normalize_feature_vector","text":"Scale value user-specified range visualisation analysis","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_vector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scale each value into a user-specified range for visualisation and analysis — normalize_feature_vector","text":"","code":"normalize_feature_vector(   x,   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\") )"},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_vector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scale each value into a user-specified range for visualisation and analysis — normalize_feature_vector","text":"x vector scalar values method rescaling/normalising method apply. Defaults \"RobustSigmoid\"","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_vector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scale each value into a user-specified range for visualisation and analysis — normalize_feature_vector","text":"vector scalar values normalised selected range","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_vector.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Scale each value into a user-specified range for visualisation and analysis — normalize_feature_vector","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/normalize_feature_vector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scale each value into a user-specified range for visualisation and analysis — normalize_feature_vector","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.    x <- featMat[featMat$names == \"DN_HistogramMode_5\", ] xnormed <- normalise_feature_vector(x$values, method = \"RobustSigmoid\")"},{"path":"https://hendersontrent.github.io/theft/reference/plot.feature_calculations.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce a plot for a feature_calculations object — plot.feature_calculations","title":"Produce a plot for a feature_calculations object — plot.feature_calculations","text":"Produce plot feature_calculations object","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot.feature_calculations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce a plot for a feature_calculations object — plot.feature_calculations","text":"","code":"# S3 method for feature_calculations plot(   x,   type = c(\"quality\", \"matrix\", \"cor\"),   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\"),   clust_method = c(\"average\", \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"mcquitty\",     \"median\", \"centroid\"),   cor_method = c(\"pearson\", \"spearman\"),   ... )"},{"path":"https://hendersontrent.github.io/theft/reference/plot.feature_calculations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Produce a plot for a feature_calculations object — plot.feature_calculations","text":"x feature_calculations object containing raw feature matrix produced calculate_features type character specifying type plot draw. Defaults \"quality\" method character specifying rescaling/normalising method apply type = \"matrix\" type = \"cor\". Defaults \"z-score\" clust_method character specifying hierarchical clustering method use type = \"matrix\" type = \"cor\". Defaults \"average\" cor_method character specifying correlation method use type = \"cor\". Defaults \"pearson\" ... Arguments passed methods","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot.feature_calculations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce a plot for a feature_calculations object — plot.feature_calculations","text":"object class ggplot contains graphic","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot.feature_calculations.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Produce a plot for a feature_calculations object — plot.feature_calculations","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot.low_dimension.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce a plot for a low_dimension object — plot.low_dimension","title":"Produce a plot for a low_dimension object — plot.low_dimension","text":"Produce plot low_dimension object","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot.low_dimension.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce a plot for a low_dimension object — plot.low_dimension","text":"","code":"# S3 method for low_dimension plot(x, show_covariance = TRUE, ...)"},{"path":"https://hendersontrent.github.io/theft/reference/plot.low_dimension.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Produce a plot for a low_dimension object — plot.low_dimension","text":"x low_dimension object containing dimensionality reduction projection calculated reduce_dims show_covariance Boolean whether covariance ellipses shown plot. Defaults TRUE ... Arguments passed methods","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot.low_dimension.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce a plot for a low_dimension object — plot.low_dimension","text":"object class ggplot contains graphic","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot.low_dimension.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Produce a plot for a low_dimension object — plot.low_dimension","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_all_features.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_all_features","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_all_features","text":"Produce heatmap matrix calculated feature value vectors unique time series automatic hierarchical clustering.","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_all_features.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_all_features","text":"","code":"plot_all_features(   data,   is_normalised = FALSE,   id_var = \"id\",   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\"),   clust_method = c(\"average\", \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"mcquitty\",     \"median\", \"centroid\"),   interactive = FALSE )"},{"path":"https://hendersontrent.github.io/theft/reference/plot_all_features.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_all_features","text":"data dataframe least 2 columns called \"names\" \"values\" is_normalised Boolean whether input feature values already scaled. Defaults FALSE id_var string specifying ID variable identify time series. Defaults \"id\" method rescaling/normalising method apply. Defaults \"RobustSigmoid\" clust_method hierarchical clustering method use pairwise correlation plot. Defaults \"average\" interactive Boolean whether plot interactive plotly graphic. Defaults FALSE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_all_features.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_all_features","text":"object class ggplot contains heatmap graphic","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_all_features.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_all_features","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_all_features.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_all_features","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.  plot_all_features(featMat,    is_normalised = FALSE,   id_var = \"id\",    method = \"RobustSigmoid\",   clust_method = \"average\",   interactive = FALSE) #> Applying linear rescaling of values to make plot legend cleaner."},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_correlations.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce a correlation matrix plot showing pairwise correlations of feature vectors by unique id with automatic hierarchical clustering. — plot_feature_correlations","title":"Produce a correlation matrix plot showing pairwise correlations of feature vectors by unique id with automatic hierarchical clustering. — plot_feature_correlations","text":"Produce correlation matrix plot showing pairwise correlations feature vectors unique id automatic hierarchical clustering.","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_correlations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce a correlation matrix plot showing pairwise correlations of feature vectors by unique id with automatic hierarchical clustering. — plot_feature_correlations","text":"","code":"plot_feature_correlations(   data,   is_normalised = NULL,   id_var = \"id\",   names_var = \"names\",   values_var = \"values\",   method = NULL,   cor_method = c(\"pearson\", \"spearman\"),   clust_method = c(\"average\", \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"mcquitty\",     \"median\", \"centroid\"),   interactive = FALSE )"},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_correlations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Produce a correlation matrix plot showing pairwise correlations of feature vectors by unique id with automatic hierarchical clustering. — plot_feature_correlations","text":"data dataframe least 3 columns 'id', 'names' 'values' is_normalised deprecated 0.4.0; use id_var string specifying ID variable compute pairwise correlations . Defaults \"id\" names_var string denoting name variable/column holds feature names. Defaults \"names\" values_var string denoting name variable/column holds numerical feature values. Defaults \"values\" method deprecated 0.4.0; use cor_method correlation method use. Defaults \"pearson\" clust_method hierarchical clustering method use pairwise correlation plot. Defaults \"average\" interactive Boolean whether plot interactive plotly graphic. Defaults FALSE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_correlations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce a correlation matrix plot showing pairwise correlations of feature vectors by unique id with automatic hierarchical clustering. — plot_feature_correlations","text":"object class ggplot contains correlation matrix graphic","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_correlations.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Produce a correlation matrix plot showing pairwise correlations of feature vectors by unique id with automatic hierarchical clustering. — plot_feature_correlations","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_correlations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Produce a correlation matrix plot showing pairwise correlations of feature vectors by unique id with automatic hierarchical clustering. — plot_feature_correlations","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.    plot_feature_correlations(data = featMat,    id_var = \"id\",    names_var = \"names\",    values_var = \"values\",   cor_method = \"pearson\",   clust_method = \"average\",   interactive = FALSE)"},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_feature_matrix","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_feature_matrix","text":"Produce heatmap matrix calculated feature value vectors unique time series automatic hierarchical clustering.","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_feature_matrix","text":"","code":"plot_feature_matrix(   data,   is_normalised = FALSE,   id_var = \"id\",   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\"),   clust_method = c(\"average\", \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"mcquitty\",     \"median\", \"centroid\"),   interactive = FALSE )"},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_feature_matrix","text":"data dataframe least 2 columns called \"names\" \"values\" is_normalised Boolean whether input feature values already scaled. Defaults FALSE id_var string specifying ID variable identify time series. Defaults \"id\" method rescaling/normalising method apply. Defaults \"RobustSigmoid\" clust_method hierarchical clustering method use pairwise correlation plot. Defaults \"average\" interactive Boolean whether plot interactive plotly graphic. Defaults FALSE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_feature_matrix","text":"object class ggplot contains heatmap graphic","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_matrix.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_feature_matrix","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_feature_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Produce a heatmap matrix of the calculated feature value vectors and each unique time series with automatic hierarchical clustering. — plot_feature_matrix","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.  plot_feature_matrix(featMat,    is_normalised = FALSE,   id_var = \"id\",    method = \"RobustSigmoid\",   clust_method = \"average\",   interactive = FALSE) #> Warning: As of 0.3.6 plot_feature_matrix is deprecated. Please use 'plot_all_features' instead #> This warning is displayed once per session. #> Applying linear rescaling of values to make plot legend cleaner."},{"path":"https://hendersontrent.github.io/theft/reference/plot_low_dimension.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce a principal components analysis (PCA) on normalised feature values and render a bivariate plot to visualise it — plot_low_dimension","title":"Produce a principal components analysis (PCA) on normalised feature values and render a bivariate plot to visualise it — plot_low_dimension","text":"Produce principal components analysis (PCA) normalised feature values render bivariate plot visualise ","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_low_dimension.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce a principal components analysis (PCA) on normalised feature values and render a bivariate plot to visualise it — plot_low_dimension","text":"","code":"plot_low_dimension(   data,   is_normalised = FALSE,   id_var = \"id\",   group_var = NULL,   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\"),   low_dim_method = c(\"PCA\", \"t-SNE\"),   perplexity = 30,   plot = TRUE,   show_covariance = FALSE,   seed = 123 )"},{"path":"https://hendersontrent.github.io/theft/reference/plot_low_dimension.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Produce a principal components analysis (PCA) on normalised feature values and render a bivariate plot to visualise it — plot_low_dimension","text":"data dataframe least 2 columns called \"names\" \"values\" is_normalised Boolean whether input feature values already scaled. Defaults FALSE id_var string specifying ID variable uniquely identify time series. Defaults \"id\" group_var string specifying grouping variable data aggregates (one exists). Defaults NULL method rescaling/normalising method apply. Defaults \"z-score\" low_dim_method low dimensional embedding method use. Defaults \"PCA\" perplexity perplexity hyperparameter use t-SNE algorithm selected. Defaults 30 plot Boolean whether plot model fit information returned. Defaults TRUE show_covariance Boolean whether covariance ellipses shown plot. Defaults FALSE seed fixed number R's random number generator ensure reproducibility","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_low_dimension.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce a principal components analysis (PCA) on normalised feature values and render a bivariate plot to visualise it — plot_low_dimension","text":"plot = TRUE, returns object class ggplot, plot = FALSE returns object class dataframe PCA results","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_low_dimension.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Produce a principal components analysis (PCA) on normalised feature values and render a bivariate plot to visualise it — plot_low_dimension","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_low_dimension.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Produce a principal components analysis (PCA) on normalised feature values and render a bivariate plot to visualise it — plot_low_dimension","text":"","code":"# \\donttest{ featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.  plot_low_dimension(featMat,    is_normalised = FALSE,    id_var = \"id\",    group_var = \"group\",    method = \"RobustSigmoid\",    low_dim_method = \"PCA\",    plot = TRUE,   show_covariance = TRUE,   seed = 123)  # }"},{"path":"https://hendersontrent.github.io/theft/reference/plot_quality_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce a matrix visualisation of data types computed by feature calculation function. — plot_quality_matrix","title":"Produce a matrix visualisation of data types computed by feature calculation function. — plot_quality_matrix","text":"Produce matrix visualisation data types computed feature calculation function.","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_quality_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce a matrix visualisation of data types computed by feature calculation function. — plot_quality_matrix","text":"","code":"plot_quality_matrix(data, ignore_good_features = FALSE)"},{"path":"https://hendersontrent.github.io/theft/reference/plot_quality_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Produce a matrix visualisation of data types computed by feature calculation function. — plot_quality_matrix","text":"data dataframe least 2 columns called \"names\" \"values\" ignore_good_features Boolean whether remove \"good\" features (.e., successful numeric values) plot. Defaults FALSE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_quality_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce a matrix visualisation of data types computed by feature calculation function. — plot_quality_matrix","text":"object class ggplot","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_quality_matrix.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Produce a matrix visualisation of data types computed by feature calculation function. — plot_quality_matrix","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_quality_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Produce a matrix visualisation of data types computed by feature calculation function. — plot_quality_matrix","text":"","code":"featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.  plot_quality_matrix(data = featMat,   ignore_good_features = FALSE)"},{"path":"https://hendersontrent.github.io/theft/reference/plot_ts_correlations.html","id":null,"dir":"Reference","previous_headings":"","what":"Produce a correlation matrix plot showing pairwise correlations of time series with automatic hierarchical clustering — plot_ts_correlations","title":"Produce a correlation matrix plot showing pairwise correlations of time series with automatic hierarchical clustering — plot_ts_correlations","text":"Produce correlation matrix plot showing pairwise correlations time series automatic hierarchical clustering","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_ts_correlations.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Produce a correlation matrix plot showing pairwise correlations of time series with automatic hierarchical clustering — plot_ts_correlations","text":"","code":"plot_ts_correlations(   data,   is_normalised = NULL,   id_var = \"id\",   time_var = \"timepoint\",   values_var = \"values\",   method = NULL,   clust_method = c(\"average\", \"ward.D\", \"ward.D2\", \"single\", \"complete\", \"mcquitty\",     \"median\", \"centroid\"),   cor_method = c(\"pearson\", \"spearman\"),   interactive = FALSE )"},{"path":"https://hendersontrent.github.io/theft/reference/plot_ts_correlations.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Produce a correlation matrix plot showing pairwise correlations of time series with automatic hierarchical clustering — plot_ts_correlations","text":"data dataframewith least 2 columns \"id\" \"values\" variables is_normalised deprecated 0.4.0; use id_var string specifying ID variable compute pairwise correlations . Defaults \"id\" time_var string specifying time index variable. Defaults NULL values_var string denoting name variable/column holds numerical feature values. Defaults \"values\" method deprecated 0.4.0; use clust_method hierarchical clustering method use pairwise correlation plot. Defaults \"average\" cor_method correlation method use. Defaults \"pearson\" interactive Boolean whether plot interactive plotly graphic. Defaults FALSE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_ts_correlations.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Produce a correlation matrix plot showing pairwise correlations of time series with automatic hierarchical clustering — plot_ts_correlations","text":"object class ggplot","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_ts_correlations.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Produce a correlation matrix plot showing pairwise correlations of time series with automatic hierarchical clustering — plot_ts_correlations","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/plot_ts_correlations.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Produce a correlation matrix plot showing pairwise correlations of time series with automatic hierarchical clustering — plot_ts_correlations","text":"","code":"plot_ts_correlations(data = simData,    id_var = \"id\",    time_var = \"timepoint\",   values_var = \"values\",   method = \"RobustSigmoid\",   cor_method = \"pearson\",   clust_method = \"average\",   interactive = FALSE) #> Warning: As of 0.4.0 'is_normalised' and 'method' are no longer arguments to plot_ts_correlations #> This warning is displayed once per session."},{"path":"https://hendersontrent.github.io/theft/reference/process_hctsa_file.html","id":null,"dir":"Reference","previous_headings":"","what":"Load in hctsa formatted MATLAB files of time series data into a tidy format ready for feature extraction — process_hctsa_file","title":"Load in hctsa formatted MATLAB files of time series data into a tidy format ready for feature extraction — process_hctsa_file","text":"Load hctsa formatted MATLAB files time series data tidy format ready feature extraction","code":""},{"path":"https://hendersontrent.github.io/theft/reference/process_hctsa_file.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load in hctsa formatted MATLAB files of time series data into a tidy format ready for feature extraction — process_hctsa_file","text":"","code":"process_hctsa_file(data)"},{"path":"https://hendersontrent.github.io/theft/reference/process_hctsa_file.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load in hctsa formatted MATLAB files of time series data into a tidy format ready for feature extraction — process_hctsa_file","text":"data string specifying filepath MATLAB file parse","code":""},{"path":"https://hendersontrent.github.io/theft/reference/process_hctsa_file.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load in hctsa formatted MATLAB files of time series data into a tidy format ready for feature extraction — process_hctsa_file","text":"object class data.frame tidy format","code":""},{"path":"https://hendersontrent.github.io/theft/reference/process_hctsa_file.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Load in hctsa formatted MATLAB files of time series data into a tidy format ready for feature extraction — process_hctsa_file","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/reduce_dims.html","id":null,"dir":"Reference","previous_headings":"","what":"Project a feature matrix into a low dimensional representation using PCA or t-SNE — reduce_dims","title":"Project a feature matrix into a low dimensional representation using PCA or t-SNE — reduce_dims","text":"Project feature matrix low dimensional representation using PCA t-SNE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/reduce_dims.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Project a feature matrix into a low dimensional representation using PCA or t-SNE — reduce_dims","text":"","code":"reduce_dims(   data,   method = c(\"z-score\", \"Sigmoid\", \"RobustSigmoid\", \"MinMax\"),   low_dim_method = c(\"PCA\", \"t-SNE\"),   perplexity = 10,   seed = 123,   ... )"},{"path":"https://hendersontrent.github.io/theft/reference/reduce_dims.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Project a feature matrix into a low dimensional representation using PCA or t-SNE — reduce_dims","text":"data feature_calculations object containing raw feature matrix produced calculate_features method character specifying rescaling/normalising method apply. Defaults \"z-score\" low_dim_method character specifying low dimensional embedding method use. Defaults \"PCA\" perplexity integer denoting perplexity hyperparameter use low_dim_method \"t-SNE\". Defaults 10 seed integer fix R's random number generator ensure reproducibility. Defaults 123 ... arguments passed either stats::prcomp Rtsne::Rtsne depending whether \"low_dim_method\" \"PCA\" \"t-SNE\"","code":""},{"path":"https://hendersontrent.github.io/theft/reference/reduce_dims.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Project a feature matrix into a low dimensional representation using PCA or t-SNE — reduce_dims","text":"object class low_dimension","code":""},{"path":"https://hendersontrent.github.io/theft/reference/reduce_dims.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Project a feature matrix into a low dimensional representation using PCA or t-SNE — reduce_dims","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resampled_ttest.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute correlated t-statistic and p-value for resampled data from correctR package — resampled_ttest","title":"Compute correlated t-statistic and p-value for resampled data from correctR package — resampled_ttest","text":"Compute correlated t-statistic p-value resampled data correctR package","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resampled_ttest.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute correlated t-statistic and p-value for resampled data from correctR package — resampled_ttest","text":"","code":"resampled_ttest(x, y, n, n1, n2)"},{"path":"https://hendersontrent.github.io/theft/reference/resampled_ttest.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute correlated t-statistic and p-value for resampled data from correctR package — resampled_ttest","text":"x numeric vector values model y numeric vector values model B n integer denoting number repeat samples. Defaults length(x) n1 integer denoting train set size n2 integer denoting test set size","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resampled_ttest.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute correlated t-statistic and p-value for resampled data from correctR package — resampled_ttest","text":"object class data.frame","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resampled_ttest.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute correlated t-statistic and p-value for resampled data from correctR package — resampled_ttest","text":"Nadeau, C., Bengio, Y. Inference Generalization Error. Machine Learning 52, (2003). Bouckaert, R. R., Frank, E. Evaluating Replicability Significance Tests Comparing Learning Algorithms. Advances Knowledge Discovery Data Mining. PAKDD 2004. Lecture Notes Computer Science, 3056, (2004).","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resampled_ttest.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Compute correlated t-statistic and p-value for resampled data from correctR package — resampled_ttest","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resample_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper function to create a resampled dataset — resample_data","title":"Helper function to create a resampled dataset — resample_data","text":"Helper function create resampled dataset","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resample_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper function to create a resampled dataset — resample_data","text":"","code":"resample_data(data, train_rows, test_rows, train_groups, test_groups, seed)"},{"path":"https://hendersontrent.github.io/theft/reference/resample_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper function to create a resampled dataset — resample_data","text":"data data.frame containing time-series features train_rows integer denoting number cases train set test_rows integer denoting number cases test set train_groups data.frame containing proportions class original train split test_groups data.frame containing proportions class original test split seed integer denoting fixed value R's pseudorandom number generator","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resample_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper function to create a resampled dataset — resample_data","text":"list containing new train test data","code":""},{"path":"https://hendersontrent.github.io/theft/reference/resample_data.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Helper function to create a resampled dataset — resample_data","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/rescale_zscore.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate z-score for all columns in a dataset using train set central tendency and spread — rescale_zscore","title":"Calculate z-score for all columns in a dataset using train set central tendency and spread — rescale_zscore","text":"Calculate z-score columns dataset using train set central tendency spread","code":""},{"path":"https://hendersontrent.github.io/theft/reference/rescale_zscore.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate z-score for all columns in a dataset using train set central tendency and spread — rescale_zscore","text":"","code":"rescale_zscore(data, rescalers)"},{"path":"https://hendersontrent.github.io/theft/reference/rescale_zscore.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate z-score for all columns in a dataset using train set central tendency and spread — rescale_zscore","text":"data data.frame containing data normalise rescalers list containing central tendency spread values train set","code":""},{"path":"https://hendersontrent.github.io/theft/reference/rescale_zscore.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate z-score for all columns in a dataset using train set central tendency and spread — rescale_zscore","text":"data.frame rescaled data","code":""},{"path":"https://hendersontrent.github.io/theft/reference/rescale_zscore.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate z-score for all columns in a dataset using train set central tendency and spread — rescale_zscore","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/robustsigmoid_scaler.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescales a numeric vector using an outlier-robust Sigmoidal transformation and then into the unit interval [0,1] — robustsigmoid_scaler","title":"Rescales a numeric vector using an outlier-robust Sigmoidal transformation and then into the unit interval [0,1] — robustsigmoid_scaler","text":"\\(z_{} = \\left[1 + \\exp\\left(-\\frac{x_{} - \\text{median}(\\mathbf{x})}{\\text{IQR}(\\mathbf{x})/{1.35}}\\right)\\right]^{-1}\\)","code":""},{"path":"https://hendersontrent.github.io/theft/reference/robustsigmoid_scaler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescales a numeric vector using an outlier-robust Sigmoidal transformation and then into the unit interval [0,1] — robustsigmoid_scaler","text":"","code":"robustsigmoid_scaler(x, unitInt = TRUE)"},{"path":"https://hendersontrent.github.io/theft/reference/robustsigmoid_scaler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescales a numeric vector using an outlier-robust Sigmoidal transformation and then into the unit interval [0,1] — robustsigmoid_scaler","text":"x numeric vector unitInt Boolean whether rescale unit interval [0,1]. Defaults TRUE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/robustsigmoid_scaler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescales a numeric vector using an outlier-robust Sigmoidal transformation and then into the unit interval [0,1] — robustsigmoid_scaler","text":"numeric vector","code":""},{"path":"https://hendersontrent.github.io/theft/reference/robustsigmoid_scaler.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Rescales a numeric vector using an outlier-robust Sigmoidal transformation and then into the unit interval [0,1] — robustsigmoid_scaler","text":"Fulcher, Ben D., Little, Max ., Jones, Nick S. Highly Comparative Time-Series Analysis: Empirical Structure Time Series Methods. Journal Royal Society Interface 10(83), (2013).","code":""},{"path":"https://hendersontrent.github.io/theft/reference/robustsigmoid_scaler.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Rescales a numeric vector using an outlier-robust Sigmoidal transformation and then into the unit interval [0,1] — robustsigmoid_scaler","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/select_k_best.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform fast and simple univariate feature selection based on an output vector using analysis of variance or correlation — select_k_best","title":"Perform fast and simple univariate feature selection based on an output vector using analysis of variance or correlation — select_k_best","text":"Perform fast simple univariate feature selection based output vector using analysis variance correlation","code":""},{"path":"https://hendersontrent.github.io/theft/reference/select_k_best.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform fast and simple univariate feature selection based on an output vector using analysis of variance or correlation — select_k_best","text":"","code":"select_k_best(   data,   k = floor(length(unique(data[[1]]$names))/2),   outputs = NULL )"},{"path":"https://hendersontrent.github.io/theft/reference/select_k_best.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform fast and simple univariate feature selection based on an output vector using analysis of variance or correlation — select_k_best","text":"data feature_calculations object containing raw feature matrix produced calculate_features k integer denoting number features retain. Defaults half length unique features available data outputs data.frame containing output data included group column initially running calculate_features. 2 columns first ID variable can joined id column data second output interest (.e., 'y' variable). character factor, classification assumed, numeric, regression assumed. Defaults NULL assumes group variable exists data","code":""},{"path":"https://hendersontrent.github.io/theft/reference/select_k_best.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform fast and simple univariate feature selection based on an output vector using analysis of variance or correlation — select_k_best","text":"object class feature_calculations contains summary statistics feature retained","code":""},{"path":"https://hendersontrent.github.io/theft/reference/select_k_best.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Perform fast and simple univariate feature selection based on an output vector using analysis of variance or correlation — select_k_best","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/sigmoid_scaler.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescales a numeric vector using a Sigmoidal transformation — sigmoid_scaler","title":"Rescales a numeric vector using a Sigmoidal transformation — sigmoid_scaler","text":"\\(z_{} = \\left[1 + \\exp(-\\frac{x_{} - \\mu}{\\sigma})\\right]^{-1}\\)","code":""},{"path":"https://hendersontrent.github.io/theft/reference/sigmoid_scaler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescales a numeric vector using a Sigmoidal transformation — sigmoid_scaler","text":"","code":"sigmoid_scaler(x, unitInt = TRUE)"},{"path":"https://hendersontrent.github.io/theft/reference/sigmoid_scaler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescales a numeric vector using a Sigmoidal transformation — sigmoid_scaler","text":"x numeric vector unitInt Boolean whether rescale unit interval [0,1]. Defaults TRUE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/sigmoid_scaler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescales a numeric vector using a Sigmoidal transformation — sigmoid_scaler","text":"numeric vector","code":""},{"path":"https://hendersontrent.github.io/theft/reference/sigmoid_scaler.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Rescales a numeric vector using a Sigmoidal transformation — sigmoid_scaler","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/simData.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample of randomly-generated time series to produce function tests and vignettes — simData","title":"Sample of randomly-generated time series to produce function tests and vignettes — simData","text":"variables include:","code":""},{"path":"https://hendersontrent.github.io/theft/reference/simData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample of randomly-generated time series to produce function tests and vignettes — simData","text":"","code":"simData"},{"path":"https://hendersontrent.github.io/theft/reference/simData.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample of randomly-generated time series to produce function tests and vignettes — simData","text":"tidy data frame 4 variables: id Unique identifier time series timepoint Time index values Value process Group label type time series","code":""},{"path":"https://hendersontrent.github.io/theft/reference/stat_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate p-values for feature sets or features relative to an empirical null or each other using resampled t-tests — stat_test","title":"Calculate p-values for feature sets or features relative to an empirical null or each other using resampled t-tests — stat_test","text":"Calculate p-values feature sets features relative empirical null using resampled t-tests","code":""},{"path":"https://hendersontrent.github.io/theft/reference/stat_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate p-values for feature sets or features relative to an empirical null or each other using resampled t-tests — stat_test","text":"","code":"stat_test(   data,   iter_data,   row_id,   by_set = FALSE,   hypothesis,   metric,   train_test_sizes,   n_resamples )"},{"path":"https://hendersontrent.github.io/theft/reference/stat_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate p-values for feature sets or features relative to an empirical null or each other using resampled t-tests — stat_test","text":"data data.frame raw classification accuracy results iter_data data.frame containing values iterate seed either feature name set name row_id integer denoting row ID iter_data filter by_set Boolean specifying whether want compare feature sets (TRUE) individual features (FALSE). hypothesis character denoting whether p-values calculated feature set feature (depending by_set argument) individually relative null use_null = TRUE tsfeature_classifier \"null\", whether pairwise comparisons set feature conducted main model fits \"pairwise\". metric character denoting classification performance metric use statistical testing. Can one \"accuracy\", \"precision\", \"recall\", \"f1\". Defaults \"accuracy\" train_test_sizes integer vector containing train test set sample sizes n_resamples integer denoting number resamples calculated","code":""},{"path":"https://hendersontrent.github.io/theft/reference/stat_test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate p-values for feature sets or features relative to an empirical null or each other using resampled t-tests — stat_test","text":"object class data.frame","code":""},{"path":"https://hendersontrent.github.io/theft/reference/stat_test.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Calculate p-values for feature sets or features relative to an empirical null or each other using resampled t-tests — stat_test","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/theft.html","id":null,"dir":"Reference","previous_headings":"","what":"Tools for Handling Extraction of Features from Time-series — theft","title":"Tools for Handling Extraction of Features from Time-series — theft","text":"Tools Handling Extraction Features Time-series","code":""},{"path":"https://hendersontrent.github.io/theft/reference/tsfeature_classifier.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit classifiers using time-series features using a resample-based approach and get a fast understanding of performance — tsfeature_classifier","title":"Fit classifiers using time-series features using a resample-based approach and get a fast understanding of performance — tsfeature_classifier","text":"Fit classifiers using time-series features using resample-based approach get fast understanding performance","code":""},{"path":"https://hendersontrent.github.io/theft/reference/tsfeature_classifier.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit classifiers using time-series features using a resample-based approach and get a fast understanding of performance — tsfeature_classifier","text":"","code":"tsfeature_classifier(   data,   classifier = NULL,   train_size = 0.75,   n_resamples = 30,   by_set = TRUE,   use_null = FALSE,   seed = 123,   preference = c(\"feasts\", \"tsfeatures\", \"Kats\") )"},{"path":"https://hendersontrent.github.io/theft/reference/tsfeature_classifier.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit classifiers using time-series features using a resample-based approach and get a fast understanding of performance — tsfeature_classifier","text":"data feature_calculations object containing raw feature matrix produced calculate_features included group column per theft::calculate_features classifier function specifying classifier fit. function 2 arguments: formula data containing classifier compatible R's predict functionality. Please note tsfeature_classifier z-scores data prior modelling using train set's information disabling default scaling function uses recommended. Defaults NULL means following linear SVM fit: classifier = function(formula, data){mod <- e1071::svm(formula, data = data, kernel = \"linear\", scale = FALSE, probability = TRUE)} train_size numeric denoting proportion samples use training set. Defaults 0.75 n_resamples integer denoting number resamples calculate. Defaults 30 by_set Boolean specifying whether compute classifiers feature set. Defaults TRUE. FALSE, function instead find best individually-performing features use_null Boolean whether fit null models class labels shuffled order generate null distribution can compared performance correct class labels. Defaults FALSE seed integer fix R's random number generator ensure reproducibility. Defaults 123 preference character denoting feature set keep (meaning others filtered ) \"feasts\", \"tsfeatures\", \"Kats\" since considerable overlap three sets. Defaults \"feasts\". applies by_set = TRUE (since set \"features\" constructed automatically comparator)","code":""},{"path":"https://hendersontrent.github.io/theft/reference/tsfeature_classifier.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit classifiers using time-series features using a resample-based approach and get a fast understanding of performance — tsfeature_classifier","text":"list containing named vector train-test set sizes, data.frame classification performance results","code":""},{"path":"https://hendersontrent.github.io/theft/reference/tsfeature_classifier.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Fit classifiers using time-series features using a resample-based approach and get a fast understanding of performance — tsfeature_classifier","text":"Trent Henderson","code":""},{"path":"https://hendersontrent.github.io/theft/reference/tsfeature_classifier.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit classifiers using time-series features using a resample-based approach and get a fast understanding of performance — tsfeature_classifier","text":"","code":"# \\donttest{ featMat <- calculate_features(data = simData,    id_var = \"id\",    time_var = \"timepoint\",    values_var = \"values\",    group_var = \"process\",    feature_set = \"catch22\",   seed = 123) #> No IDs removed. All value vectors good for feature extraction. #> Running computations for catch22... #>  #> Calculations completed for catch22.    classifiers <- tsfeature_classifier(featMat,   by_set = FALSE) #> Fitting model 1/660 #> Fitting model 2/660 #> Fitting model 3/660 #> Fitting model 4/660 #> Fitting model 5/660 #> Fitting model 6/660 #> Fitting model 7/660 #> Fitting model 8/660 #> Fitting model 9/660 #> Fitting model 10/660 #> Fitting model 11/660 #> Fitting model 12/660 #> Fitting model 13/660 #> Fitting model 14/660 #> Fitting model 15/660 #> Fitting model 16/660 #> Fitting model 17/660 #> Fitting model 18/660 #> Fitting model 19/660 #> Fitting model 20/660 #> Fitting model 21/660 #> Fitting model 22/660 #> Fitting model 23/660 #> Fitting model 24/660 #> Fitting model 25/660 #> Fitting model 26/660 #> Fitting model 27/660 #> Fitting model 28/660 #> Fitting model 29/660 #> Fitting model 30/660 #> Fitting model 31/660 #> Fitting model 32/660 #> Fitting model 33/660 #> Fitting model 34/660 #> Fitting model 35/660 #> Fitting model 36/660 #> Fitting model 37/660 #> Fitting model 38/660 #> Fitting model 39/660 #> Fitting model 40/660 #> Fitting model 41/660 #> Fitting model 42/660 #> Fitting model 43/660 #> Fitting model 44/660 #> Fitting model 45/660 #> Fitting model 46/660 #> Fitting model 47/660 #> Fitting model 48/660 #> Fitting model 49/660 #> Fitting model 50/660 #> Fitting model 51/660 #> Fitting model 52/660 #> Fitting model 53/660 #> Fitting model 54/660 #> Fitting model 55/660 #> Fitting model 56/660 #> Fitting model 57/660 #> Fitting model 58/660 #> Fitting model 59/660 #> Fitting model 60/660 #> Fitting model 61/660 #> Fitting model 62/660 #> Fitting model 63/660 #> Fitting model 64/660 #> Fitting model 65/660 #> Fitting model 66/660 #> Fitting model 67/660 #> Fitting model 68/660 #> Fitting model 69/660 #> Fitting model 70/660 #> Fitting model 71/660 #> Fitting model 72/660 #> Fitting model 73/660 #> Fitting model 74/660 #> Fitting model 75/660 #> Fitting model 76/660 #> Fitting model 77/660 #> Fitting model 78/660 #> Fitting model 79/660 #> Fitting model 80/660 #> Fitting model 81/660 #> Fitting model 82/660 #> Fitting model 83/660 #> Fitting model 84/660 #> Fitting model 85/660 #> Fitting model 86/660 #> Fitting model 87/660 #> Fitting model 88/660 #> Fitting model 89/660 #> Fitting model 90/660 #> Fitting model 91/660 #> Fitting model 92/660 #> Fitting model 93/660 #> Fitting model 94/660 #> Fitting model 95/660 #> Fitting model 96/660 #> Fitting model 97/660 #> Fitting model 98/660 #> Fitting model 99/660 #> Fitting model 100/660 #> Fitting model 101/660 #> Fitting model 102/660 #> Fitting model 103/660 #> Fitting model 104/660 #> Fitting model 105/660 #> Fitting model 106/660 #> Fitting model 107/660 #> Fitting model 108/660 #> Fitting model 109/660 #> Fitting model 110/660 #> Fitting model 111/660 #> Fitting model 112/660 #> Fitting model 113/660 #> Fitting model 114/660 #> Fitting model 115/660 #> Fitting model 116/660 #> Fitting model 117/660 #> Fitting model 118/660 #> Fitting model 119/660 #> Fitting model 120/660 #> Fitting model 121/660 #> Fitting model 122/660 #> Fitting model 123/660 #> Fitting model 124/660 #> Fitting model 125/660 #> Fitting model 126/660 #> Fitting model 127/660 #> Fitting model 128/660 #> Fitting model 129/660 #> Fitting model 130/660 #> Fitting model 131/660 #> Fitting model 132/660 #> Fitting model 133/660 #> Fitting model 134/660 #> Fitting model 135/660 #> Fitting model 136/660 #> Fitting model 137/660 #> Fitting model 138/660 #> Fitting model 139/660 #> Fitting model 140/660 #> Fitting model 141/660 #> Fitting model 142/660 #> Fitting model 143/660 #> Fitting model 144/660 #> Fitting model 145/660 #> Fitting model 146/660 #> Fitting model 147/660 #> Fitting model 148/660 #> Fitting model 149/660 #> Fitting model 150/660 #> Fitting model 151/660 #> Fitting model 152/660 #> Fitting model 153/660 #> Fitting model 154/660 #> Fitting model 155/660 #> Fitting model 156/660 #> Fitting model 157/660 #> Fitting model 158/660 #> Fitting model 159/660 #> Fitting model 160/660 #> Fitting model 161/660 #> Fitting model 162/660 #> Fitting model 163/660 #> Fitting model 164/660 #> Fitting model 165/660 #> Fitting model 166/660 #> Fitting model 167/660 #> Fitting model 168/660 #> Fitting model 169/660 #> Fitting model 170/660 #> Fitting model 171/660 #> Fitting model 172/660 #> Fitting model 173/660 #> Fitting model 174/660 #> Fitting model 175/660 #> Fitting model 176/660 #> Fitting model 177/660 #> Fitting model 178/660 #> Fitting model 179/660 #> Fitting model 180/660 #> Fitting model 181/660 #> Fitting model 182/660 #> Fitting model 183/660 #> Fitting model 184/660 #> Fitting model 185/660 #> Fitting model 186/660 #> Fitting model 187/660 #> Fitting model 188/660 #> Fitting model 189/660 #> Fitting model 190/660 #> Fitting model 191/660 #> Fitting model 192/660 #> Fitting model 193/660 #> Fitting model 194/660 #> Fitting model 195/660 #> Fitting model 196/660 #> Fitting model 197/660 #> Fitting model 198/660 #> Fitting model 199/660 #> Fitting model 200/660 #> Fitting model 201/660 #> Fitting model 202/660 #> Fitting model 203/660 #> Fitting model 204/660 #> Fitting model 205/660 #> Fitting model 206/660 #> Fitting model 207/660 #> Fitting model 208/660 #> Fitting model 209/660 #> Fitting model 210/660 #> Fitting model 211/660 #> Fitting model 212/660 #> Fitting model 213/660 #> Fitting model 214/660 #> Fitting model 215/660 #> Fitting model 216/660 #> Fitting model 217/660 #> Fitting model 218/660 #> Fitting model 219/660 #> Fitting model 220/660 #> Fitting model 221/660 #> Fitting model 222/660 #> Fitting model 223/660 #> Fitting model 224/660 #> Fitting model 225/660 #> Fitting model 226/660 #> Fitting model 227/660 #> Fitting model 228/660 #> Fitting model 229/660 #> Fitting model 230/660 #> Fitting model 231/660 #> Fitting model 232/660 #> Fitting model 233/660 #> Fitting model 234/660 #> Fitting model 235/660 #> Fitting model 236/660 #> Fitting model 237/660 #> Fitting model 238/660 #> Fitting model 239/660 #> Fitting model 240/660 #> Fitting model 241/660 #> Fitting model 242/660 #> Fitting model 243/660 #> Fitting model 244/660 #> Fitting model 245/660 #> Fitting model 246/660 #> Fitting model 247/660 #> Fitting model 248/660 #> Fitting model 249/660 #> Fitting model 250/660 #> Fitting model 251/660 #> Fitting model 252/660 #> Fitting model 253/660 #> Fitting model 254/660 #> Fitting model 255/660 #> Fitting model 256/660 #> Fitting model 257/660 #> Fitting model 258/660 #> Fitting model 259/660 #> Fitting model 260/660 #> Fitting model 261/660 #> Fitting model 262/660 #> Fitting model 263/660 #> Fitting model 264/660 #> Fitting model 265/660 #> Fitting model 266/660 #> Fitting model 267/660 #> Fitting model 268/660 #> Fitting model 269/660 #> Fitting model 270/660 #> Fitting model 271/660 #> Fitting model 272/660 #> Fitting model 273/660 #> Fitting model 274/660 #> Fitting model 275/660 #> Fitting model 276/660 #> Fitting model 277/660 #> Fitting model 278/660 #> Fitting model 279/660 #> Fitting model 280/660 #> Fitting model 281/660 #> Fitting model 282/660 #> Fitting model 283/660 #> Fitting model 284/660 #> Fitting model 285/660 #> Fitting model 286/660 #> Fitting model 287/660 #> Fitting model 288/660 #> Fitting model 289/660 #> Fitting model 290/660 #> Fitting model 291/660 #> Fitting model 292/660 #> Fitting model 293/660 #> Fitting model 294/660 #> Fitting model 295/660 #> Fitting model 296/660 #> Fitting model 297/660 #> Fitting model 298/660 #> Fitting model 299/660 #> Fitting model 300/660 #> Fitting model 301/660 #> Fitting model 302/660 #> Fitting model 303/660 #> Fitting model 304/660 #> Fitting model 305/660 #> Fitting model 306/660 #> Fitting model 307/660 #> Fitting model 308/660 #> Fitting model 309/660 #> Fitting model 310/660 #> Fitting model 311/660 #> Fitting model 312/660 #> Fitting model 313/660 #> Fitting model 314/660 #> Fitting model 315/660 #> Fitting model 316/660 #> Fitting model 317/660 #> Fitting model 318/660 #> Fitting model 319/660 #> Fitting model 320/660 #> Fitting model 321/660 #> Fitting model 322/660 #> Fitting model 323/660 #> Fitting model 324/660 #> Fitting model 325/660 #> Fitting model 326/660 #> Fitting model 327/660 #> Fitting model 328/660 #> Fitting model 329/660 #> Fitting model 330/660 #> Fitting model 331/660 #> Fitting model 332/660 #> Fitting model 333/660 #> Fitting model 334/660 #> Fitting model 335/660 #> Fitting model 336/660 #> Fitting model 337/660 #> Fitting model 338/660 #> Fitting model 339/660 #> Fitting model 340/660 #> Fitting model 341/660 #> Fitting model 342/660 #> Fitting model 343/660 #> Fitting model 344/660 #> Fitting model 345/660 #> Fitting model 346/660 #> Fitting model 347/660 #> Fitting model 348/660 #> Fitting model 349/660 #> Fitting model 350/660 #> Fitting model 351/660 #> Fitting model 352/660 #> Fitting model 353/660 #> Fitting model 354/660 #> Fitting model 355/660 #> Fitting model 356/660 #> Fitting model 357/660 #> Fitting model 358/660 #> Fitting model 359/660 #> Fitting model 360/660 #> Fitting model 361/660 #> Fitting model 362/660 #> Fitting model 363/660 #> Fitting model 364/660 #> Fitting model 365/660 #> Fitting model 366/660 #> Fitting model 367/660 #> Fitting model 368/660 #> Fitting model 369/660 #> Fitting model 370/660 #> Fitting model 371/660 #> Fitting model 372/660 #> Fitting model 373/660 #> Fitting model 374/660 #> Fitting model 375/660 #> Fitting model 376/660 #> Fitting model 377/660 #> Fitting model 378/660 #> Fitting model 379/660 #> Fitting model 380/660 #> Fitting model 381/660 #> Fitting model 382/660 #> Fitting model 383/660 #> Fitting model 384/660 #> Fitting model 385/660 #> Fitting model 386/660 #> Fitting model 387/660 #> Fitting model 388/660 #> Fitting model 389/660 #> Fitting model 390/660 #> Fitting model 391/660 #> Fitting model 392/660 #> Fitting model 393/660 #> Fitting model 394/660 #> Fitting model 395/660 #> Fitting model 396/660 #> Fitting model 397/660 #> Fitting model 398/660 #> Fitting model 399/660 #> Fitting model 400/660 #> Fitting model 401/660 #> Fitting model 402/660 #> Fitting model 403/660 #> Fitting model 404/660 #> Fitting model 405/660 #> Fitting model 406/660 #> Fitting model 407/660 #> Fitting model 408/660 #> Fitting model 409/660 #> Fitting model 410/660 #> Fitting model 411/660 #> Fitting model 412/660 #> Fitting model 413/660 #> Fitting model 414/660 #> Fitting model 415/660 #> Fitting model 416/660 #> Fitting model 417/660 #> Fitting model 418/660 #> Fitting model 419/660 #> Fitting model 420/660 #> Fitting model 421/660 #> Fitting model 422/660 #> Fitting model 423/660 #> Fitting model 424/660 #> Fitting model 425/660 #> Fitting model 426/660 #> Fitting model 427/660 #> Fitting model 428/660 #> Fitting model 429/660 #> Fitting model 430/660 #> Fitting model 431/660 #> Fitting model 432/660 #> Fitting model 433/660 #> Fitting model 434/660 #> Fitting model 435/660 #> Fitting model 436/660 #> Fitting model 437/660 #> Fitting model 438/660 #> Fitting model 439/660 #> Fitting model 440/660 #> Fitting model 441/660 #> Fitting model 442/660 #> Fitting model 443/660 #> Fitting model 444/660 #> Fitting model 445/660 #> Fitting model 446/660 #> Fitting model 447/660 #> Fitting model 448/660 #> Fitting model 449/660 #> Fitting model 450/660 #> Fitting model 451/660 #> Fitting model 452/660 #> Fitting model 453/660 #> Fitting model 454/660 #> Fitting model 455/660 #> Fitting model 456/660 #> Fitting model 457/660 #> Fitting model 458/660 #> Fitting model 459/660 #> Fitting model 460/660 #> Fitting model 461/660 #> Fitting model 462/660 #> Fitting model 463/660 #> Fitting model 464/660 #> Fitting model 465/660 #> Fitting model 466/660 #> Fitting model 467/660 #> Fitting model 468/660 #> Fitting model 469/660 #> Fitting model 470/660 #> Fitting model 471/660 #> Fitting model 472/660 #> Fitting model 473/660 #> Fitting model 474/660 #> Fitting model 475/660 #> Fitting model 476/660 #> Fitting model 477/660 #> Fitting model 478/660 #> Fitting model 479/660 #> Fitting model 480/660 #> Fitting model 481/660 #> Fitting model 482/660 #> Fitting model 483/660 #> Fitting model 484/660 #> Fitting model 485/660 #> Fitting model 486/660 #> Fitting model 487/660 #> Fitting model 488/660 #> Fitting model 489/660 #> Fitting model 490/660 #> Fitting model 491/660 #> Fitting model 492/660 #> Fitting model 493/660 #> Fitting model 494/660 #> Fitting model 495/660 #> Fitting model 496/660 #> Fitting model 497/660 #> Fitting model 498/660 #> Fitting model 499/660 #> Fitting model 500/660 #> Fitting model 501/660 #> Fitting model 502/660 #> Fitting model 503/660 #> Fitting model 504/660 #> Fitting model 505/660 #> Fitting model 506/660 #> Fitting model 507/660 #> Fitting model 508/660 #> Fitting model 509/660 #> Fitting model 510/660 #> Fitting model 511/660 #> Fitting model 512/660 #> Fitting model 513/660 #> Fitting model 514/660 #> Fitting model 515/660 #> Fitting model 516/660 #> Fitting model 517/660 #> Fitting model 518/660 #> Fitting model 519/660 #> Fitting model 520/660 #> Fitting model 521/660 #> Fitting model 522/660 #> Fitting model 523/660 #> Fitting model 524/660 #> Fitting model 525/660 #> Fitting model 526/660 #> Fitting model 527/660 #> Fitting model 528/660 #> Fitting model 529/660 #> Fitting model 530/660 #> Fitting model 531/660 #> Fitting model 532/660 #> Fitting model 533/660 #> Fitting model 534/660 #> Fitting model 535/660 #> Fitting model 536/660 #> Fitting model 537/660 #> Fitting model 538/660 #> Fitting model 539/660 #> Fitting model 540/660 #> Fitting model 541/660 #> Fitting model 542/660 #> Fitting model 543/660 #> Fitting model 544/660 #> Fitting model 545/660 #> Fitting model 546/660 #> Fitting model 547/660 #> Fitting model 548/660 #> Fitting model 549/660 #> Fitting model 550/660 #> Fitting model 551/660 #> Fitting model 552/660 #> Fitting model 553/660 #> Fitting model 554/660 #> Fitting model 555/660 #> Fitting model 556/660 #> Fitting model 557/660 #> Fitting model 558/660 #> Fitting model 559/660 #> Fitting model 560/660 #> Fitting model 561/660 #> Fitting model 562/660 #> Fitting model 563/660 #> Fitting model 564/660 #> Fitting model 565/660 #> Fitting model 566/660 #> Fitting model 567/660 #> Fitting model 568/660 #> Fitting model 569/660 #> Fitting model 570/660 #> Fitting model 571/660 #> Fitting model 572/660 #> Fitting model 573/660 #> Fitting model 574/660 #> Fitting model 575/660 #> Fitting model 576/660 #> Fitting model 577/660 #> Fitting model 578/660 #> Fitting model 579/660 #> Fitting model 580/660 #> Fitting model 581/660 #> Fitting model 582/660 #> Fitting model 583/660 #> Fitting model 584/660 #> Fitting model 585/660 #> Fitting model 586/660 #> Fitting model 587/660 #> Fitting model 588/660 #> Fitting model 589/660 #> Fitting model 590/660 #> Fitting model 591/660 #> Fitting model 592/660 #> Fitting model 593/660 #> Fitting model 594/660 #> Fitting model 595/660 #> Fitting model 596/660 #> Fitting model 597/660 #> Fitting model 598/660 #> Fitting model 599/660 #> Fitting model 600/660 #> Fitting model 601/660 #> Fitting model 602/660 #> Fitting model 603/660 #> Fitting model 604/660 #> Fitting model 605/660 #> Fitting model 606/660 #> Fitting model 607/660 #> Fitting model 608/660 #> Fitting model 609/660 #> Fitting model 610/660 #> Fitting model 611/660 #> Fitting model 612/660 #> Fitting model 613/660 #> Fitting model 614/660 #> Fitting model 615/660 #> Fitting model 616/660 #> Fitting model 617/660 #> Fitting model 618/660 #> Fitting model 619/660 #> Fitting model 620/660 #> Fitting model 621/660 #> Fitting model 622/660 #> Fitting model 623/660 #> Fitting model 624/660 #> Fitting model 625/660 #> Fitting model 626/660 #> Fitting model 627/660 #> Fitting model 628/660 #> Fitting model 629/660 #> Fitting model 630/660 #> Fitting model 631/660 #> Fitting model 632/660 #> Fitting model 633/660 #> Fitting model 634/660 #> Fitting model 635/660 #> Fitting model 636/660 #> Fitting model 637/660 #> Fitting model 638/660 #> Fitting model 639/660 #> Fitting model 640/660 #> Fitting model 641/660 #> Fitting model 642/660 #> Fitting model 643/660 #> Fitting model 644/660 #> Fitting model 645/660 #> Fitting model 646/660 #> Fitting model 647/660 #> Fitting model 648/660 #> Fitting model 649/660 #> Fitting model 650/660 #> Fitting model 651/660 #> Fitting model 652/660 #> Fitting model 653/660 #> Fitting model 654/660 #> Fitting model 655/660 #> Fitting model 656/660 #> Fitting model 657/660 #> Fitting model 658/660 #> Fitting model 659/660 #> Fitting model 660/660 # }"},{"path":"https://hendersontrent.github.io/theft/reference/zscore_scaler.html","id":null,"dir":"Reference","previous_headings":"","what":"Rescales a numeric vector into z-scores and then into the unit interval [0,1] — zscore_scaler","title":"Rescales a numeric vector into z-scores and then into the unit interval [0,1] — zscore_scaler","text":"\\(z_{} = \\frac{x_{} - \\mu}{\\sigma}\\)","code":""},{"path":"https://hendersontrent.github.io/theft/reference/zscore_scaler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rescales a numeric vector into z-scores and then into the unit interval [0,1] — zscore_scaler","text":"","code":"zscore_scaler(x, unitInt = TRUE)"},{"path":"https://hendersontrent.github.io/theft/reference/zscore_scaler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rescales a numeric vector into z-scores and then into the unit interval [0,1] — zscore_scaler","text":"x numeric vector unitInt Boolean whether rescale unit interval [0,1]. Defaults TRUE","code":""},{"path":"https://hendersontrent.github.io/theft/reference/zscore_scaler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rescales a numeric vector into z-scores and then into the unit interval [0,1] — zscore_scaler","text":"numeric vector","code":""},{"path":"https://hendersontrent.github.io/theft/reference/zscore_scaler.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Rescales a numeric vector into z-scores and then into the unit interval [0,1] — zscore_scaler","text":"Trent Henderson","code":""}]
