---
title: "Introduction to theft"
author: "Trent Henderson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to theft}
  %\VignetteEngine{knitr::knitr}
  %\usepackage[UTF-8]{inputenc}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.height = 6,
  fig.width = 9
)
```

```{r setup, message = FALSE, warning = FALSE}
library(theft)
```

## Core calculation functions

We are going to use a dataset that comes standard with `theft` called `simData`. This dataset contains a collection of randomly generated time series for six different types of processes. The dataset can be accessed via:

```{r, message = FALSE, warning = FALSE, eval = FALSE}
theft::simData
```

The data follows the following structure:

```{r, message = FALSE, warning = FALSE}
head(simData)
```

### Calculating feature summary statistics

The core function that automates the calculation of the feature statistics at once is `calculate_features`. You can choose which subset of features to calculate with the `feature_set` argument. The choices are currently `all`, `catch22`, `feasts`, `kats`, `tsfeatures` or `tsfresh`.

Note that `Kats`, `tsfresh` and `tsfel` are Python packages. The R package `reticulate` is used to call Python code that uses these packages and applies it within the broader *tidy* data philosophy embodied by `theft`. At present, depending on the input time-series, `theft` provides access to ~1200 features. Prior to using `theft` (only if you want to use the `Kats`, `tsfresh` or `tsfel` feature sets - the R-based sets will run fine) you should have a working Python installation and download `Kats` using the instructions located [here](https://facebookresearch.github.io/Kats/), `tsfresh` [here](https://tsfresh.com) or `tsfel` [here](https://github.com/fraunhoferportugal/tsfel). For this reason, features included in these three sets are not included in the `all` argument option. 

Here is an example with the `catch22` set:

```{r, message = FALSE, warning = FALSE}
feature_matrix <- calculate_features(data = simData, 
                                     id_var = "id", 
                                     time_var = "timepoint", 
                                     values_var = "values", 
                                     group_var = "process", 
                                     feature_set = "catch22")
```

Note that for the `catch22` set you can set the additional `catch24` argument to calculate the mean and standard deviation in addition to the standard 22 features:

```{r, message = FALSE, warning = FALSE, eval = FALSE}
feature_matrix <- calculate_features(data = simData, 
                                     id_var = "id", 
                                     time_var = "timepoint", 
                                     values_var = "values", 
                                     group_var = "process", 
                                     feature_set = "catch22",
                                     catch24 = TRUE)
```

If you do not want to or need to run the entire sets included in `calculate_features` you can also access the individual feature calculations as their unique function names, for example `IN_AutoMutualInfoStats_40_gaussian_fmmi`. A tidy dataframe of most of the included feature calculations and the set they correspond to is available:

```{r, message = FALSE, warning = FALSE, eval = FALSE}
View(feature_list)
```

## Data quality checks

The package comes with a function to visualise the data types of the calculated feature vectors.

```{r, message = FALSE, warning = FALSE}
plot_quality_matrix(feature_matrix)
```

## Normalising/scaling functions

Putting calculated feature vectors on an equal scale is crucial for any statistical or machine learning model as variables with high variance can adversely impact the model's capacity to fit the data appropriately, learn appropriate weight values, or minimise a loss function. `theft` includes the functions `normalise_feature_vector` to rescale a vector of values (e.g. vector of values for all participants in a study on the `SB_BinaryStats_mean_longstretch1` feature), and `normalise_feature_frame` to rescale a vector within a dataframe into a variety of different ranges for ease-of-use. Current transformations available in the package include:

* z-score - `"z-score"`
* Sigmoid - `"Sigmoid"`
* Outlier-robust Sigmoid (credit to Ben Fulcher for creating the original [MATLAB version](https://github.com/benfulcher/hctsa)) - `"RobustSigmoid"`
* Min-max - `"RobustSigmoid"`

Dataframe-based normalisation can be performed using the following:

```{r, message = FALSE, warning = FALSE}
normed <- normalise_feature_frame(feature_matrix, 
                                  names_var = "names", 
                                  values_var = "values", 
                                  method = "RobustSigmoid")
```

## Data visualisation functions

The package also comes with a built-in plotting engine for calculating and visualising (at present) three types of statistical graphics:

* Feature by time-series matrices as heatmaps
* Low dimension projections of the feature space for each time series as scatterplots
* Unique ID feature connectivity matrices as correlation-driven heatmaps

### Feature matrix

The function `plot_feature_matrix` takes the output of any included feature calculation function and produces a `ggplot` object heatmap showing the feature vectors across the `x` axis and each time series down the `y` axis. Prior to plotting, the function hierarchically clusters the data across both rows and columns to visually highlight the empirical structure.

```{r, message = FALSE, warning = FALSE}
plot_feature_matrix(feature_matrix, 
                    is_normalised = FALSE, 
                    id_var = "id", 
                    method = "RobustSigmoid",
                    interactive = FALSE)
```

If you set `interactive = TRUE` the function will return an interactive plot using `plotly` which lets you hover over entries in the matrix to better understand which is which. This is especially useful if your dataset is large.

```{r, message = FALSE, warning = FALSE}
plot_feature_matrix(feature_matrix, 
                    is_normalised = FALSE, 
                    id_var = "id", 
                    method = "RobustSigmoid",
                    interactive = TRUE)
```

### Dimension reduction

The function `plot_low_dimension` takes the output of any included feature calculation function, calculates a PCA or t-SNE, and produces a `ggplot` object scatterplot showing the first and second principal components on the `x` and `y` axis respectively, their individual explained variance, and each time series as a point on the `x-y` space. If `plot = FALSE` the function returns a dataframe of PCA results. If a variable is specified in the `group_var` argument, the scatterplot points will be automatically coloured by group.

```{r, message = FALSE, warning = FALSE}
plot_low_dimension(feature_matrix, 
                   is_normalised = FALSE, 
                   id_var = "id", 
                   group_var = "group", 
                   method = "RobustSigmoid", 
                   low_dim_method = "PCA", 
                   plot = TRUE,
                   show_covariance = TRUE)
```

Alternatively, a t-SNE version can be specified in a similar fashion, with the `perplexity` hyperparameter able to be controlled by the user. Typical values for this range between 5 and 50, depending on the size of the data.

```{r, message = FALSE, warning = FALSE}
plot_low_dimension(feature_matrix, 
                   is_normalised = FALSE, 
                   id_var = "id", 
                   group_var = "group", 
                   method = "RobustSigmoid", 
                   low_dim_method = "t-SNE", 
                   perplexity = 5, 
                   plot = TRUE,
                   show_covariance = FALSE)
```

### Correlation matrix

#### Time-series correlations

The function `plot_ts_correlations` automates the calculation of correlations between values of each unique time series, the hierarchical clustering of rows and columns of these correlations, and the production of a final heatmap-style graphic. You can switch the default Pearson correlation to a Spearman rank correlation by changing the `cor_method` argument to `spearman`.

```{r, message = FALSE, warning = FALSE}
plot_ts_correlations(simData, 
                     is_normalised = FALSE, 
                     id_var = "id", 
                     time_var = "timepoint",
                     values_var = "values",
                     method = "RobustSigmoid",
                     cor_method = "pearson",
                     interactive = FALSE)
```

Again, if you set `interactive = TRUE` the function will return an interactive plot using `plotly` which lets you hover over entries in the matrix to better understand which is which.

```{r, message = FALSE, warning = FALSE}
plot_ts_correlations(simData, 
                     is_normalised = FALSE, 
                     id_var = "id", 
                     time_var = "timepoint",
                     values_var = "values",
                     method = "RobustSigmoid",
                     cor_method = "spearman",
                     interactive = TRUE)
```

#### Feature correlations

Similarly, one can also plot correlations between feature vectors using `plot_feature_correlations`. The general argument structure is the same as `plot_ts_correlations`.

```{r, message = FALSE, warning = FALSE}
plot_feature_correlations(feature_matrix, 
                          is_normalised = FALSE, 
                          id_var = "id", 
                          names_var = "names",
                          values_var = "values",
                          method = "RobustSigmoid",
                          cor_method = "pearson",
                          interactive = FALSE)
```

## Time-series classification

## Feature-by-feature

Since feature-based time-series analysis has shown particular promise for classification problems, `theft` includes functionality for exploring group separation in addition to the low dimensional representation. The `compute_top_features` function takes the computed time series x feature matrix and performs the following:

1. Computes a classification algorithm on the entire input dataframe
2. Extracts individual feature performance from the classification algorithm
3. Filters the time series x feature matrix to the top `n` performing features (where `n` is specified by the user)
4. Plots a feature x feature correlation matrix as a heatmap for the top performing features
5. Plots a matrix of violin plots coloured by class label for the top performing features

This analysis is useful because it can help guide researchers toward more efficient and appropriate interpretation of high-performing features (if any exist) and helps protect against over-interpretation of feature values.

This function returns an object with three components to summarise the above steps:

1. `ResultsTable` -- a dataframe containing feature names, feature set membership, and performance statistics for the top performing features
2. `FeatureFeatureCorrelationPlot` -- a `ggplot` containing the pairwise correlations between top performing features represented as a heatmap
3. `ViolinPlots` -- a `ggplot` containing a matrix of violin plots showing class discrimination by feature

The code below produces analysis for a multiclass problem using a linear support vector machine.

```{r, message = FALSE, warning = FALSE}
classifier_outputs <- compute_top_features(feature_matrix, 
                                           id_var = "id", 
                                           group_var = "group", 
                                           normalise = TRUE, 
                                           method = "z-score", 
                                           cor_method = "pearson", 
                                           test_method = "linear svm")
```

Each component is named and can be accessed through regular use of the `$` operator or through list indexing (both return the same object). Here's the top few rows:

```{r, message = FALSE, warning = FALSE}
head(classifier_outputs$ResultsTable)
```

We could equivalently achieve the same with:

```{r, message = FALSE, warning = FALSE, eval = FALSE}
head(classifier_outputs[[1]])
```

The feature-feature correlation plots are accessed as the second object:

```{r, message = FALSE, warning = FALSE}
print(classifier_outputs$FeatureFeatureCorrelationPlot)
```

The violin plots are accessed as the third object:

```{r, message = FALSE, warning = FALSE}
print(classifier_outputs$ViolinPlots)
```

For two-class problems, users can fit any of the available models (which includes `"t-test"`, `"wilcox"`, `"binomial logistic"`, `"linear svm"`, and `"rbf svm"`) as string arguments to `test_method`. For multiclass problems the options are currently either `"linear svm"` or `"rbf svm"`.

The core functions that comprise `compute_top_features` are available individually for use as `fit_feature_classifier` and `plot_feature_discrimination`. The feature-feature correlation plot is a modified version of `plot_feature_correlations`.

## Multivariate

A multivariate option is also available. The `fit_multivariate_classifier` function fits all features at once instead of by feature to estimate classification accuracy. This can be split by feature set (if the `by_set` argument is set to `TRUE`) to enable systematic comparisons between those made available in `theft`.

First, let's calculate a feature matrix with more than one set:

```{r, message = FALSE, warning = FALSE}
feature_matrix2 <- calculate_features(data = simData, 
                                      id_var = "id", 
                                      time_var = "timepoint", 
                                      values_var = "values", 
                                      group_var = "process", 
                                      feature_set = c("catch22", "feasts", "tsfeatures"))
```

Now we can use our multivariate functionality:

```{r, message = FALSE, warning = FALSE}
multi_classifier_outputs <- fit_multivariate_classifier(feature_matrix, 
                                                        id_var = "id", 
                                                        group_var = "group", 
                                                        by_set = FALSE, 
                                                        num_splits = 10, 
                                                        test_method = "linear svm")
```

We can now access the various named objects returned by this function, which include:

1. `FeatureSetResultsPlot` (only if `by_set` is set to `TRUE`) -- a `ggplot` displaying mean classification accuracy and pointwise standard deviation by feature set
2. `FeatureSetTestStatistics` (only if `by_set` is set to `TRUE`) -- a dataframe containing a summary of test statistics for each feature set
3. `RawClassificationResults` -- a dataframe containing results from each seeded train-test split of main and empirical null models

Here's the feature set comparison plot:

```{r, message = FALSE, warning = FALSE}
print(multi_classifier_outputs$FeatureSetResultsPlot)
```

Here's the test statistics summary:

```{r, message = FALSE, warning = FALSE}
head(multi_classifier_outputs$FeatureSetTestStatistics)
```

And here's the raw classifier results:

```{r, message = FALSE, warning = FALSE}
head(multi_classifier_outputs$RawClassificationResults)
```

## Reading and processing hctsa-formatted files

As `theft` is based on the foundations laid by [`hctsa`](https://github.com/benfulcher/hctsa), there is also functionality for reading in `hctsa`-formatted Matlab files and automatically processing them into tidy dataframes ready for feature extraction in `theft`. The `process_hctsa_file` function takes a string filepath to the Matlab file and does all the work for you, returning a dataframe with naming conventions consistent with other `theft` functionality. As per `hctsa` specifications for [Input File Format 1](https://hctsa-users.gitbook.io/hctsa-manual/calculating/input_files), this file should have 3 variables with the following exact names: `timeSeriesData`, `labels`, and `keywords`.  Here is an example using the [Bonn University EEG dataset](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.64.061907)^[Andrzejak, Ralph G., et al. (2001) "Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state." Physical Review E 64(6): 061907].

```{r, message = FALSE, warning = FALSE, eval = FALSE}
d2 <- process_hctsa_file("https://cloudstor.aarnet.edu.au/plus/s/6sRD6IPMJyZLNlN/download")
```
